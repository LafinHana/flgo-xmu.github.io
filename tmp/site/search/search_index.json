{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#flgo","title":"FLGo","text":"<p>A research-oriented federated learning platform for fast developing and benchmarking FL algorithms.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Algorithm Development Friendly - highly integration, flexible APIs, and few codes. </li> <li>Comprehensive Benchmarks - 20+ built-in benchmarks across CV, NLP, Graph, and Tabular datasets. API for customizing personal datasets into federated ones.</li> <li>Real-world Simulation - arbitrarily customized simulation for scenarios like mobile, IoT, ...</li> <li>Experiment Tools - easy to manage experimental records and conduct analysis.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code># 1. update pip\npip install --upgrade pip\n\n# 2. install flgo through pip\npip install flgo\n</code></pre>"},{"location":"about/","title":"Contact us","text":"<p>Spatial Sensing and Computing Lab, Xiamen University</p> <p>zwang@stu.xmu.edu.cn</p> <p>fanxiaoliang@xmu.edu.cn</p>"},{"location":"getting_started/","title":"Get Started","text":""},{"location":"getting_started/#install-flgo","title":"Install FLGo","text":"<p>Install FLGo through pip. </p> <pre><code>pip install flgo\n</code></pre> <p>If the package is not found, please use the command below to update pip</p> <pre><code>pip install --upgrade pip\n</code></pre>"},{"location":"getting_started/#create-your-first-federated-task","title":"Create Your First Federated Task","text":"<p>Here we take the classical federated benchmark, Federated MNIST [1], as the example, where the MNIST dataset is splitted into 100 parts identically and independently.</p> <pre><code>import flgo\nimport os\n\n# the target path of the task\ntask_path = './my_first_task'\n\n# create task configuration\ntask_config = {'benchmark':{'name': 'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDPartitioner', 'para':{'num_clients':100}}}\n\n# generate the task if the task doesn't exist\nif not os.path.exist(task_path):\n    flgo.gen_task(task_config, task_path)\n</code></pre> <p>After running the codes above, a federated dataset is successfully created in the <code>task_path</code>. The visualization of the task is stored in <code>task_path/res.png</code> as below </p>"},{"location":"getting_started/#run-fedavg-to-train-your-model","title":"Run FedAvg to Train Your Model","text":"<p>Now we are going to run the classical federated optimization algorithm, FedAvg [1], on the task created by us to train a model.</p> <pre><code>import flgo.algorithm.fedavg as fedavg\n# create fedavg runner on the task\nrunner = flgo.init(task, fedavg, {'gpu':[0,],'log_file':True, 'num_steps':5})\nrunner.run()\n</code></pre>"},{"location":"getting_started/#show-training-result","title":"Show Training Result","text":"<p>The training result is saved as a record under the dictionary of the task <code>task_path/record</code>. We use the built-in analyzer to read and show it.</p> <pre><code>import flgo.experiment.analyzer\n# create the analysis plan\nanalysis_plan = {\n    'Selector':{'task': task_path, 'header':['fedavg',], },\n    'Painter':{'Curve':[{'args':{'x':'communication_round', 'y':'valid_loss'}}]},\n    'Table':{'min_value':[{'x':'valid_loss'}]},\n}\n\nflgo.experiment.analyzer.show(analysis_plan)\n</code></pre> <p></p>"},{"location":"Docs/FLGo/","title":"FLGo","text":""},{"location":"Docs/FLGo/#flgo.VirtualCommunicator","title":"<code>VirtualCommunicator</code>","text":"<p>Communicator that simulates the communication phase between any two objects</p> Source code in <code>flgo\\__init__.py</code> <pre><code>class VirtualCommunicator:\n\"\"\"\n    Communicator that simulates the communication phase between any two objects\n    \"\"\"\n    def __init__(self, objects):\n        self.objects_map = {obj.id:obj for obj in objects}\n        self.objects = objects\n\n    def request(self, source, target, package):\n        # send package to the target object with `package` and `mtype`, and then listen from it\n        return self.objects_map[target].message_handler(package)\n</code></pre>"},{"location":"Docs/documents/","title":"Documents","text":""},{"location":"Docs/documents/#flgo.algorithm","title":"<code>flgo.algorithm</code>","text":""},{"location":"Docs/documents/#flgo.benchmark","title":"<code>flgo.benchmark</code>","text":"<p>This module is designed for fast creating federated tasks. For example, in FL, a commonly used benchmark is federated MNIST that splits MNIST into 100 shards and each shard contains data of two types of labels.</p> <p>In FLGo, three basic components are created to describe a general procedure that can easily convert various ML tasks into federated ones.</p> Components <ul> <li> <p><code>TaskGenerator</code></p> <ul> <li>load the original dataset</li> <li>partition the original dataset into local data</li> </ul> </li> <li> <p><code>TaskPipe</code></p> <ul> <li>store the partition information of TaskGenerator into the disk     when generating federated tasks</li> <li>load the original dataset and the partition information to     create the federated scenario when optimizing models</li> </ul> </li> <li> <p><code>TaskCalculator</code></p> <ul> <li>support task-specific computation when optimizing models, such     as putting data into device, computing loss, evaluating models,     and creating the data loader</li> </ul> </li> </ul> <p>The architecture of a complete federate benchmark is shown as follows:</p> <pre><code>benchmark_name                  # benchmark folder\n\u251c\u2500 core.py                      # core file\n\u2502   \u251c\u2500 TaskGenerator            # class TaskGenerator(...)\n\u2502   \u251c\u2500 TaskPipe                 # class TaskPipe(...)\n\u2502   \u2514\u2500 TaskCalculator           # class TaskCalculator(...)\n\u2502\n\u251c\u2500  model                       # model folder (i.e. contains various types of models)\n\u2502   \u251c\u2500 model1_name.py           # model 1 (e.g. CNN)\n\u2502   \u251c\u2500 ...\n\u2502   \u2514\u2500 modelN_name.py           # model N (e.g. ResNet)\n\u2502       \u251c\u2500 init_local_module    # the function initializes personal models for parties\n\u2502       \u2514\u2500 init_global_module   # the function initializes the global models for parties\n\u2502\n\u2514\u2500 __init__.py                  # containing the variable default_model\n</code></pre> <p>Example: The architecture of MNIST is</p> <pre><code>\u251c\u2500 core.py\n\u2502   \u251c\u2500 TaskGenerator\n\u2502   \u251c\u2500 TaskPipe\n\u2502   \u2514\u2500 TaskCalculator\n\u251c\u2500  model\n\u2502   \u251c\u2500 cnn.py\n\u2502   \u2514\u2500 mlp.py\n\u2502       \u251c\u2500 init_local_module\n\u2502       \u2514\u2500 init_global_module\n\u2514\u2500 __init__.py\n</code></pre> <p>The details of implementing a customized benchmark are in Tutorial.3</p>"},{"location":"Docs/documents/#flgo.experiment","title":"<code>flgo.experiment</code>","text":"<p>This module is created for various experimental purposes</p>"},{"location":"Docs/documents/#flgo.simulator","title":"<code>flgo.simulator</code>","text":"<p>This module is to simulate arbitrary system heterogeneity that may occur in practice. We conclude four types of system heterogeneity from existing works.</p> System Heterogeneity Description <ol> <li> <p>Availability: the devices will be either available or unavailable at each moment, where only the                 available devices can be selected to participate in training.</p> </li> <li> <p>Responsiveness: the responsiveness describes the length of the period from the server broadcasting the                 gloabl model to the server receiving the locally trained model from a particular client.</p> </li> <li> <p>Completeness: since the server cannot fully control the behavior of devices,it's possible for devices to                 upload imcomplete model updates (i.e. only training for a few steps).</p> </li> <li> <p>Connectivity: the clients who promise to complete training may suffer accidients so that the server may lose                 connections with these client who will never return the currently trained local model.</p> </li> </ol> <p>We build up a client state machine to simulate the four types of system heterogeneity, and provide high-level APIs to allow customized system heterogeneity simulation.</p> <p>Example: How to customize the system heterogeneity:</p> <pre><code>&gt;&gt;&gt; class MySimulator(flgo.simulator.base.BasicSimulator):\n...     def update_client_availability(self):\n...         # update the variable 'prob_available' and 'prob_unavailable' for all the clients\n...         self.set_variable(self.all_clients, 'prob_available', [0.9 for _ in self.all_clients])\n...         self.set_variable(self.all_clients, 'prob_unavailable', [0.1 for _ in self.all_clients])\n...\n...     def update_client_connectivity(self, client_ids):\n...         # update the variable 'prob_drop' for clients in client_ids\n...         self.set_variable(client_ids, 'prob_drop', [0.1 for _ in client_ids])\n...\n...     def update_client_responsiveness(self, client_ids, *args, **kwargs):\n...         # update the variable 'latency' for clients in client_ids\n...         self.set_variable(client_ids, 'latency', [np.random.randint(5,100) for _ in client_ids])\n...\n...     def update_client_completeness(self, client_ids, *args, **kwargs):\n...         # update the variable 'working_amount' for clients in client_ids\n...         self.set_variable(client_ids, 'working_amount',  [max(int(self.clients[cid].num_steps*np.random.rand()), 1) for cid in client_ids])\n&gt;&gt;&gt; r = flgo.init(task, algorithm=fedavg, Simulator=MySimulator)\n&gt;&gt;&gt; # The runner r will be runned under the customized system heterogeneity, where the clients' states will be flushed by\n&gt;&gt;&gt; # MySimulator.update_client_xxx at each moment of the virtual clock or particular events happen (i.e. a client was selected)\n</code></pre> <p>We also provide some preset Simulator like flgo.simulator.DefaultSimulator and flgo.simulator.</p>"},{"location":"Docs/documents/#flgo.utils","title":"<code>flgo.utils</code>","text":""},{"location":"Docs/empty/","title":"flgo.benchmark.toolkits.tabular","text":"<p>To be implemented...</p>"},{"location":"Docs/algorithm/","title":"Index","text":""},{"location":"Docs/algorithm/#flgo.algorithm","title":"<code>flgo.algorithm</code>","text":""},{"location":"Docs/algorithm/fedbase/","title":"flgo.algorithm.fedbase","text":""},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase","title":"<code>flgo.algorithm.fedbase</code>","text":""},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient","title":"<code>BasicClient</code>","text":"<p>         Bases: <code>BasicParty</code></p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.get_batch_data","title":"<code>get_batch_data()</code>","text":"<p>Get the batch of training data</p> <p>Returns:</p> Type Description <p>a batch of data</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.get_time_response","title":"<code>get_time_response()</code>","text":"<p>Get the latency amount of the client</p> <p>Returns:</p> Type Description <p>self.latency_amount if client not dropping out</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.is_dropped","title":"<code>is_dropped()</code>","text":"<p>Check if the client drops out during communicating.</p> <p>Returns:</p> Type Description <p>True if the client was being dropped</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.is_idle","title":"<code>is_idle()</code>","text":"<p>Check if the client is available to participate training.</p> <p>Returns:</p> Type Description <p>True if the client is available according to the active_rate else False</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.is_working","title":"<code>is_working()</code>","text":"<p>Check if the client is training the model.</p> <p>Returns:</p> Type Description <p>True if the client is working</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.pack","title":"<code>pack(model, *args, **kwargs)</code>","text":"<p>Packing the package to be send to the server. The operations of compression of encryption of the package should be done here.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>the locally trained model</p> required <p>Returns:</p> Name Type Description <code>package</code> <p>a dict that contains the necessary information for the server</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.register_server","title":"<code>register_server(server=None)</code>","text":"<p>Register the server to self.server</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.reply","title":"<code>reply(svr_pkg)</code>","text":"<p>Reply a package to the server. The whole local procedure should be defined here. The standard form consists of three procedure: unpacking the server_package to obtain the global model, training the global model, and finally packing the updated model into client_package.</p> <p>Parameters:</p> Name Type Description Default <code>svr_pkg</code> <code>dict</code> <p>the package received from the server</p> required <p>Returns:</p> Name Type Description <code>client_pkg</code> <code>dict</code> <p>the package to be send to the server</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.set_batch_size","title":"<code>set_batch_size(batch_size=None)</code>","text":"<p>Set local training batch size</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>the training batch size</p> <code>None</code>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.set_learning_rate","title":"<code>set_learning_rate(lr=None)</code>","text":"<p>Set the learning rate of local training</p> <p>Parameters:</p> Name Type Description Default <code>lr</code> <code>float</code> <p>a real number</p> <code>None</code>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.set_local_epochs","title":"<code>set_local_epochs(epochs=None)</code>","text":"<p>Set local training epochs</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.test","title":"<code>test(model, flag='valid')</code>","text":"<p>Evaluate the model on the dataset owned by the client</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>flgo.utils.fmodule.FModule</code> <p>the model need to be evaluated</p> required <code>flag</code> <code>str</code> <p>choose the data to evaluate the model</p> <code>'valid'</code> <p>Returns:</p> Name Type Description <code>metric</code> <code>dict</code> <p>the evaluating results (e.g. metric = {'loss':1.02})</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.train","title":"<code>train(model)</code>","text":"<p>Standard local training procedure. Train the transmitted model with local training dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>FModule</code> <p>the global model</p> required"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.train_loss","title":"<code>train_loss(model)</code>","text":"<p>Get the loss value of the model on local training data</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>flgo.utils.fmodule.FModule | torch.nn.Module</code> <p>model</p> required <p>Returns:</p> Type Description <p>the training loss of model on self's training data</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.unpack","title":"<code>unpack(received_pkg)</code>","text":"<p>Unpack the package received from the server</p> <p>Parameters:</p> Name Type Description Default <code>received_pkg</code> <code>dict</code> <p>a dict contains the global model as default</p> required <p>Returns:</p> Type Description <p>the unpacked information</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.update_device","title":"<code>update_device(dev)</code>","text":"<p>Update running-time GPU device to dev</p> <p>Parameters:</p> Name Type Description Default <code>dev</code> <code>torch.device</code> <p>target dev</p> required"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicClient.valid_loss","title":"<code>valid_loss(model)</code>","text":"<p>Get the loss value of the model on local validating data</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>flgo.utils.fmodule.FModule | torch.nn.Module</code> <p>model</p> required <p>Returns:</p> Type Description <p>the validation loss of model on self's validation data</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty","title":"<code>BasicParty</code>","text":""},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.communicate_with","title":"<code>communicate_with(target_id, package={})</code>","text":"<p>Send the package to target object according to its id, and receive the response from it</p> <p>Parameters:</p> Name Type Description Default <code>target_id</code> <code>int</code> <p>the id of the object to communicate with</p> required <code>package</code> <code>dict</code> <p>the package to be sended to the object</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>client_package</code> <code>dict</code> <p>the reply from the target object and will be 'None' if losing connection</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.initialize","title":"<code>initialize(*args, **kwargs)</code>","text":"<p>API for customizing the initializing process of the object</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.message_handler","title":"<code>message_handler(package)</code>","text":"<p>Handling the received message by excuting the corresponding action.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>dict</code> <p>the package received from other parties (i.e. the content of the message)</p> required <p>Returns:</p> Type Description <p>action_reult</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.register_action_to_mtype","title":"<code>register_action_to_mtype(action_name, mtype)</code>","text":"<p>Register an existing method as the action corresponding to the message type.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>the name of the instance method</p> required <code>mtype</code> <p>the message type</p> required"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.register_objects","title":"<code>register_objects(parties, parties_name='parties')</code>","text":"<p>Set self's attribute party_names (e.g. parties as default) to be parties if self has no attribute named party_names. Otherwise, parties will be extend to the attribute party_names of self.</p> <p>Parameters:</p> Name Type Description Default <code>parties</code> <code>list</code> <p>a list of objects</p> required <code>parties_name</code> <code>str</code> <p>the name of attribute to store parties</p> <code>'parties'</code> <p>Example:</p> <pre><code>    &gt;&gt;&gt; a = BasicParty()\n    &gt;&gt;&gt; b = BasicParty()\n    &gt;&gt;&gt; c = BasicParty()\n    &gt;&gt;&gt; a.register_objects([b, c], 'parties')\n    &gt;&gt;&gt; a.parties # will be [b,c]\n    &gt;&gt;&gt; d = BasicParty()\n    &gt;&gt;&gt; a.register_objects([d], 'parties')\n    &gt;&gt;&gt; a.parties # will be [b,c,d]\n</code></pre>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.set_data","title":"<code>set_data(data, flag='train')</code>","text":"<p>Set self's attibute 'xxx_data' to be data where xxx is the flag. For example, after calling self.set_data([1,2,3], 'test'), self.test_data will be [1,2,3]. Particularly, If the flag is 'train', the batchsize and the num_steps will be reset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>anything</p> required <code>flag</code> <code>str</code> <p>the name of the data</p> <code>'train'</code>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.set_id","title":"<code>set_id(id=None)</code>","text":"<p>Set self's attibute 'id' to be id where self.id = id</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicParty.set_model","title":"<code>set_model(model, model_name='model')</code>","text":"<p>Set self's attibute 'model_name' to be model. For example, after calling self.set_model(my_model, 'model'), self.model will be my_model.</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer","title":"<code>BasicServer</code>","text":"<p>         Bases: <code>BasicParty</code></p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.available_clients","title":"<code>available_clients</code>  <code>property</code>","text":"<p>Return all the available clients at the current round.</p> <p>Returns:</p> Type Description <p>a list of indices of currently available clients</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.aggregate","title":"<code>aggregate(models, *args, **kwargs)</code>","text":"<p>Aggregate the locally trained models into the new one. The aggregation will be according to self.aggregate_option where</p> <p>pk = nk/n where n=self.data_vol K = |S_t| N = |S|</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.aggregate--weighted_scale-uniform-default-weighted_com-original-fedavg-other","title":"weighted_scale                 |uniform (default)          |weighted_com (original fedavg)   |other","text":"<p>N/K * \u03a3pk * model_k             |1/K * \u03a3model_k             |(1-\u03a3pk) * w_old + \u03a3pk * model_k  |\u03a3(pk/\u03a3pk) * model_k</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>list</code> <p>a list of local models</p> required <p>Returns:</p> Type Description <p>the aggregated model</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; models = [m1, m2] # m1, m2 are models with the same architecture\n    &gt;&gt;&gt; m_new = self.aggregate(models)\n</code></pre>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.communicate","title":"<code>communicate(selected_clients, mtype=0, asynchronous=False)</code>","text":"<p>The whole simulating communication procedure with the selected clients. This part supports for simulating the client dropping out.</p> <p>Parameters:</p> Name Type Description Default <code>selected_clients</code> <code>list of int</code> <p>the clients to communicate with</p> required <code>mtype</code> <code>anytype</code> <p>type of message</p> <code>0</code> <code>asynchronous</code> <code>bool</code> <p>asynchronous communciation or synchronous communcation</p> <code>False</code> <p>Returns:</p> Type Description <p>the unpacked response from clients that is created ny self.unpack()</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.communicate_with","title":"<code>communicate_with(target_id, package={})</code>","text":"<p>Communicate with the object under system simulator that simulates the network latency. Send the package to target object according to its id, and receive the response from it</p> <p>Parameters:</p> Name Type Description Default <code>target_id</code> <code>int</code> <p>the id of the object to communicate with</p> required <code>package</code> <code>dict</code> <p>the package to be sended to the object</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>client_package</code> <code>dict</code> <p>the reply from the target object and</p> <p>will be 'None' if losing connection</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.get_tolerance_for_latency","title":"<code>get_tolerance_for_latency()</code>","text":"<p>Get the tolerance for latency of waiting for clients' responses</p> <p>Returns:</p> Type Description <p>a int number (i.e. self.tolerance_for_latency)</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.global_lr_scheduler","title":"<code>global_lr_scheduler(current_round)</code>","text":"<p>Control the step size (i.e. learning rate) of local training</p> <p>Parameters:</p> Name Type Description Default <code>current_round</code> <code>int</code> <p>the current communication round</p> required"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.global_test","title":"<code>global_test(model=None, flag='valid')</code>","text":"<p>Collect local testing result of all the clients.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>flgo.utils.fmodule.FModule | torch.nn.Module</code> <p>the model to be sevaluated</p> <code>None</code> <code>flag</code> <code>str</code> <p>choose the data to evaluate the model</p> <code>'valid'</code> <p>Returns:</p> Name Type Description <code>metrics</code> <code>dict</code> <p>a dict contains key-value pairs like (metric_name,</p> <p>the lists of metric results of the clients)</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.init_algo_para","title":"<code>init_algo_para(algo_para)</code>","text":"<p>Initialize the algorithm-dependent hyper-parameters for the server and all the clients.</p> <p>Parameters:</p> Name Type Description Default <code>algo_paras</code> <code>dict</code> <p>the dict that defines the hyper-parameters (i.e. name, value and type) for the algorithm.</p> required <p>Example:</p> <pre><code>    &gt;&gt;&gt; # s is an instance of Server and s.clients are instances of Client\n    &gt;&gt;&gt; s.u # will raise error\n    &gt;&gt;&gt; [c.u for c in s.clients] # will raise errors too\n    &gt;&gt;&gt; s.init_algo_para({'u': 0.1})\n    &gt;&gt;&gt; s.u # will be 0.1\n    &gt;&gt;&gt; [c.u for c in s.clients] # will be [0.1, 0.1,..., 0.1]\n</code></pre> Note <p>Once <code>option['algo_para']</code> is not <code>None</code>, the value of the pre-defined hyperparameters will be replaced by the list of values in <code>option['algo_para']</code>, which requires the length of <code>option['algo_para']</code> is equal to the length of <code>algo_paras</code></p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.iterate","title":"<code>iterate()</code>","text":"<p>The standard iteration of each federated communication round that contains three necessary procedure in FL: client selection, communication and model aggregation.</p> <p>Returns:</p> Type Description <p>False if the global model is not updated in this iteration</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.pack","title":"<code>pack(client_id, mtype=0, *args, **kwargs)</code>","text":"<p>Pack the necessary information for the client's local training. Any operations of compression or encryption should be done here.</p> <p>Parameters:</p> Name Type Description Default <code>client_id</code> <code>int</code> <p>the id of the client to communicate with</p> required <code>mtype</code> <p>the message type</p> <code>0</code> <p>Returns:</p> Type Description <p>a dict contains necessary information (e.g. a copy of the global model as default)</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.register_clients","title":"<code>register_clients(clients)</code>","text":"<p>Regiser clients to self.clients, and update related attributes (e.g. self.num_clients)</p> <p>Parameters:</p> Name Type Description Default <code>clients</code> <code>list</code> <p>a list of objects</p> required"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.run","title":"<code>run()</code>","text":"<p>Running the FL symtem where the global model is trained and evaluated iteratively.</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.sample","title":"<code>sample()</code>","text":"<p>Sample the clients. There are three types of sampling manners: full sample, uniform sample without replacement, and MDSample with replacement. Particularly, if 'available' is in self.sample_option, the server will only sample from currently available clients.</p> <p>Returns:</p> Type Description <p>a list of the ids of the selected clients</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; selected_clients=self.sample()\n    &gt;&gt;&gt; selected_clients\n    &gt;&gt;&gt; # The selected_clients is a list of clients' ids\n</code></pre>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.test","title":"<code>test(model=None, flag='test')</code>","text":"<p>Evaluate the model on the test dataset owned by the server.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>flgo.utils.fmodule.FModule</code> <p>the model need to be evaluated</p> <code>None</code> <code>flag</code> <code>str</code> <p>choose the data to evaluate the model</p> <code>'test'</code> <p>Returns:</p> Name Type Description <code>metrics</code> <code>dict</code> <p>the dict contains the evaluating results</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.unpack","title":"<code>unpack(packages_received_from_clients)</code>","text":"<p>Unpack the information from the received packages. Return models and losses as default.</p> <p>Parameters:</p> Name Type Description Default <code>packages_received_from_clients</code> <code>list</code> <p>a list of packages</p> required <p>Returns:</p> Name Type Description <code>res</code> <code>dict</code> <p>collections.defaultdict that contains several lists of the clients' reply</p>"},{"location":"Docs/algorithm/fedbase/#flgo.algorithm.fedbase.BasicServer.wait_time","title":"<code>wait_time(t=1)</code>","text":"<p>Wait for the time of the virtual clock to pass t units</p>"},{"location":"Docs/algorithm/vflbase/","title":"flgo.algorithm.vflbase","text":""},{"location":"Docs/algorithm/vflbase/#flgo.algorithm.vflbase","title":"<code>flgo.algorithm.vflbase</code>","text":""},{"location":"Docs/algorithm/vflbase/#flgo.algorithm.vflbase.ActiveParty","title":"<code>ActiveParty</code>","text":"<p>         Bases: <code>PassiveParty</code></p>"},{"location":"Docs/algorithm/vflbase/#flgo.algorithm.vflbase.ActiveParty.communicate","title":"<code>communicate(selected_clients, mtype=0, asynchronous=False)</code>","text":"<p>The whole simulating communication procedure with the selected clients. This part supports for simulating the client dropping out.</p> <p>Parameters:</p> Name Type Description Default <code>selected_clients</code> <p>the clients to communicate with</p> required <p>Returns:</p> Type Description <p>the unpacked response from clients that is created ny self.unpack()</p>"},{"location":"Docs/algorithm/vflbase/#flgo.algorithm.vflbase.ActiveParty.get_batch_data","title":"<code>get_batch_data()</code>","text":"<p>Get the batch of data</p> <p>Returns:</p> Name Type Description <code>batch_data</code> <p>a batch of data</p>"},{"location":"Docs/algorithm/vflbase/#flgo.algorithm.vflbase.ActiveParty.run","title":"<code>run()</code>","text":"<p>Start the federated learning symtem where the global model is trained iteratively.</p>"},{"location":"Docs/algorithm/vflbase/#flgo.algorithm.vflbase.ActiveParty.unpack","title":"<code>unpack(packages_received_from_clients)</code>","text":"<p>Unpack the information from the received packages. Return models and losses as default.</p> <p>Parameters:</p> Name Type Description Default <code>packages_received_from_clients</code> <code>list of dict</code> required <p>Returns:</p> Name Type Description <code>res</code> <code>dict</code> <p>collections.defaultdict that contains several lists of the clients' reply</p>"},{"location":"Docs/benchmark/","title":"Index","text":""},{"location":"Docs/benchmark/#flgo.benchmark","title":"<code>flgo.benchmark</code>","text":"<p>This module is designed for fast creating federated tasks. For example, in FL, a commonly used benchmark is federated MNIST that splits MNIST into 100 shards and each shard contains data of two types of labels.</p> <p>In FLGo, three basic components are created to describe a general procedure that can easily convert various ML tasks into federated ones.</p> Components <ul> <li> <p><code>TaskGenerator</code></p> <ul> <li>load the original dataset</li> <li>partition the original dataset into local data</li> </ul> </li> <li> <p><code>TaskPipe</code></p> <ul> <li>store the partition information of TaskGenerator into the disk     when generating federated tasks</li> <li>load the original dataset and the partition information to     create the federated scenario when optimizing models</li> </ul> </li> <li> <p><code>TaskCalculator</code></p> <ul> <li>support task-specific computation when optimizing models, such     as putting data into device, computing loss, evaluating models,     and creating the data loader</li> </ul> </li> </ul> <p>The architecture of a complete federate benchmark is shown as follows:</p> <pre><code>benchmark_name                  # benchmark folder\n\u251c\u2500 core.py                      # core file\n\u2502   \u251c\u2500 TaskGenerator            # class TaskGenerator(...)\n\u2502   \u251c\u2500 TaskPipe                 # class TaskPipe(...)\n\u2502   \u2514\u2500 TaskCalculator           # class TaskCalculator(...)\n\u2502\n\u251c\u2500  model                       # model folder (i.e. contains various types of models)\n\u2502   \u251c\u2500 model1_name.py           # model 1 (e.g. CNN)\n\u2502   \u251c\u2500 ...\n\u2502   \u2514\u2500 modelN_name.py           # model N (e.g. ResNet)\n\u2502       \u251c\u2500 init_local_module    # the function initializes personal models for parties\n\u2502       \u2514\u2500 init_global_module   # the function initializes the global models for parties\n\u2502\n\u2514\u2500 __init__.py                  # containing the variable default_model\n</code></pre> <p>Example: The architecture of MNIST is</p> <pre><code>\u251c\u2500 core.py\n\u2502   \u251c\u2500 TaskGenerator\n\u2502   \u251c\u2500 TaskPipe\n\u2502   \u2514\u2500 TaskCalculator\n\u251c\u2500  model\n\u2502   \u251c\u2500 cnn.py\n\u2502   \u2514\u2500 mlp.py\n\u2502       \u251c\u2500 init_local_module\n\u2502       \u2514\u2500 init_global_module\n\u2514\u2500 __init__.py\n</code></pre> <p>The details of implementing a customized benchmark are in Tutorial.3</p>"},{"location":"Docs/benchmark/base/","title":"flgo.benchmark.base","text":""},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskCalculator","title":"<code>AbstractTaskCalculator</code>","text":"<p>Abstract Task Calculator</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class AbstractTaskCalculator(metaclass=ABCMeta):\nr\"\"\"\n    Abstract Task Calculator\n    \"\"\"\n    @abstractmethod\n    def to_device(self, *args, **kwargs):\n\"\"\"Put the data into the gpu device\"\"\"\n        pass\n\n    @abstractmethod\n    def get_dataloader(self, *args, **kwargs):\n\"\"\"Return a data loader that splits the input data into batches\"\"\"\n        pass\n\n    @abstractmethod\n    def test(self, model, data, *args, **kwargs):\n\"\"\"Evaluate the model on the data\"\"\"\n        pass\n\n    @abstractmethod\n    def compute_loss(self, model, data, *args, **kwargs):\n\"\"\"Compute the loss of the model on the data to complete the forward process\"\"\"\n        pass\n\n    @abstractmethod\n    def get_optimizer(self, model, *args, **kwargs):\n\"\"\"Return the optimizer on the parameters of the model\"\"\"\n        pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskCalculator.compute_loss","title":"<code>compute_loss(model, data, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Compute the loss of the model on the data to complete the forward process</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef compute_loss(self, model, data, *args, **kwargs):\n\"\"\"Compute the loss of the model on the data to complete the forward process\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskCalculator.get_dataloader","title":"<code>get_dataloader(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return a data loader that splits the input data into batches</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef get_dataloader(self, *args, **kwargs):\n\"\"\"Return a data loader that splits the input data into batches\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskCalculator.get_optimizer","title":"<code>get_optimizer(model, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Return the optimizer on the parameters of the model</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef get_optimizer(self, model, *args, **kwargs):\n\"\"\"Return the optimizer on the parameters of the model\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskCalculator.test","title":"<code>test(model, data, *args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the model on the data</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef test(self, model, data, *args, **kwargs):\n\"\"\"Evaluate the model on the data\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskCalculator.to_device","title":"<code>to_device(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Put the data into the gpu device</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef to_device(self, *args, **kwargs):\n\"\"\"Put the data into the gpu device\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskGenerator","title":"<code>AbstractTaskGenerator</code>","text":"<p>Abstract Task Generator</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class AbstractTaskGenerator(metaclass=ABCMeta):\nr\"\"\"\n    Abstract Task Generator\n    \"\"\"\n    @abstractmethod\n    def load_data(self, *args, **kwarg):\n\"\"\"Load the original data into memory that can be partitioned\"\"\"\n        pass\n\n    @abstractmethod\n    def partition(self, *args, **kwarg):\n\"\"\"Partition the loaded data into subsets of data owned by clients\n        and the test data owned by the server\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def generate(self, *args, **kwarg):\n\"\"\"Load and partition the data, and then generate the necessary\n        information about the federated task (e.g. path, partition way, ...)\"\"\"\n        pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskGenerator.generate","title":"<code>generate(*args, **kwarg)</code>  <code>abstractmethod</code>","text":"<p>Load and partition the data, and then generate the necessary information about the federated task (e.g. path, partition way, ...)</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef generate(self, *args, **kwarg):\n\"\"\"Load and partition the data, and then generate the necessary\n    information about the federated task (e.g. path, partition way, ...)\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskGenerator.load_data","title":"<code>load_data(*args, **kwarg)</code>  <code>abstractmethod</code>","text":"<p>Load the original data into memory that can be partitioned</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef load_data(self, *args, **kwarg):\n\"\"\"Load the original data into memory that can be partitioned\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskGenerator.partition","title":"<code>partition(*args, **kwarg)</code>  <code>abstractmethod</code>","text":"<p>Partition the loaded data into subsets of data owned by clients and the test data owned by the server</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef partition(self, *args, **kwarg):\n\"\"\"Partition the loaded data into subsets of data owned by clients\n    and the test data owned by the server\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskPipe","title":"<code>AbstractTaskPipe</code>","text":"<p>Abstract Task Pipe</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class AbstractTaskPipe(metaclass=ABCMeta):\nr\"\"\"\n    Abstract Task Pipe\n    \"\"\"\n    @abstractmethod\n    def save_task(self, *args, **kwargs):\n\"\"\"Save a federated task created by TaskGenerator as a static file on the disk\"\"\"\n        pass\n\n    @abstractmethod\n    def load_task(self, *args, **kwargs):\n\"\"\"Load a federated task from disk\"\"\"\n        pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskPipe.load_task","title":"<code>load_task(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Load a federated task from disk</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef load_task(self, *args, **kwargs):\n\"\"\"Load a federated task from disk\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.AbstractTaskPipe.save_task","title":"<code>save_task(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Save a federated task created by TaskGenerator as a static file on the disk</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>@abstractmethod\ndef save_task(self, *args, **kwargs):\n\"\"\"Save a federated task created by TaskGenerator as a static file on the disk\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskCalculator","title":"<code>BasicTaskCalculator</code>","text":"<p>         Bases: <code>AbstractTaskCalculator</code></p> <p>Support task-specific computation when optimizing models, such as putting data into device, computing loss, evaluating models, and creating the data loader</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class BasicTaskCalculator(AbstractTaskCalculator):\nr\"\"\"\n    Support task-specific computation when optimizing models, such\n    as putting data into device, computing loss, evaluating models,\n    and creating the data loader\n    \"\"\"\n\n    def __init__(self, device, optimizer_name='sgd'):\nr\"\"\"\n        Args:\n            device (torch.device): device\n            optimizer_name (str): the name of the optimizer\n        \"\"\"\n        self.device = device\n        self.optimizer_name = optimizer_name\n        self.criterion = None\n        self.DataLoader = None\n\n    def to_device(self, data, *args, **kwargs):\n        return NotImplementedError\n\n    def get_dataloader(self, *args, **kwargs):\n        return NotImplementedError\n\n    def test(self, model, data, *args, **kwargs):\n        return NotImplementedError\n\n    def compute_loss(self, model, data, *args, **kwargs):\n        return NotImplementedError\n\n    def get_optimizer(self, model=None, lr=0.1, weight_decay=0, momentum=0):\nr\"\"\"\n        Create optimizer of the model parameters\n\n        Args:\n            model (torch.nn.Module): model\n            lr (float): learning rate\n            weight_decay (float): the weight_decay coefficient\n            momentum (float): the momentum coefficient\n\n        Returns:\n            the optimizer\n        \"\"\"\n        OPTIM = getattr(importlib.import_module('torch.optim'), self.optimizer_name)\n        filter_fn = filter(lambda p: p.requires_grad, model.parameters())\n        if self.optimizer_name.lower() == 'sgd':\n            return OPTIM(filter_fn, lr=lr, momentum=momentum, weight_decay=weight_decay)\n        elif self.optimizer_name.lower() in ['adam', 'rmsprop', 'adagrad']:\n            return OPTIM(filter_fn, lr=lr, weight_decay=weight_decay)\n        else:\n            raise RuntimeError(\"Invalid Optimizer.\")\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskCalculator.__init__","title":"<code>__init__(device, optimizer_name='sgd')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>device</p> required <code>optimizer_name</code> <code>str</code> <p>the name of the optimizer</p> <code>'sgd'</code> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def __init__(self, device, optimizer_name='sgd'):\nr\"\"\"\n    Args:\n        device (torch.device): device\n        optimizer_name (str): the name of the optimizer\n    \"\"\"\n    self.device = device\n    self.optimizer_name = optimizer_name\n    self.criterion = None\n    self.DataLoader = None\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskCalculator.get_optimizer","title":"<code>get_optimizer(model=None, lr=0.1, weight_decay=0, momentum=0)</code>","text":"<p>Create optimizer of the model parameters</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>torch.nn.Module</code> <p>model</p> <code>None</code> <code>lr</code> <code>float</code> <p>learning rate</p> <code>0.1</code> <code>weight_decay</code> <code>float</code> <p>the weight_decay coefficient</p> <code>0</code> <code>momentum</code> <code>float</code> <p>the momentum coefficient</p> <code>0</code> <p>Returns:</p> Type Description <p>the optimizer</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def get_optimizer(self, model=None, lr=0.1, weight_decay=0, momentum=0):\nr\"\"\"\n    Create optimizer of the model parameters\n\n    Args:\n        model (torch.nn.Module): model\n        lr (float): learning rate\n        weight_decay (float): the weight_decay coefficient\n        momentum (float): the momentum coefficient\n\n    Returns:\n        the optimizer\n    \"\"\"\n    OPTIM = getattr(importlib.import_module('torch.optim'), self.optimizer_name)\n    filter_fn = filter(lambda p: p.requires_grad, model.parameters())\n    if self.optimizer_name.lower() == 'sgd':\n        return OPTIM(filter_fn, lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif self.optimizer_name.lower() in ['adam', 'rmsprop', 'adagrad']:\n        return OPTIM(filter_fn, lr=lr, weight_decay=weight_decay)\n    else:\n        raise RuntimeError(\"Invalid Optimizer.\")\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator","title":"<code>BasicTaskGenerator</code>","text":"<p>         Bases: <code>AbstractTaskGenerator</code></p> <p>Load the original dataset and partition the original dataset into local data</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class BasicTaskGenerator(AbstractTaskGenerator):\nr\"\"\"\n        Load the original dataset and partition the\n        original dataset into local data\n    \"\"\"\n    def __init__(self, benchmark:str, rawdata_path:str):\n\"\"\"\n        Args:\n            benchmark (str): the name of the federated task\n            rawdata_path (str): the dictionary of the original dataset\n        \"\"\"\n        # basic attribution\n        self.benchmark = benchmark\n        self.rawdata_path = rawdata_path\n        # optional attribution\n        self.partitioner = None\n        self.train_data = None\n        self.test_data = None\n        self.task_name = None\n        self.para = {}\n        self.additional_option = {}\n        self.train_additional_option = {}\n        self.test_additional_option = {}\n\n    def generate(self, *args, **kwarg):\n\"\"\"The whole process to generate federated task. \"\"\"\n        # load data\n        self.load_data()\n        # partition\n        self.partition()\n        # generate task name\n        self.task_name = self.get_task_name()\n        return\n\n    def load_data(self, *args, **kwargs):\n\"\"\"Download and load dataset into memory.\"\"\"\n        raise NotImplementedError\n\n    def partition(self, *args, **kwargs):\n\"\"\"Partition the data into different local datasets\"\"\"\n        return\n\n    def register_partitioner(self, partitioner=None):\n\"\"\"Register the partitioner as self's data partitioner\"\"\"\n        self.partitioner = partitioner\n\n    def init_para(self, para_list=None):\n        pnames = list(self.para.keys())\n        if para_list is not None:\n            for i, pv in enumerate(para_list):\n                pname = pnames[i]\n                try:\n                    self.para[pname] = type(self.para[pname])(pv)\n                except:\n                    self.para[pname] = pv\n        for pname, pv in self.para.items():\n            self.__setattr__(pname, pv)\n        return\n\n    def get_task_name(self):\nr\"\"\"\n        Create the default name of the task\n        \"\"\"\n        if not hasattr(self.partitioner, 'num_parties') and hasattr(self.partitioner, 'num_clients'):\n            self.partitioner.num_parties = self.partitioner.num_clients\n        else: self.partitioner.num_parties = 'unknown'\n        return '_'.join(['B-' + self.benchmark, 'P-' + str(self.partitioner), 'N-' + str(self.partitioner.num_parties)])\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator.__init__","title":"<code>__init__(benchmark, rawdata_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>str</code> <p>the name of the federated task</p> required <code>rawdata_path</code> <code>str</code> <p>the dictionary of the original dataset</p> required Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def __init__(self, benchmark:str, rawdata_path:str):\n\"\"\"\n    Args:\n        benchmark (str): the name of the federated task\n        rawdata_path (str): the dictionary of the original dataset\n    \"\"\"\n    # basic attribution\n    self.benchmark = benchmark\n    self.rawdata_path = rawdata_path\n    # optional attribution\n    self.partitioner = None\n    self.train_data = None\n    self.test_data = None\n    self.task_name = None\n    self.para = {}\n    self.additional_option = {}\n    self.train_additional_option = {}\n    self.test_additional_option = {}\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator.generate","title":"<code>generate(*args, **kwarg)</code>","text":"<p>The whole process to generate federated task.</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def generate(self, *args, **kwarg):\n\"\"\"The whole process to generate federated task. \"\"\"\n    # load data\n    self.load_data()\n    # partition\n    self.partition()\n    # generate task name\n    self.task_name = self.get_task_name()\n    return\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator.get_task_name","title":"<code>get_task_name()</code>","text":"<p>Create the default name of the task</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def get_task_name(self):\nr\"\"\"\n    Create the default name of the task\n    \"\"\"\n    if not hasattr(self.partitioner, 'num_parties') and hasattr(self.partitioner, 'num_clients'):\n        self.partitioner.num_parties = self.partitioner.num_clients\n    else: self.partitioner.num_parties = 'unknown'\n    return '_'.join(['B-' + self.benchmark, 'P-' + str(self.partitioner), 'N-' + str(self.partitioner.num_parties)])\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator.load_data","title":"<code>load_data(*args, **kwargs)</code>","text":"<p>Download and load dataset into memory.</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def load_data(self, *args, **kwargs):\n\"\"\"Download and load dataset into memory.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator.partition","title":"<code>partition(*args, **kwargs)</code>","text":"<p>Partition the data into different local datasets</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def partition(self, *args, **kwargs):\n\"\"\"Partition the data into different local datasets\"\"\"\n    return\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskGenerator.register_partitioner","title":"<code>register_partitioner(partitioner=None)</code>","text":"<p>Register the partitioner as self's data partitioner</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def register_partitioner(self, partitioner=None):\n\"\"\"Register the partitioner as self's data partitioner\"\"\"\n    self.partitioner = partitioner\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe","title":"<code>BasicTaskPipe</code>","text":"<p>         Bases: <code>AbstractTaskPipe</code></p> <p>Store the partition information of TaskGenerator into the disk when generating federated tasks.</p> <p>Load the original dataset and the partition information to create the federated scenario when optimizing models</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class BasicTaskPipe(AbstractTaskPipe):\nr\"\"\"\n    Store the partition information of TaskGenerator into the disk\n    when generating federated tasks.\n\n    Load the original dataset and the partition information to\n    create the federated scenario when optimizing models\n    \"\"\"\n    TaskDataset = None\n\n    def __init__(self, task_path):\nr\"\"\"\n        Args:\n            task_path (str): the path of the federated task\n        \"\"\"\n        self.task_path = task_path\n        if os.path.exists(os.path.join(self.task_path, 'data.json')):\n            with open(os.path.join(self.task_path, 'data.json'), 'r') as inf:\n                self.feddata = json.load(inf)\n\n    def save_task(self, generator):\n\"\"\"Construct `feddata` and store it into the disk for recovering\n        the partitioned datasets again from it\"\"\"\n        raise NotImplementedError\n\n    def load_data(self, running_time_option) -&gt; dict:\n\"\"\"Load the data and process it to the format that can be distributed\n        to different objects\"\"\"\n        raise NotImplementedError\n\n    def generate_objects(self, running_time_option, algorithm, scene='horizontal') -&gt; list:\nr\"\"\"\n        Generate the virtual objects (i.e. coordinators and participants)\n        in the FL system\n\n        Args:\n            running_time_option (dict): the option (i.e. configuration)\n            algorithm (module|class): algorithm\n            scene (str): horizontal or vertical\n        \"\"\"\n        if scene=='horizontal':\n            # init clients\n            Client = algorithm.Client\n            clients = [Client(running_time_option) for _ in range(len(self.feddata['client_names']))]\n            for cid, c in enumerate(clients):\n                c.id = cid\n                c.name = self.feddata['client_names'][cid]\n            # init server\n            server = algorithm.Server(running_time_option)\n            server.name = 'server'\n            server.id = -1\n            # bind clients and server\n            server.register_clients(clients)\n            for c in clients: c.register_server(server)\n            # return objects as list\n            objects = [server]\n            objects.extend(clients)\n        elif scene=='vertical':\n            PassiveParty = algorithm.PassiveParty\n            ActiveParty = algorithm.ActiveParty\n            objects = []\n            for pid, pname in enumerate(self.feddata['party_names']):\n                is_active = self.feddata[pname]['data']['with_label']\n                obj = ActiveParty(running_time_option) if is_active else PassiveParty(running_time_option)\n                obj.id = pid\n                obj.name = pname\n                objects.append(obj)\n            for party in objects:\n                party.register_objects(objects)\n        return objects\n\n    def save_info(self, generator):\nr\"\"\"\n        Save the basic information of the generated task into the disk\n        \"\"\"\n        info = {'benchmark': '.'.join(generator.__module__.split('.')[:-1])}\n        info['scene'] = generator.scene if hasattr(generator, 'scene') else 'unknown'\n        info['num_clients'] = generator.num_clients if hasattr(generator, 'num_clients') else (generator.num_parties if hasattr(self, 'num_parties') else 'unknown')\n        with open(os.path.join(self.task_path, 'info'), 'w') as outf:\n            json.dump(info, outf)\n\n    def load_task(self, running_time_option, *args, **kwargs):\nr\"\"\"\n        Load the generated task into disk and create objects in the federated\n        scenario.\n        \"\"\"\n        task_data = self.load_data(running_time_option)\n        objects = self.generate_objects(running_time_option)\n        self.distribute(task_data, objects)\n        return objects\n\n    def distribute(self, task_data: dict, objects: list):\nr\"\"\"\n        Distribute the loaded local datasets to different objects in\n        the federated scenario\n        \"\"\"\n        for ob in objects:\n            ob_data = task_data[ob.name]\n            for data_name, data in ob_data.items():\n                ob.set_data(data, data_name)\n\n    def split_dataset(self, dataset, p=0.0):\nr\"\"\"\n        Split the dataset into two parts.\n\n        Args:\n            dataset (torch.utils.data.Dataset): the dataset to be splitted\n            p (float): the ratio of the splitting\n\n        Returns:\n            The two split parts\n        \"\"\"\n        if p == 0: return dataset, None\n        s1 = int(len(dataset) * p)\n        s2 = len(dataset) - s1\n        return torch.utils.data.random_split(dataset, [s2, s1])\n\n    def task_exists(self):\nr\"\"\"\n        Check whether the task already exists.\n\n        Returns:\n            True if the task already exists\n        \"\"\"\n        return os.path.exists(self.task_path)\n\n    def remove_task(self):\nr\"\"\"Remove this task\"\"\"\n        if self.task_exists():\n            shutil.rmtree(self.task_path)\n        return\n\n    def create_task_architecture(self):\n\"\"\"Create the directories of the task.\"\"\"\n        if not self.task_exists():\n            os.mkdir(self.task_path)\n            os.mkdir(os.path.join(self.task_path, 'record'))\n            os.mkdir(os.path.join(self.task_path, 'log'))\n        else:\n            raise FileExistsError(\"federated task {} already exists!\".format(self.task_path))\n\n    def gen_client_names(self, num_clients):\nr\"\"\"\n        Generate the names of clients\n\n        Returns:\n            a list of strings\n        \"\"\"\n        return [('Client{:0&gt;' + str(len(str(num_clients))) + 'd}').format(i) for i in range(num_clients)]\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.__init__","title":"<code>__init__(task_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task_path</code> <code>str</code> <p>the path of the federated task</p> required Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def __init__(self, task_path):\nr\"\"\"\n    Args:\n        task_path (str): the path of the federated task\n    \"\"\"\n    self.task_path = task_path\n    if os.path.exists(os.path.join(self.task_path, 'data.json')):\n        with open(os.path.join(self.task_path, 'data.json'), 'r') as inf:\n            self.feddata = json.load(inf)\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.create_task_architecture","title":"<code>create_task_architecture()</code>","text":"<p>Create the directories of the task.</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def create_task_architecture(self):\n\"\"\"Create the directories of the task.\"\"\"\n    if not self.task_exists():\n        os.mkdir(self.task_path)\n        os.mkdir(os.path.join(self.task_path, 'record'))\n        os.mkdir(os.path.join(self.task_path, 'log'))\n    else:\n        raise FileExistsError(\"federated task {} already exists!\".format(self.task_path))\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.distribute","title":"<code>distribute(task_data, objects)</code>","text":"<p>Distribute the loaded local datasets to different objects in the federated scenario</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def distribute(self, task_data: dict, objects: list):\nr\"\"\"\n    Distribute the loaded local datasets to different objects in\n    the federated scenario\n    \"\"\"\n    for ob in objects:\n        ob_data = task_data[ob.name]\n        for data_name, data in ob_data.items():\n            ob.set_data(data, data_name)\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.gen_client_names","title":"<code>gen_client_names(num_clients)</code>","text":"<p>Generate the names of clients</p> <p>Returns:</p> Type Description <p>a list of strings</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def gen_client_names(self, num_clients):\nr\"\"\"\n    Generate the names of clients\n\n    Returns:\n        a list of strings\n    \"\"\"\n    return [('Client{:0&gt;' + str(len(str(num_clients))) + 'd}').format(i) for i in range(num_clients)]\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.generate_objects","title":"<code>generate_objects(running_time_option, algorithm, scene='horizontal')</code>","text":"<p>Generate the virtual objects (i.e. coordinators and participants) in the FL system</p> <p>Parameters:</p> Name Type Description Default <code>running_time_option</code> <code>dict</code> <p>the option (i.e. configuration)</p> required <code>algorithm</code> <code>module|class</code> <p>algorithm</p> required <code>scene</code> <code>str</code> <p>horizontal or vertical</p> <code>'horizontal'</code> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def generate_objects(self, running_time_option, algorithm, scene='horizontal') -&gt; list:\nr\"\"\"\n    Generate the virtual objects (i.e. coordinators and participants)\n    in the FL system\n\n    Args:\n        running_time_option (dict): the option (i.e. configuration)\n        algorithm (module|class): algorithm\n        scene (str): horizontal or vertical\n    \"\"\"\n    if scene=='horizontal':\n        # init clients\n        Client = algorithm.Client\n        clients = [Client(running_time_option) for _ in range(len(self.feddata['client_names']))]\n        for cid, c in enumerate(clients):\n            c.id = cid\n            c.name = self.feddata['client_names'][cid]\n        # init server\n        server = algorithm.Server(running_time_option)\n        server.name = 'server'\n        server.id = -1\n        # bind clients and server\n        server.register_clients(clients)\n        for c in clients: c.register_server(server)\n        # return objects as list\n        objects = [server]\n        objects.extend(clients)\n    elif scene=='vertical':\n        PassiveParty = algorithm.PassiveParty\n        ActiveParty = algorithm.ActiveParty\n        objects = []\n        for pid, pname in enumerate(self.feddata['party_names']):\n            is_active = self.feddata[pname]['data']['with_label']\n            obj = ActiveParty(running_time_option) if is_active else PassiveParty(running_time_option)\n            obj.id = pid\n            obj.name = pname\n            objects.append(obj)\n        for party in objects:\n            party.register_objects(objects)\n    return objects\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.load_data","title":"<code>load_data(running_time_option)</code>","text":"<p>Load the data and process it to the format that can be distributed to different objects</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def load_data(self, running_time_option) -&gt; dict:\n\"\"\"Load the data and process it to the format that can be distributed\n    to different objects\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.load_task","title":"<code>load_task(running_time_option, *args, **kwargs)</code>","text":"<p>Load the generated task into disk and create objects in the federated scenario.</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def load_task(self, running_time_option, *args, **kwargs):\nr\"\"\"\n    Load the generated task into disk and create objects in the federated\n    scenario.\n    \"\"\"\n    task_data = self.load_data(running_time_option)\n    objects = self.generate_objects(running_time_option)\n    self.distribute(task_data, objects)\n    return objects\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.remove_task","title":"<code>remove_task()</code>","text":"<p>Remove this task</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def remove_task(self):\nr\"\"\"Remove this task\"\"\"\n    if self.task_exists():\n        shutil.rmtree(self.task_path)\n    return\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.save_info","title":"<code>save_info(generator)</code>","text":"<p>Save the basic information of the generated task into the disk</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def save_info(self, generator):\nr\"\"\"\n    Save the basic information of the generated task into the disk\n    \"\"\"\n    info = {'benchmark': '.'.join(generator.__module__.split('.')[:-1])}\n    info['scene'] = generator.scene if hasattr(generator, 'scene') else 'unknown'\n    info['num_clients'] = generator.num_clients if hasattr(generator, 'num_clients') else (generator.num_parties if hasattr(self, 'num_parties') else 'unknown')\n    with open(os.path.join(self.task_path, 'info'), 'w') as outf:\n        json.dump(info, outf)\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.save_task","title":"<code>save_task(generator)</code>","text":"<p>Construct <code>feddata</code> and store it into the disk for recovering the partitioned datasets again from it</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def save_task(self, generator):\n\"\"\"Construct `feddata` and store it into the disk for recovering\n    the partitioned datasets again from it\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.split_dataset","title":"<code>split_dataset(dataset, p=0.0)</code>","text":"<p>Split the dataset into two parts.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>torch.utils.data.Dataset</code> <p>the dataset to be splitted</p> required <code>p</code> <code>float</code> <p>the ratio of the splitting</p> <code>0.0</code> <p>Returns:</p> Type Description <p>The two split parts</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def split_dataset(self, dataset, p=0.0):\nr\"\"\"\n    Split the dataset into two parts.\n\n    Args:\n        dataset (torch.utils.data.Dataset): the dataset to be splitted\n        p (float): the ratio of the splitting\n\n    Returns:\n        The two split parts\n    \"\"\"\n    if p == 0: return dataset, None\n    s1 = int(len(dataset) * p)\n    s2 = len(dataset) - s1\n    return torch.utils.data.random_split(dataset, [s2, s1])\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.BasicTaskPipe.task_exists","title":"<code>task_exists()</code>","text":"<p>Check whether the task already exists.</p> <p>Returns:</p> Type Description <p>True if the task already exists</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>def task_exists(self):\nr\"\"\"\n    Check whether the task already exists.\n\n    Returns:\n        True if the task already exists\n    \"\"\"\n    return os.path.exists(self.task_path)\n</code></pre>"},{"location":"Docs/benchmark/base/#flgo.benchmark.base.XYHorizontalTaskPipe","title":"<code>XYHorizontalTaskPipe</code>","text":"<p>         Bases: <code>BasicTaskPipe</code></p> <p>This pipe is for supervised learning where each sample contains a feature $x_i$ and a label $y_i$  that can be indexed by $i$. To use this pipe, it's necessary to set the attribute <code>test_data</code> of the generator to be a dict like:     {'x': [...], 'y':[...]} and the attribute <code>local_datas</code> to be a list of the above dict that means the local data owned by clients:     [{'x':[...], 'y':[...]}, ..., ]</p> Source code in <code>flgo\\benchmark\\base.py</code> <pre><code>class XYHorizontalTaskPipe(BasicTaskPipe):\n\"\"\"\n    This pipe is for supervised learning where each sample contains a feature $x_i$ and a label $y_i$\n     that can be indexed by $i$.\n    To use this pipe, it's necessary to set the attribute `test_data` of the generator to be a dict like:\n        {'x': [...], 'y':[...]}\n    and the attribute `local_datas` to be a list of the above dict that means the local data owned by clients:\n        [{'x':[...], 'y':[...]}, ..., ]\n    \"\"\"\n    TaskDataset = torch.utils.data.TensorDataset\n\n    def save_task(self, generator):\n        client_names = self.gen_client_names(len(generator.local_datas))\n        feddata = {'client_names': client_names, 'server': {'data': generator.test_data}}\n        for cid in range(len(client_names)): feddata[client_names[cid]] = {'data': generator.local_datas[cid]}\n        with open(os.path.join(self.task_path, 'data.json'), 'w') as outf:\n            json.dump(feddata, outf)\n\n    def load_data(self, running_time_option) -&gt; dict:\n        test_data = self.feddata['server']['data']\n        test_data = self.TaskDataset(torch.tensor(test_data['x']), torch.tensor(test_data['y']))\n        local_datas = [self.TaskDataset(torch.tensor(self.feddata[cname]['data']['x']),\n                                        torch.tensor(self.feddata[cname]['data']['y'])) for cname in\n                       self.feddata['client_names']]\n        server_data_test, server_data_valid = self.split_dataset(test_data, running_time_option['test_holdout'])\n        task_data = {'server': {'test': server_data_test, 'valid': server_data_valid}}\n        for key in self.feddata['server'].keys():\n            if key == 'data':\n                continue\n            task_data['server'][key] = self.feddata['server'][key]\n        for cid, cname in enumerate(self.feddata['client_names']):\n            cdata = local_datas[cid]\n            cdata_train, cdata_valid = self.split_dataset(cdata, running_time_option['train_holdout'])\n            task_data[cname] = {'train': cdata_train, 'valid': cdata_valid}\n            for key in self.feddata[cname]:\n                if key == 'data':\n                    continue\n                task_data[cname][key] = self.feddata[cname][key]\n        return task_data\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/","title":"flgo.benchmark.toolkits.partition","text":"<p>This file contains preset partitioners for the benchmarks. All the Partitioner should implement the method <code>__call__(self, data)</code> where <code>data</code> is the dataset to be partitioned and the return is a list of the partitioned result.</p> <p>For example, The IIDPartitioner.call receives a indexable object (i.e. instance of torchvision.datasets.mnsit.MNSIT) and I.I.D. selects samples' indices in the original dataset as each client's local data. The list of list of sample indices are finally returnerd (e.g. [[0,1,2,...,1008], ...,[25,23,98,...,997]]).</p> <p>To use the partitioner, you can specify Partitioner in the configuration dict for <code>flgo.gen_task</code>.  Example 1: passing the parameter of init of the Partitioner through the dict <code>para</code></p> <p>import flgo config = {'benchmark':{'name':'flgo.benchmark.mnist_classification'}, ...            'partitioner':{'name':'IIDPartitioner', 'para':{'num_clients':20, 'alpha':1.0}}} flgo.gen_task(config, './test_partition')</p>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.BasicPartitioner","title":"<code>BasicPartitioner</code>","text":"<p>         Bases: <code>AbstractPartitioner</code></p> <p>This is the basic class of data partitioner. The partitioner will be directly called by the task generator of different benchmarks. By overwriting call method, different partitioners can be realized. The input of call is usually a dataset.</p> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class BasicPartitioner(AbstractPartitioner):\n\"\"\"This is the basic class of data partitioner. The partitioner will be directly called by the\n    task generator of different benchmarks. By overwriting __call__ method, different partitioners\n    can be realized. The input of __call__ is usually a dataset.\n    \"\"\"\n    def __call__(self, *args, **kwargs):\n        return\n\n    def register_generator(self, generator):\nr\"\"\"Register the generator as an self's attribute\"\"\"\n        self.generator = generator\n\n    def data_imbalance_generator(self, num_clients, datasize, imbalance=0):\nr\"\"\"\n        Split the data size into several parts\n\n        Args:\n            num_clients (int): the number of clients\n            datasize (int): the total data size\n            imbalance (float): the degree of data imbalance across clients\n\n        Returns:\n            a list of integer numbers that represents local data sizes\n        \"\"\"\n        if imbalance == 0:\n            samples_per_client = [int(datasize / num_clients) for _ in range(num_clients)]\n            for _ in range(datasize % num_clients): samples_per_client[_] += 1\n        else:\n            imbalance = max(0.1, imbalance)\n            sigma = imbalance\n            mean_datasize = datasize / num_clients\n            mu = np.log(mean_datasize) - sigma ** 2 / 2.0\n            samples_per_client = np.random.lognormal(mu, sigma, (num_clients)).astype(int)\n            thresold = int(imbalance ** 1.5 * (datasize - num_clients * 10))\n            delta = int(0.1 * thresold)\n            crt_data_size = sum(samples_per_client)\n            # force current data size to match the total data size\n            while crt_data_size != datasize:\n                if crt_data_size - datasize &gt;= thresold:\n                    maxid = np.argmax(samples_per_client)\n                    maxvol = samples_per_client[maxid]\n                    new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                    while min(new_samples) &gt; maxvol:\n                        new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                    new_size_id = np.argmin(\n                        [np.abs(crt_data_size - samples_per_client[maxid] + s - datasize) for s in new_samples])\n                    samples_per_client[maxid] = new_samples[new_size_id]\n                elif crt_data_size - datasize &gt;= delta:\n                    maxid = np.argmax(samples_per_client)\n                    samples_per_client[maxid] -= delta\n                elif crt_data_size - datasize &gt; 0:\n                    maxid = np.argmax(samples_per_client)\n                    samples_per_client[maxid] -= (crt_data_size - datasize)\n                elif datasize - crt_data_size &gt;= thresold:\n                    minid = np.argmin(samples_per_client)\n                    minvol = samples_per_client[minid]\n                    new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                    while max(new_samples) &lt; minvol:\n                        new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                    new_size_id = np.argmin(\n                        [np.abs(crt_data_size - samples_per_client[minid] + s - datasize) for s in new_samples])\n                    samples_per_client[minid] = new_samples[new_size_id]\n                elif datasize - crt_data_size &gt;= delta:\n                    minid = np.argmin(samples_per_client)\n                    samples_per_client[minid] += delta\n                else:\n                    minid = np.argmin(samples_per_client)\n                    samples_per_client[minid] += (datasize - crt_data_size)\n                crt_data_size = sum(samples_per_client)\n        return samples_per_client\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.BasicPartitioner.data_imbalance_generator","title":"<code>data_imbalance_generator(num_clients, datasize, imbalance=0)</code>","text":"<p>Split the data size into several parts</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>the number of clients</p> required <code>datasize</code> <code>int</code> <p>the total data size</p> required <code>imbalance</code> <code>float</code> <p>the degree of data imbalance across clients</p> <code>0</code> <p>Returns:</p> Type Description <p>a list of integer numbers that represents local data sizes</p> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>def data_imbalance_generator(self, num_clients, datasize, imbalance=0):\nr\"\"\"\n    Split the data size into several parts\n\n    Args:\n        num_clients (int): the number of clients\n        datasize (int): the total data size\n        imbalance (float): the degree of data imbalance across clients\n\n    Returns:\n        a list of integer numbers that represents local data sizes\n    \"\"\"\n    if imbalance == 0:\n        samples_per_client = [int(datasize / num_clients) for _ in range(num_clients)]\n        for _ in range(datasize % num_clients): samples_per_client[_] += 1\n    else:\n        imbalance = max(0.1, imbalance)\n        sigma = imbalance\n        mean_datasize = datasize / num_clients\n        mu = np.log(mean_datasize) - sigma ** 2 / 2.0\n        samples_per_client = np.random.lognormal(mu, sigma, (num_clients)).astype(int)\n        thresold = int(imbalance ** 1.5 * (datasize - num_clients * 10))\n        delta = int(0.1 * thresold)\n        crt_data_size = sum(samples_per_client)\n        # force current data size to match the total data size\n        while crt_data_size != datasize:\n            if crt_data_size - datasize &gt;= thresold:\n                maxid = np.argmax(samples_per_client)\n                maxvol = samples_per_client[maxid]\n                new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                while min(new_samples) &gt; maxvol:\n                    new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                new_size_id = np.argmin(\n                    [np.abs(crt_data_size - samples_per_client[maxid] + s - datasize) for s in new_samples])\n                samples_per_client[maxid] = new_samples[new_size_id]\n            elif crt_data_size - datasize &gt;= delta:\n                maxid = np.argmax(samples_per_client)\n                samples_per_client[maxid] -= delta\n            elif crt_data_size - datasize &gt; 0:\n                maxid = np.argmax(samples_per_client)\n                samples_per_client[maxid] -= (crt_data_size - datasize)\n            elif datasize - crt_data_size &gt;= thresold:\n                minid = np.argmin(samples_per_client)\n                minvol = samples_per_client[minid]\n                new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                while max(new_samples) &lt; minvol:\n                    new_samples = np.random.lognormal(mu, sigma, (10 * num_clients))\n                new_size_id = np.argmin(\n                    [np.abs(crt_data_size - samples_per_client[minid] + s - datasize) for s in new_samples])\n                samples_per_client[minid] = new_samples[new_size_id]\n            elif datasize - crt_data_size &gt;= delta:\n                minid = np.argmin(samples_per_client)\n                samples_per_client[minid] += delta\n            else:\n                minid = np.argmin(samples_per_client)\n                samples_per_client[minid] += (datasize - crt_data_size)\n            crt_data_size = sum(samples_per_client)\n    return samples_per_client\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.BasicPartitioner.register_generator","title":"<code>register_generator(generator)</code>","text":"<p>Register the generator as an self's attribute</p> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>def register_generator(self, generator):\nr\"\"\"Register the generator as an self's attribute\"\"\"\n    self.generator = generator\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.DirichletPartitioner","title":"<code>DirichletPartitioner</code>","text":"<p>         Bases: <code>BasicPartitioner</code></p> <p>`Partition the indices of samples in the original dataset according to Dirichlet distribution of the particular attribute. This way of partition is widely used by existing works in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>the number of clients</p> <code>100</code> <code>alpha</code> <code>float</code> <p><code>alpha</code>(i.e. alpha&gt;=0) in Dir(alpha*p) where p is the global distribution. The smaller alpha is, the higher heterogeneity the data is.</p> <code>1.0</code> <code>imbalance</code> <code>float</code> <p>the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)</p> <code>0</code> <code>error_bar</code> <code>float</code> <p>the allowed error when the generated distribution mismatches the distirbution that is actually wanted, since there may be no solution for particular imbalance and alpha.</p> <code>1e-06</code> <code>flag_index</code> <code>int</code> <p>the index of the distribution-dependent (i.e. label) attribute in each sample.</p> <code>-1</code> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class DirichletPartitioner(BasicPartitioner):\n\"\"\"`Partition the indices of samples in the original dataset according to Dirichlet distribution of the\n    particular attribute. This way of partition is widely used by existing works in federated learning.\n\n    Args:\n        num_clients (int, optional): the number of clients\n        alpha (float, optional): `alpha`(i.e. alpha&gt;=0) in Dir(alpha*p) where p is the global distribution. The smaller alpha is, the higher heterogeneity the data is.\n        imbalance (float, optional): the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)\n        error_bar (float, optional): the allowed error when the generated distribution mismatches the distirbution that is actually wanted, since there may be no solution for particular imbalance and alpha.\n        flag_index (int, optional): the index of the distribution-dependent (i.e. label) attribute in each sample.\n    \"\"\"\n    def __init__(self, num_clients=100, alpha=1.0, error_bar=1e-6, imbalance=0, flag_index=-1):\n        self.num_clients = num_clients\n        self.alpha = alpha\n        self.imbalance = imbalance\n        self.flag_index = flag_index\n        self.error_bar = error_bar\n\n    def __str__(self):\n        name = \"dir{:.2f}_err{}\".format(self.alpha, self.error_bar)\n        if self.imbalance &gt; 0: name += '_imb{:.1f}'.format(self.imbalance)\n        return name\n\n    def __call__(self, data):\n        attrs = [d[self.flag_index] for d in data]\n        num_attrs = len(set(attrs))\n        samples_per_client = self.data_imbalance_generator(self.num_clients, len(data), self.imbalance)\n        # count the label distribution\n        lb_counter = collections.Counter(attrs)\n        p = np.array([1.0 * v / len(data) for v in lb_counter.values()])\n        lb_dict = {}\n        attrs = np.array(attrs)\n        for lb in range(len(lb_counter.keys())):\n            lb_dict[lb] = np.where(attrs == lb)[0]\n        proportions = [np.random.dirichlet(self.alpha * p) for _ in range(self.num_clients)]\n        while np.any(np.isnan(proportions)):\n            proportions = [np.random.dirichlet(self.alpha * p) for _ in range(self.num_clients)]\n        sorted_cid_map = {k: i for k, i in zip(np.argsort(samples_per_client), [_ for _ in range(self.num_clients)])}\n        error_increase_interval = 500\n        max_error = self.error_bar\n        loop_count = 0\n        crt_id = 0\n        crt_error = 100000\n        while True:\n            if loop_count &gt;= error_increase_interval:\n                loop_count = 0\n                max_error = max_error * 10\n            # generate dirichlet distribution till ||E(proportion) - P(D)||&lt;=1e-5*self.num_classes\n            mean_prop = np.sum([pi * di for pi, di in zip(proportions, samples_per_client)], axis=0)\n            mean_prop = mean_prop / mean_prop.sum()\n            error_norm = ((mean_prop - p) ** 2).sum()\n            if crt_error - error_norm &gt;= max_error:\n                print(\"Error: {:.8f}\".format(error_norm))\n                crt_error = error_norm\n            if error_norm &lt;= max_error:\n                break\n            excid = sorted_cid_map[crt_id]\n            crt_id = (crt_id + 1) % self.num_clients\n            sup_prop = [np.random.dirichlet(self.alpha * p) for _ in range(self.num_clients)]\n            del_prop = np.sum([pi * di for pi, di in zip(proportions, samples_per_client)], axis=0)\n            del_prop -= samples_per_client[excid] * proportions[excid]\n            for i in range(error_increase_interval - loop_count):\n                alter_norms = []\n                for cid in range(self.num_clients):\n                    if np.any(np.isnan(sup_prop[cid])):\n                        continue\n                    alter_prop = del_prop + samples_per_client[excid] * sup_prop[cid]\n                    alter_prop = alter_prop / alter_prop.sum()\n                    error_alter = ((alter_prop - p) ** 2).sum()\n                    alter_norms.append(error_alter)\n                if min(alter_norms) &lt; error_norm:\n                    break\n            if len(alter_norms) &gt; 0 and min(alter_norms) &lt; error_norm:\n                alcid = np.argmin(alter_norms)\n                proportions[excid] = sup_prop[alcid]\n            loop_count += 1\n        local_datas = [[] for _ in range(self.num_clients)]\n        self.dirichlet_dist = []  # for efficiently visualizing\n        for lb in lb_counter.keys():\n            lb_idxs = lb_dict[lb]\n            lb_proportion = np.array([pi[lb] * si for pi, si in zip(proportions, samples_per_client)])\n            lb_proportion = lb_proportion / lb_proportion.sum()\n            lb_proportion = (np.cumsum(lb_proportion) * len(lb_idxs)).astype(int)[:-1]\n            lb_datas = np.split(lb_idxs, lb_proportion)\n            self.dirichlet_dist.append([len(lb_data) for lb_data in lb_datas])\n            local_datas = [local_data + lb_data.tolist() for local_data, lb_data in zip(local_datas, lb_datas)]\n        self.dirichlet_dist = np.array(self.dirichlet_dist).T\n        for i in range(self.num_clients): np.random.shuffle(local_datas[i])\n        self.local_datas = local_datas\n        return local_datas\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.DiversityPartitioner","title":"<code>DiversityPartitioner</code>","text":"<p>         Bases: <code>BasicPartitioner</code></p> <p>`Partition the indices of samples in the original dataset according to numbers of types of a particular attribute (e.g. label) . This way of partition is widely used by existing works in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>the number of clients</p> <code>100</code> <code>diversity</code> <code>float</code> <p>the ratio of locally owned types of the attributes (i.e. the actual number=diversity * total_num_of_types)</p> <code>1.0</code> <code>imbalance</code> <code>float</code> <p>the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)</p> required <code>flag_index</code> <code>int</code> <p>the index of the distribution-dependent (i.e. label) attribute in each sample.</p> <code>-1</code> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class DiversityPartitioner(BasicPartitioner):\n\"\"\"`Partition the indices of samples in the original dataset according to numbers of types of a particular\n    attribute (e.g. label) . This way of partition is widely used by existing works in federated learning.\n\n    Args:\n        num_clients (int, optional): the number of clients\n        diversity (float, optional): the ratio of locally owned types of the attributes (i.e. the actual number=diversity * total_num_of_types)\n        imbalance (float, optional): the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)\n        flag_index (int, optional): the index of the distribution-dependent (i.e. label) attribute in each sample.\n    \"\"\"\n    def __init__(self, num_clients=100, diversity=1.0, flag_index=-1):\n        self.num_clients = num_clients\n        self.diversity = diversity\n        self.flag_index = flag_index\n\n    def __str__(self):\n        name = \"div{:.1f}\".format(self.diversity)\n        return name\n\n    def __call__(self, data):\n        labels = [d[self.flag_index] for d in data]\n        num_classes = len(set(labels))\n        dpairs = [[did, lb] for did, lb in zip(list(range(len(data))), labels)]\n        num = max(int(self.diversity * num_classes), 1)\n        K = num_classes\n        local_datas = [[] for _ in range(self.num_clients)]\n        if num == K:\n            for k in range(K):\n                idx_k = [p[0] for p in dpairs if p[1] == k]\n                np.random.shuffle(idx_k)\n                split = np.array_split(idx_k, self.num_clients)\n                for cid in range(self.num_clients):\n                    local_datas[cid].extend(split[cid].tolist())\n        else:\n            times = [0 for _ in range(num_classes)]\n            contain = []\n            for i in range(self.num_clients):\n                current = []\n                j = 0\n                while (j &lt; num):\n                    mintime = np.min(times)\n                    ind = np.random.choice(np.where(times == mintime)[0])\n                    if (ind not in current):\n                        j = j + 1\n                        current.append(ind)\n                        times[ind] += 1\n                contain.append(current)\n            for k in range(K):\n                idx_k = [p[0] for p in dpairs if p[1] == k]\n                np.random.shuffle(idx_k)\n                split = np.array_split(idx_k, times[k])\n                ids = 0\n                for cid in range(self.num_clients):\n                    if k in contain[cid]:\n                        local_datas[cid].extend(split[ids].tolist())\n                        ids += 1\n        return local_datas\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.GaussianPerturbationPartitioner","title":"<code>GaussianPerturbationPartitioner</code>","text":"<p>         Bases: <code>BasicPartitioner</code></p> <p>`Partition the indices of samples I.I.D. and bind additional and static gaussian noise to each sample, which is a setting of feature skew in federated learning.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>the number of clients</p> <code>100</code> <code>imbalance</code> <code>float</code> <p>the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)</p> <code>0.0</code> <code>sigma</code> <code>float</code> <p>the degree of feature skew</p> <code>0.1</code> <code>scale</code> <code>float</code> <p>the standard deviation of noise</p> <code>0.1</code> <code>feature_index</code> <code>int</code> <p>the index of the feature to be processed for each sample.</p> <code>0</code> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class GaussianPerturbationPartitioner(BasicPartitioner):\n\"\"\"`Partition the indices of samples I.I.D. and bind additional and static gaussian noise to each sample, which is\n    a setting of feature skew in federated learning.\n\n    Args:\n        num_clients (int, optional): the number of clients\n        imbalance (float, optional): the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)\n        sigma (float, optional): the degree of feature skew\n        scale (float, optional): the standard deviation of noise\n        feature_index (int, optional): the index of the feature to be processed for each sample.\n    \"\"\"\n    def __init__(self, num_clients=100, imbalance=0.0, sigma=0.1, scale=0.1, feature_index=0):\n        self.num_clients = num_clients\n        self.imbalance = imbalance\n        self.sigma = sigma\n        self.scale = scale\n        self.feature_index = feature_index\n\n    def __str__(self):\n        name = \"perturb_gs{:.1f}_{:.1f}\".format(self.sigma, self.scale)\n        if self.imbalance &gt; 0: name += '_imb{:.1f}'.format(self.imbalance)\n        return name\n\n    def __call__(self, data):\n        shape = tuple(np.array(data[0][self.feature_index].shape))\n        samples_per_client = self.data_imbalance_generator(self.num_clients, len(data), self.imbalance)\n        d_idxs = np.random.permutation(len(data))\n        local_datas = np.split(d_idxs, np.cumsum(samples_per_client))[:-1]\n        local_datas = [di.tolist() for di in local_datas]\n        local_perturbation_means = [np.random.normal(0, self.sigma, shape) for _ in range(self.num_clients)]\n        local_perturbation_stds = [self.scale * np.ones(shape) for _ in range(self.num_clients)]\n        local_perturbation = []\n        for cid in range(self.num_clients):\n            c_perturbation = [np.random.normal(local_perturbation_means[cid], local_perturbation_stds[cid]).tolist() for\n                              _ in range(len(local_datas[cid]))]\n            local_perturbation.append(c_perturbation)\n        self.local_perturbation = local_perturbation\n        return local_datas\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.IDPartitioner","title":"<code>IDPartitioner</code>","text":"<p>         Bases: <code>BasicPartitioner</code></p> <p><code>Partition the indices of samples I.I.D. according to the ID of each sample, which requires the passed parameter</code>data<code>has attribution</code>id`.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>the number of clients</p> <code>-1</code> <code>priority</code> <code>str</code> <p>The value should be in set ('random', 'max', 'min'). If the number of clients is smaller than the total number of all the clients, this term will decide the selected clients according to their local data sizes.</p> <code>'random'</code> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class IDPartitioner(BasicPartitioner):\n\"\"\"`Partition the indices of samples I.I.D. according to the ID of each sample, which requires the passed parameter\n    `data` has attribution `id`.\n\n    Args:\n        num_clients (int, optional): the number of clients\n        priority (str, optional): The value should be in set ('random', 'max', 'min'). If the number of clients is smaller than the total number of all the clients, this term will decide the selected clients according to their local data sizes.\n    \"\"\"\n    def __init__(self, num_clients=-1, priority='random'):\n        self.num_clients = int(num_clients)\n        self.priorty = priority\n        return\n\n    def __str__(self):\n        return 'id'\n\n    def __call__(self, data):\n        all_data = list(range(len(data)))\n        data_owners = data.id\n        local_datas = collections.defaultdict(list)\n        for idx in range(len(all_data)):\n            local_datas[data_owners[idx]].append(all_data[idx])\n        local_datas = list(local_datas.values())\n        if self.num_clients &lt; 0:\n            self.num_clients = len(local_datas)\n        elif self.priorty == 'max':\n            local_datas = sorted(local_datas, key=lambda x: len('x'), reverse=True)[:self.num_clients]\n        elif self.priorty == 'min':\n            local_datas = sorted(local_datas, key=lambda x: len('x'))[:self.num_clients]\n        elif self.priorty == 'random':\n            random.shuffle(local_datas)\n            local_datas = local_datas[:self.num_clients]\n        return local_datas\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.IIDPartitioner","title":"<code>IIDPartitioner</code>","text":"<p>         Bases: <code>BasicPartitioner</code></p> <p>`Partition the indices of samples in the original dataset indentically and independently.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>the number of clients</p> <code>100</code> <code>imbalance</code> <code>float</code> <p>the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)</p> <code>0</code> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class IIDPartitioner(BasicPartitioner):\n\"\"\"`Partition the indices of samples in the original dataset indentically and independently.\n\n    Args:\n        num_clients (int, optional): the number of clients\n        imbalance (float, optional): the degree of imbalance of the amounts of different local data (0&lt;=imbalance&lt;=1)\n    \"\"\"\n    def __init__(self, num_clients=100, imbalance=0):\n        self.num_clients = num_clients\n        self.imbalance = imbalance\n\n    def __str__(self):\n        name = \"iid\"\n        if self.imbalance &gt; 0: name += '_imb{:.1f}'.format(self.imbalance)\n        return name\n\n    def __call__(self, data):\n        samples_per_client = self.data_imbalance_generator(self.num_clients, len(data), self.imbalance)\n        d_idxs = np.random.permutation(len(data))\n        local_datas = np.split(d_idxs, np.cumsum(samples_per_client))[:-1]\n        local_datas = [di.tolist() for di in local_datas]\n        return local_datas\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.VerticalSplittedPartitioner","title":"<code>VerticalSplittedPartitioner</code>","text":"<p>         Bases: <code>BasicPartitioner</code></p> <p>`Partition the indices and shapes of samples in the original dataset for vertical federated learning. Different to the above partitioners, the partitioner.call returns more flexible partition information instead of the indices that can be used to rebuild the partitioned data.</p> <p>Parameters:</p> Name Type Description Default <code>num_parties</code> <code>int</code> <p>the number of parties</p> <code>-1</code> <code>imbalance</code> <code>float</code> <p>the degree of imbalance of the number of features</p> <code>0</code> <code>dim</code> <code>int</code> <p>the dim of features to be partitioned</p> <code>-1</code> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>class VerticalSplittedPartitioner(BasicPartitioner):\n\"\"\"`Partition the indices and shapes of samples in the original dataset for vertical federated learning. Different\n    to the above partitioners, the partitioner.__call__ returns more flexible partition information instead of the indices that\n    can be used to rebuild the partitioned data.\n\n    Args:\n        num_parties (int, optional): the number of parties\n        imbalance (float, optional): the degree of imbalance of the number of features\n        dim (int, optional): the dim of features to be partitioned\n    \"\"\"\n    def __init__(self, num_parties=-1, imbalance=0, dim=-1):\n        self.num_parties = int(num_parties)\n        self.imbalance = imbalance\n        self.feature_pointers = []\n        self.dim = dim\n\n    def __str__(self):\n        return 'vertical_splitted_IBM{}'.format(self.imbalance)\n\n    def __call__(self, data):\n        local_datas = []\n        feature = data[0][0]\n        shape = feature.shape\n        if self.dim == -1: self.dim = int(np.argmax(shape))\n        self.num_parties = min(shape[self.dim], self.num_parties)\n        feature_sizes = self.gen_feature_size(shape[self.dim], self.num_parties, self.imbalance)\n        for pid in range(self.num_parties):\n            pdata = {'sample_idxs': list(range(len(data))), 'pt_feature': (self.dim, feature_sizes, pid),\n                     'with_label': (pid == 0)}\n            local_datas.append(pdata)\n        return local_datas\n\n    def gen_feature_size(self, total_size, num_parties, imbalance=0):\n        size_partitions = []\n        size_gen = self.integer_k_partition(total_size, num_parties)\n        while True:\n            try:\n                tmp = next(size_gen)\n                if tmp is not None:\n                    size_partitions.append(tmp)\n            except StopIteration:\n                break\n        size_partitions = sorted(size_partitions, key=lambda x: np.std(x))\n        res = size_partitions[int(imbalance * (len(size_partitions) - 1))]\n        return res\n\n    def integer_k_partition(self, n, k, l=1):\n'''n is the integer to partition, k is the length of partitions, l is the min partition element size'''\n        if k &lt; 1:\n            return None\n        if k == 1:\n            if n &gt;= l:\n                yield (n,)\n            return None\n        for i in range(l, n + 1):\n            for result in self.integer_k_partition(n - i, k - 1, i):\n                yield (i,) + result\n</code></pre>"},{"location":"Docs/benchmark/toolkits/parition/#flgo.benchmark.toolkits.partition.VerticalSplittedPartitioner.integer_k_partition","title":"<code>integer_k_partition(n, k, l=1)</code>","text":"<p>n is the integer to partition, k is the length of partitions, l is the min partition element size</p> Source code in <code>flgo\\benchmark\\toolkits\\partition.py</code> <pre><code>def integer_k_partition(self, n, k, l=1):\n'''n is the integer to partition, k is the length of partitions, l is the min partition element size'''\n    if k &lt; 1:\n        return None\n    if k == 1:\n        if n &gt;= l:\n            yield (n,)\n        return None\n    for i in range(l, n + 1):\n        for result in self.integer_k_partition(n - i, k - 1, i):\n            yield (i,) + result\n</code></pre>"},{"location":"Docs/benchmark/toolkits/visualization/","title":"flgo.benchmark.toolkits.visualization","text":""},{"location":"Docs/benchmark/toolkits/visualization/#flgo.benchmark.toolkits.visualization.visualize_by_class","title":"<code>visualize_by_class(generator, partitioner, task_path)</code>","text":"<p>Visualize the partitioned dataset and save the figure</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>flgo.benchmark.toolkits.BasicTaskGenerator</code> <p>task generator</p> required <code>partitioner</code> <code>flgo.benchmark.toolkits.partition.BasicPartitioner</code> <p>partitioner</p> required <code>task_path</code> <code>str</code> <p>the path storing the figure</p> required Source code in <code>flgo\\benchmark\\toolkits\\visualization.py</code> <pre><code>def visualize_by_class(generator, partitioner, task_path:str):\nr\"\"\"\n    Visualize the partitioned dataset and save the figure\n\n    Args:\n        generator (flgo.benchmark.toolkits.BasicTaskGenerator): task generator\n        partitioner (flgo.benchmark.toolkits.partition.BasicPartitioner): partitioner\n        task_path (str): the path storing the figure\n    \"\"\"\n    all_labels = [d[-1] for d in generator.train_data]\n    num_classes = len(set(all_labels))\n    ax = plt.subplots()\n    colors = [key for key in matplotlib.colors.CSS4_COLORS.keys()]\n    random.shuffle(colors)\n    client_height = 1\n    if hasattr(partitioner, 'dirichlet_dist'):\n        client_dist = generator.partitioner.dirichlet_dist.tolist()\n        data_columns = [sum(cprop) for cprop in client_dist]\n        row_map = {k: i for k, i in zip(np.argsort(data_columns), [_ for _ in range(generator.partitioner.num_parties)])}\n        for cid, cprop in enumerate(client_dist):\n            offset = 0\n            y_bottom = row_map[cid] - client_height / 2.0\n            y_top = row_map[cid] + client_height / 2.0\n            for lbi in range(len(cprop)):\n                plt.fill_between([offset, offset + cprop[lbi]], y_bottom, y_top, facecolor=colors[lbi])\n                # plt.barh(cid, cprop[lbi], client_height, left=offset, color=)\n                offset += cprop[lbi]\n    else:\n        data_columns = [len(cidx) for cidx in generator.local_datas]\n        row_map = {k: i for k, i in zip(np.argsort(data_columns), [_ for _ in range(generator.partitioner.num_parties)])}\n        for cid, cidxs in enumerate(generator.local_datas):\n            labels = [int(generator.train_data[did][-1]) for did in cidxs]\n            lb_counter = collections.Counter(labels)\n            offset = 0\n            y_bottom = row_map[cid] - client_height / 2.0\n            y_top = row_map[cid] + client_height / 2.0\n            for lbi in range(num_classes):\n                plt.fill_between([offset, offset + lb_counter[lbi]], y_bottom, y_top, facecolor=colors[lbi])\n                offset += lb_counter[lbi]\n    plt.xlim(0, max(data_columns))\n    plt.ylim(-0.5, generator.partitioner.num_parties - 0.5)\n    plt.ylabel('Client ID')\n    plt.xlabel('Number of Samples')\n    plt.savefig(os.path.join(task_path, 'res.png'))\n    plt.show()\n</code></pre>"},{"location":"Docs/benchmark/toolkits/cv/horizontal/image_classification/","title":"flgo.benchmark.toolkits.cv.horizontal.image_classification","text":""},{"location":"Docs/benchmark/toolkits/cv/horizontal/image_classification/#flgo.benchmark.toolkits.cv.horizontal.image_classification.BuiltinClassGenerator","title":"<code>BuiltinClassGenerator</code>","text":"<p>         Bases: <code>BasicTaskGenerator</code></p> <p>Generator for the dataset in torchvision.datasets.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>str</code> <p>the name of the benchmark</p> required <code>rawdata_path</code> <code>str</code> <p>the path storing the raw data</p> required <code>builtin_class</code> <code>class</code> <p>class in torchvision.datasets</p> required <code>transform</code> <code>torchvision.transforms.*</code> <p>the transform</p> <code>None</code> Source code in <code>flgo\\benchmark\\toolkits\\cv\\horizontal\\image_classification.py</code> <pre><code>class BuiltinClassGenerator(BasicTaskGenerator):\nr\"\"\"\n    Generator for the dataset in torchvision.datasets.\n\n    Args:\n        benchmark (str): the name of the benchmark\n        rawdata_path (str): the path storing the raw data\n        builtin_class (class): class in torchvision.datasets\n        transform (torchvision.transforms.*): the transform\n    \"\"\"\n    def __init__(self, benchmark, rawdata_path, builtin_class, transform=None):\n        super(BuiltinClassGenerator, self).__init__(benchmark, rawdata_path)\n        self.builtin_class = builtin_class\n        self.transform = transform\n        self.additional_option = {}\n        self.train_additional_option = {}\n        self.test_additional_option = {}\n\n    def load_data(self):\n        self.train_data = self.builtin_class(root=self.rawdata_path, download=True, train=True, transform=self.transform)\n        self.test_data = self.builtin_class(root=self.rawdata_path, download=True, train=False, transform=self.transform)\n\n    def partition(self):\n        self.local_datas = self.partitioner(self.train_data)\n        self.num_clients = len(self.local_datas)\n</code></pre>"},{"location":"Docs/benchmark/toolkits/cv/horizontal/image_classification/#flgo.benchmark.toolkits.cv.horizontal.image_classification.BuiltinClassPipe","title":"<code>BuiltinClassPipe</code>","text":"<p>         Bases: <code>BasicTaskPipe</code></p> <p>TaskPipe for the dataset in torchvision.datasets.</p> <p>Parameters:</p> Name Type Description Default <code>task_path</code> <code>str</code> <p>the path of the task</p> required <code>builtin_class</code> <code>class</code> <p>class in torchvision.datasets</p> required <code>transform</code> <code>torchvision.transforms.*</code> <p>the transform</p> <code>None</code> Source code in <code>flgo\\benchmark\\toolkits\\cv\\horizontal\\image_classification.py</code> <pre><code>class BuiltinClassPipe(BasicTaskPipe):\nr\"\"\"\n    TaskPipe for the dataset in torchvision.datasets.\n\n    Args:\n        task_path (str): the path of the task\n        builtin_class (class): class in torchvision.datasets\n        transform (torchvision.transforms.*): the transform\n    \"\"\"\n    class TaskDataset(torch.utils.data.Subset):\n        def __init__(self, dataset, indices, perturbation=None, pin_memory=False):\n            super().__init__(dataset, indices)\n            self.dataset = dataset\n            self.indices = indices\n            self.perturbation = {idx:p for idx, p in zip(indices, perturbation)} if perturbation is not None else None\n            self.pin_memory = pin_memory\n            if not self.pin_memory:\n                self.X = None\n                self.Y = None\n            else:\n                self.X = torch.stack([self.dataset[i][0] for i in self.indices])\n                self.Y = torch.LongTensor([self.dataset[i][1] for i in self.indices])\n\n        def __getitem__(self, idx):\n            if self.X is not None:\n                if self.perturbation is None:\n                    return self.X[idx], self.Y[idx]\n                else:\n                    return self.X[idx]+self.perturbation[self.indices[idx]], self.Y[idx]\n            else:\n                if self.perturbation is None:\n                    if isinstance(idx, list):\n                        return self.dataset[[self.indices[i] for i in idx]]\n                    return self.dataset[self.indices[idx]]\n                else:\n                    return self.dataset[self.indices[idx]][0] + self.perturbation[self.indices[idx]],  self.dataset[self.indices[idx]][1]\n\n    def __init__(self, task_path, buildin_class, transform=None):\n        super(BuiltinClassPipe, self).__init__(task_path)\n        self.builtin_class = buildin_class\n        self.transform = transform\n\n    def save_task(self, generator):\n        client_names = self.gen_client_names(len(generator.local_datas))\n        feddata = {'client_names': client_names, 'server_data': list(range(len(generator.test_data))),  'rawdata_path': generator.rawdata_path, 'additional_option': generator.additional_option, 'train_additional_option':generator.train_additional_option, 'test_additional_option':generator.test_additional_option,}\n        for cid in range(len(client_names)): feddata[client_names[cid]] = {'data': generator.local_datas[cid],}\n        if hasattr(generator.partitioner, 'local_perturbation'): feddata['local_perturbation'] = generator.partitioner.local_perturbation\n        with open(os.path.join(self.task_path, 'data.json'), 'w') as outf:\n            json.dump(feddata, outf)\n        return\n\n    def load_data(self, running_time_option) -&gt; dict:\n        # load the datasets\n        train_default_init_para = {'root': self.feddata['rawdata_path'], 'download':True, 'train':True, 'transform':self.transform}\n        test_default_init_para = {'root': self.feddata['rawdata_path'], 'download':True, 'train':False, 'transform':self.transform}\n        if 'additional_option' in self.feddata.keys():\n            train_default_init_para.update(self.feddata['additional_option'])\n            test_default_init_para.update(self.feddata['additional_option'])\n        if 'train_additional_option' in self.feddata.keys(): train_default_init_para.update(self.feddata['train_additional_option'])\n        if 'test_additional_option' in self.feddata.keys(): test_default_init_para.update(self.feddata['test_additional_option'])\n        train_pop_key = [k for k in train_default_init_para.keys() if k not in self.builtin_class.__init__.__annotations__]\n        test_pop_key = [k for k in test_default_init_para.keys() if k not in self.builtin_class.__init__.__annotations__]\n        for k in train_pop_key: train_default_init_para.pop(k)\n        for k in test_pop_key: test_default_init_para.pop(k)\n        train_data = self.builtin_class(**train_default_init_para)\n        test_data = self.builtin_class(**test_default_init_para)\n        test_data = self.TaskDataset(test_data, list(range(len(test_data))), None, running_time_option['pin_memory'])\n        # rearrange data for server\n        server_data_test, server_data_valid = self.split_dataset(test_data, running_time_option['test_holdout'])\n        task_data = {'server': {'test': server_data_test, 'valid': server_data_valid}}\n        # rearrange data for clients\n        local_perturbation = self.feddata['local_perturbation'] if 'local_perturbation' in self.feddata.keys() else [None for _ in self.feddata['client_names']]\n        for cid, cname in enumerate(self.feddata['client_names']):\n            cpert = None if  local_perturbation[cid] is None else [torch.tensor(t) for t in local_perturbation[cid]]\n            cdata = self.TaskDataset(train_data, self.feddata[cname]['data'], cpert, running_time_option['pin_memory'])\n            cdata_train, cdata_valid = self.split_dataset(cdata, running_time_option['train_holdout'])\n            if running_time_option['train_holdout']&gt;0 and running_time_option['local_test']:\n                cdata_valid, cdata_test = self.split_dataset(cdata_valid, 0.5)\n            else:\n                cdata_test = None\n            task_data[cname] = {'train':cdata_train, 'valid':cdata_valid, 'test': cdata_test}\n        return task_data\n</code></pre>"},{"location":"Docs/benchmark/toolkits/cv/horizontal/image_classification/#flgo.benchmark.toolkits.cv.horizontal.image_classification.GeneralCalculator","title":"<code>GeneralCalculator</code>","text":"<p>         Bases: <code>BasicTaskCalculator</code></p> <p>Calculator for the dataset in torchvision.datasets.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>torch.device</code> <p>device</p> required <code>optimizer_name</code> <code>str</code> <p>the name of the optimizer</p> <code>'sgd'</code> Source code in <code>flgo\\benchmark\\toolkits\\cv\\horizontal\\image_classification.py</code> <pre><code>class GeneralCalculator(BasicTaskCalculator):\nr\"\"\"\n    Calculator for the dataset in torchvision.datasets.\n\n    Args:\n        device (torch.device): device\n        optimizer_name (str): the name of the optimizer\n    \"\"\"\n    def __init__(self, device, optimizer_name='sgd'):\n        super(GeneralCalculator, self).__init__(device, optimizer_name)\n        self.criterion = torch.nn.CrossEntropyLoss()\n        self.DataLoader = torch.utils.data.DataLoader\n\n    def compute_loss(self, model, data):\n\"\"\"\n        Args:\n            model: the model to train\n            data: the training dataset\n        Returns: dict of train-one-step's result, which should at least contains the key 'loss'\n        \"\"\"\n        tdata = self.to_device(data)\n        outputs = model(tdata[0])\n        loss = self.criterion(outputs, tdata[-1])\n        return {'loss': loss}\n\n    @torch.no_grad()\n    def test(self, model, dataset, batch_size=64, num_workers=0, pin_memory=False):\n\"\"\"\n        Metric = [mean_accuracy, mean_loss]\n\n        Args:\n            model:\n            dataset:\n            batch_size:\n        Returns: [mean_accuracy, mean_loss]\n        \"\"\"\n        model.eval()\n        if batch_size==-1:batch_size=len(dataset)\n        data_loader = self.get_dataloader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n        total_loss = 0.0\n        num_correct = 0\n        for batch_id, batch_data in enumerate(data_loader):\n            batch_data = self.to_device(batch_data)\n            outputs = model(batch_data[0])\n            batch_mean_loss = self.criterion(outputs, batch_data[-1]).item()\n            y_pred = outputs.data.max(1, keepdim=True)[1]\n            correct = y_pred.eq(batch_data[-1].data.view_as(y_pred)).long().cpu().sum()\n            num_correct += correct.item()\n            total_loss += batch_mean_loss * len(batch_data[-1])\n        return {'accuracy': 1.0*num_correct/len(dataset), 'loss':total_loss/len(dataset)}\n\n    def to_device(self, data):\n        return data[0].to(self.device), data[1].to(self.device)\n\n    def get_dataloader(self, dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=False, drop_last=False):\n        if self.DataLoader == None:\n            raise NotImplementedError(\"DataLoader Not Found.\")\n        return self.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, pin_memory=pin_memory, drop_last=drop_last)\n</code></pre>"},{"location":"Docs/benchmark/toolkits/cv/horizontal/image_classification/#flgo.benchmark.toolkits.cv.horizontal.image_classification.GeneralCalculator.compute_loss","title":"<code>compute_loss(model, data)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>model</code> <p>the model to train</p> required <code>data</code> <p>the training dataset</p> required Source code in <code>flgo\\benchmark\\toolkits\\cv\\horizontal\\image_classification.py</code> <pre><code>def compute_loss(self, model, data):\n\"\"\"\n    Args:\n        model: the model to train\n        data: the training dataset\n    Returns: dict of train-one-step's result, which should at least contains the key 'loss'\n    \"\"\"\n    tdata = self.to_device(data)\n    outputs = model(tdata[0])\n    loss = self.criterion(outputs, tdata[-1])\n    return {'loss': loss}\n</code></pre>"},{"location":"Docs/benchmark/toolkits/cv/horizontal/image_classification/#flgo.benchmark.toolkits.cv.horizontal.image_classification.GeneralCalculator.test","title":"<code>test(model, dataset, batch_size=64, num_workers=0, pin_memory=False)</code>","text":"<p>Metric = [mean_accuracy, mean_loss]</p> <p>Parameters:</p> Name Type Description Default <code>model</code> required <code>dataset</code> required <code>batch_size</code> <code>64</code> Source code in <code>flgo\\benchmark\\toolkits\\cv\\horizontal\\image_classification.py</code> <pre><code>@torch.no_grad()\ndef test(self, model, dataset, batch_size=64, num_workers=0, pin_memory=False):\n\"\"\"\n    Metric = [mean_accuracy, mean_loss]\n\n    Args:\n        model:\n        dataset:\n        batch_size:\n    Returns: [mean_accuracy, mean_loss]\n    \"\"\"\n    model.eval()\n    if batch_size==-1:batch_size=len(dataset)\n    data_loader = self.get_dataloader(dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory)\n    total_loss = 0.0\n    num_correct = 0\n    for batch_id, batch_data in enumerate(data_loader):\n        batch_data = self.to_device(batch_data)\n        outputs = model(batch_data[0])\n        batch_mean_loss = self.criterion(outputs, batch_data[-1]).item()\n        y_pred = outputs.data.max(1, keepdim=True)[1]\n        correct = y_pred.eq(batch_data[-1].data.view_as(y_pred)).long().cpu().sum()\n        num_correct += correct.item()\n        total_loss += batch_mean_loss * len(batch_data[-1])\n    return {'accuracy': 1.0*num_correct/len(dataset), 'loss':total_loss/len(dataset)}\n</code></pre>"},{"location":"Docs/experiment/","title":"Index","text":""},{"location":"Docs/experiment/#flgo.experiment","title":"<code>flgo.experiment</code>","text":"<p>This module is created for various experimental purposes</p>"},{"location":"Docs/experiment/analyzer/","title":"flgo.experiment.analyzer","text":"<p>This module is to analyze the training results saved by Logger. To use this module, a analysis plan should be designed (i.e. dict):     Selector: select the records according to the task, algorithm and options of the task     Painter: draw graphic of the selected records     Table: output some statistic of the selected records on the console</p> <p>The basic usage is to build a plan dict and pass it to flgo.experiment.analyzer</p> <p>The following three examples show how to build a customized plan:</p> How to define a Selector? <p>{'Selector': {     'task': task_path,                # all the analysis will be conducted on a single task     'header': ['fedavg'],             # only the records where the names of algorithms are in <code>header</code> will be selected      'filter': {'LR':'&lt;0.1'}          # only the records whose options satisfy the conditions in <code>filter</code> will be selected     'legend_with': ['LR', 'B', 'E']   # all the graphic will show the legends of records according to <code>legend_with</code> }, ...}</p> How to define a Painter? <p>Each <code>Painter</code> is a dict of different types of graphic (e.g. Curve, Bar and Scatter). In each types of graphic, the value is a list of figures, where each figure is defined by a dict like {'args':{...}, 'obj_option':{}, 'fig_option':{...}}</p> <pre><code>{...,\n'Painter':{\n        'Curve':[\n            {'args':{'x':'communication_round', 'y':'valid_loss'}, },\n            {...}\n        ]\n    },\n...,\n}\n</code></pre> How to define a Table? <p>{..., 'Table':{         'min_value':[             {'x':'valid_loss'},             ...             ]     } }</p> <p>A standard analysis plan usually consists of the above three parts, and <code>Painter</code> and <code>Table</code> are both optional</p>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer--plan-selector-painter-table","title":"plan = {'Selector':..., 'Painter':..., 'Table':...,}","text":"<p>flgo.experiment.analyzer.show(plan)</p>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Bar","title":"<code>Bar</code>","text":"<p>         Bases: <code>PaintObject</code></p> <p>Bar Object</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Bar(PaintObject):\n\"\"\"Bar Object\"\"\"\n    def __init__(self, rec, args,  obj_option):\n        super(Bar, self).__init__(rec, args, obj_option, 'bar')\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Curve","title":"<code>Curve</code>","text":"<p>         Bases: <code>PaintObject</code></p> <p>Curve Object</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Curve(PaintObject):\n\"\"\"Curve Object\"\"\"\n    def __init__(self, rec, args,  obj_option):\n        super(Curve, self).__init__(rec, args, obj_option, 'plot')\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.GroupCurve","title":"<code>GroupCurve</code>","text":"<p>         Bases: <code>PaintObject</code></p> <p>Group Curve Object</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class GroupCurve(PaintObject):\n\"\"\"Group Curve Object\"\"\"\n    def __init__(self, rec, args,  obj_option):\n        super(GroupCurve, self).__init__(rec, args, obj_option, '')\n\n    def draw(self, ax):\n        x = self.rec.data[self.args['x']]\n        ykey = self.args['y']\n        mean_y = self.rec.data[ykey]\n        min_y = np.min(np.array([d[ykey] for d in self.rec.datas]), axis=0)\n        max_y = np.max(np.array([d[ykey] for d in self.rec.datas]), axis=0)\n        ax.plot(x, mean_y, label=self.rec.data['label'])\n        ax.fill_between(x, max_y, min_y, alpha=0.3)\n        ax.legend()\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.PaintObject","title":"<code>PaintObject</code>","text":"<p>The basic PaintObject. Each PaintObject should inherent from this class. And the method self.draw should be overwritten if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>Record</code> <p>the record</p> required <code>args</code> <code>dict</code> <p>the painting arguments</p> required <code>obj_option</code> <code>dict</code> <p>the personal option for each object</p> required <code>draw_func</code> <code>str</code> <p>optional, the function name. All the subclass of this class won't claim this parameter.</p> required <p>Example:</p> <pre><code>    &gt;&gt;&gt; class GroupCurve(PaintObject):\n    ...     def __init__(self, rec, args,  obj_option):\n    ...         super(GroupCurve, self).__init__(rec, args, obj_option, '')\n    ...\n    ...     def draw(self, ax):\n    ...         x = self.rec.data[self.args['x']]\n    ...         ykey = self.args['y']\n    ...         mean_y = self.rec.data[ykey]\n    ...         min_y = np.min(np.array([d[ykey] for d in self.rec.datas]), axis=0)\n    ...         max_y = np.max(np.array([d[ykey] for d in self.rec.datas]), axis=0)\n    ...         ax.plot(x, mean_y, label=self.rec.data['label'])\n    ...         ax.fill_between(x, max_y, min_y, alpha=0.3)\n    ...         ax.legend()\n</code></pre> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class PaintObject:\nr\"\"\"\n    The basic PaintObject. Each PaintObject should inherent from this class.\n    And the method self.draw should be overwritten if necessary.\n\n    Args:\n        rec (Record): the record\n        args (dict): the painting arguments\n        obj_option (dict): the personal option for each object\n        draw_func (str): optional, the function name. All the subclass of this class won't claim this parameter.\n\n    Example:\n    ```python\n        &gt;&gt;&gt; class GroupCurve(PaintObject):\n        ...     def __init__(self, rec, args,  obj_option):\n        ...         super(GroupCurve, self).__init__(rec, args, obj_option, '')\n        ...\n        ...     def draw(self, ax):\n        ...         x = self.rec.data[self.args['x']]\n        ...         ykey = self.args['y']\n        ...         mean_y = self.rec.data[ykey]\n        ...         min_y = np.min(np.array([d[ykey] for d in self.rec.datas]), axis=0)\n        ...         max_y = np.max(np.array([d[ykey] for d in self.rec.datas]), axis=0)\n        ...         ax.plot(x, mean_y, label=self.rec.data['label'])\n        ...         ax.fill_between(x, max_y, min_y, alpha=0.3)\n        ...         ax.legend()\n    ```\n    \"\"\"\n    def __init__(self, rec: Record, args: dict,  obj_option: dict, draw_func: str):\n        self.rec = rec\n        self.args = args\n        self.obj_option = obj_option\n        self.draw_func = draw_func\n        self.para = (rec.data[v] for v in args.values())\n        self.with_legend = True\n\n    def draw(self, ax):\n        if 'label' in self.obj_option.keys() or 'label' not in self.rec.data.keys():\n            eval('ax.'+str(self.draw_func)+'(*self.para, **self.obj_option)')\n        else:\n            eval('ax.' + str(self.draw_func) + '(*self.para, **self.obj_option, label=self.rec.data[\"label\"])')\n        if self.with_legend: eval('ax.legend()')\n        return\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Painter","title":"<code>Painter</code>","text":"<p>Draw the information in records into figures</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list</code> <p>a list of instances of Record(...)</p> required <code>save_text</code> <code>bool</code> <p>whether to store the figures into the disk</p> required <code>path</code> <code>str</code> <p>the storing path</p> <code>'.'</code> <code>format</code> <code>str</code> <p>the storing format</p> <code>'png'</code> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Painter:\nr\"\"\"\n    Draw the information in records into figures\n\n    Args:\n        records (list): a list of instances of Record(...)\n        save_text (bool): whether to store the figures into the disk\n        path (str): the storing path\n        format (str): the storing format\n    \"\"\"\n    def __init__(self, records: list, save_figure=False, path:str='.', format='png'):\n        self.records = records\n        self.save_figure = save_figure\n        self.path = path\n        self.format = format\n\n    def create_figure(self, object_class, fig_config):\nr\"\"\"\n        Create figure according to the PaintObject and figure configurations.\n        For each record k, a PaintObject(record, object_option) will be created\n        for later drawing. Then, a figure will be created by fig_option and all \n        the PaintObject will be put onto the figure. \n        The fig_config should be a dict like:\n            {\n                'args':{...}, # ploting arguments for each record\n                'obj_option':{...}, # assign each PaintObject with different attributes like color, label...\n                'fig_option':{...}, # the options of the figure such as title, xlabel, xlim, no_legend\n            }\n\n        Args:\n            object_class (class|str): the types of the obejct to be drawed\n            fig_config (dict): the drawing configuration\n\n        Example:\n        ```python\n            &gt;&gt;&gt; p=Painter(records)\n            &gt;&gt;&gt; p.create_figure(Curve, {'args':{'x':'communication_round', 'y':'valid_loss'}})\n        ```\n        \"\"\"\n        object_class = eval(object_class) if type(object_class) is str else object_class\n        if 'split' in  fig_config.keys():\n            cols = fig_config['split']['cols'] if 'cols' in fig_config['split'] else 4\n            rows = int(math.ceil(len(self.records)/cols))\n            cols = min(len(self.records), cols)\n            if 'figsize' in fig_config['split']:\n                new_fig_size = (fig_config['split']['figsize'][0], fig_config['split']['figsize'][1])\n            else:\n                fig_size = mpl.rcParams['figure.figsize']\n                new_fig_size = (fig_size[0] * cols, fig_size[1] * rows)\n            fig, axs = plt.subplots(rows, cols, figsize=new_fig_size)\n            if type(axs) is np.ndarray:\n                axs = axs.reshape(-1)\n            else:\n                axs = [axs]\n        else:\n            fig, ax = plt.subplots()\n            axs = [ax for _ in self.records]\n        args = fig_config['args']\n        obj_options = self._generate_obj_option(fig_config['obj_option']) if 'obj_option' in fig_config.keys() else [{} for _ in self.records]\n        objects = [object_class(rec, args, obj_option) for rec, obj_option in zip(self.records, obj_options)]\n        for ob,axi in zip(objects, axs):\n            ob.draw(axi)\n        if 'fig_option' in fig_config.keys():\n            if 'no_legend' in fig_config['fig_option'].keys():\n                for obj in objects: obj.with_legend = False\n            for option_name in fig_config['fig_option']:\n                if option_name=='no_legend': continue\n                if 'split' in fig_config.keys():\n                    if type(fig_config['fig_option'][option_name]) is str:\n                        for ax in axs:\n                            eval('ax.set_'+option_name+\"('{}')\".format(fig_config['fig_option'][option_name]))\n                    else:\n                        for ax in axs:\n                            eval('ax.set_'+option_name+\"({})\".format(fig_config['fig_option'][option_name]))\n                else:\n                    if type(fig_config['fig_option'][option_name]) is str:\n                        eval('plt.'+option_name+\"('{}')\".format(fig_config['fig_option'][option_name]))\n                    else:\n                        eval('plt.' + option_name + \"({})\".format(fig_config['fig_option'][option_name]))\n        filename = None\n        if self.save_figure:\n            filename = str(uuid.uuid4())+'.'+self.format\n            plt.savefig(os.path.join(self.path, filename))\n        plt.show()\n        return filename\n\n    def _generate_obj_option(self, raw_obj_option: dict):\n        for k in raw_obj_option:\n            if type(raw_obj_option[k]) is list:\n                assert len(raw_obj_option[k]) &gt;= len(self.records)\n                raw_obj_option[k] = raw_obj_option[k][:len(self.records)]\n            else:\n                raw_obj_option[k] = [raw_obj_option[k] for _ in self.records]\n        return [{k:v[i] for k,v in raw_obj_option.items()} for i in range(len(self.records))]\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Painter.create_figure","title":"<code>create_figure(object_class, fig_config)</code>","text":"<p>Create figure according to the PaintObject and figure configurations. For each record k, a PaintObject(record, object_option) will be created for later drawing. Then, a figure will be created by fig_option and all  the PaintObject will be put onto the figure. </p> The fig_config should be a dict like <p>{     'args':{...}, # ploting arguments for each record     'obj_option':{...}, # assign each PaintObject with different attributes like color, label...     'fig_option':{...}, # the options of the figure such as title, xlabel, xlim, no_legend }</p> <p>Parameters:</p> Name Type Description Default <code>object_class</code> <code>class|str</code> <p>the types of the obejct to be drawed</p> required <code>fig_config</code> <code>dict</code> <p>the drawing configuration</p> required <p>Example:</p> <pre><code>    &gt;&gt;&gt; p=Painter(records)\n    &gt;&gt;&gt; p.create_figure(Curve, {'args':{'x':'communication_round', 'y':'valid_loss'}})\n</code></pre> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def create_figure(self, object_class, fig_config):\nr\"\"\"\n    Create figure according to the PaintObject and figure configurations.\n    For each record k, a PaintObject(record, object_option) will be created\n    for later drawing. Then, a figure will be created by fig_option and all \n    the PaintObject will be put onto the figure. \n    The fig_config should be a dict like:\n        {\n            'args':{...}, # ploting arguments for each record\n            'obj_option':{...}, # assign each PaintObject with different attributes like color, label...\n            'fig_option':{...}, # the options of the figure such as title, xlabel, xlim, no_legend\n        }\n\n    Args:\n        object_class (class|str): the types of the obejct to be drawed\n        fig_config (dict): the drawing configuration\n\n    Example:\n    ```python\n        &gt;&gt;&gt; p=Painter(records)\n        &gt;&gt;&gt; p.create_figure(Curve, {'args':{'x':'communication_round', 'y':'valid_loss'}})\n    ```\n    \"\"\"\n    object_class = eval(object_class) if type(object_class) is str else object_class\n    if 'split' in  fig_config.keys():\n        cols = fig_config['split']['cols'] if 'cols' in fig_config['split'] else 4\n        rows = int(math.ceil(len(self.records)/cols))\n        cols = min(len(self.records), cols)\n        if 'figsize' in fig_config['split']:\n            new_fig_size = (fig_config['split']['figsize'][0], fig_config['split']['figsize'][1])\n        else:\n            fig_size = mpl.rcParams['figure.figsize']\n            new_fig_size = (fig_size[0] * cols, fig_size[1] * rows)\n        fig, axs = plt.subplots(rows, cols, figsize=new_fig_size)\n        if type(axs) is np.ndarray:\n            axs = axs.reshape(-1)\n        else:\n            axs = [axs]\n    else:\n        fig, ax = plt.subplots()\n        axs = [ax for _ in self.records]\n    args = fig_config['args']\n    obj_options = self._generate_obj_option(fig_config['obj_option']) if 'obj_option' in fig_config.keys() else [{} for _ in self.records]\n    objects = [object_class(rec, args, obj_option) for rec, obj_option in zip(self.records, obj_options)]\n    for ob,axi in zip(objects, axs):\n        ob.draw(axi)\n    if 'fig_option' in fig_config.keys():\n        if 'no_legend' in fig_config['fig_option'].keys():\n            for obj in objects: obj.with_legend = False\n        for option_name in fig_config['fig_option']:\n            if option_name=='no_legend': continue\n            if 'split' in fig_config.keys():\n                if type(fig_config['fig_option'][option_name]) is str:\n                    for ax in axs:\n                        eval('ax.set_'+option_name+\"('{}')\".format(fig_config['fig_option'][option_name]))\n                else:\n                    for ax in axs:\n                        eval('ax.set_'+option_name+\"({})\".format(fig_config['fig_option'][option_name]))\n            else:\n                if type(fig_config['fig_option'][option_name]) is str:\n                    eval('plt.'+option_name+\"('{}')\".format(fig_config['fig_option'][option_name]))\n                else:\n                    eval('plt.' + option_name + \"({})\".format(fig_config['fig_option'][option_name]))\n    filename = None\n    if self.save_figure:\n        filename = str(uuid.uuid4())+'.'+self.format\n        plt.savefig(os.path.join(self.path, filename))\n    plt.show()\n    return filename\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Record","title":"<code>Record</code>","text":"<p>Read the record that is stored by each runner into the memory according to the task and the name.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>the path of the task</p> required <code>name</code> <code>str</code> <p>the name of the saved record</p> required Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Record:\nr\"\"\"\n    Read the record that is stored by each runner into the memory according\n    to the task and the name.\n\n    Args:\n        task (str): the path of the task\n        name (str): the name of the saved record\n    \"\"\"\n    def __init__(self, task, name):\n        self.task = task\n        self.name = name\n        self.rec_path = os.path.join(task, 'record', name)\n        with open(self.rec_path, 'r') as inf:\n            s_inf = inf.read()\n            rec = json.loads(s_inf)\n        self.data = rec\n        self.datas = [self.data]\n        self.set_communication_round()\n        self.set_client_id()\n\n    def set_communication_round(self):\n        num_rounds = self.data['option']['num_rounds']\n        eval_interval = self.data['option']['eval_interval']\n        x = [0]\n        for round in range(1, num_rounds + 1):\n            if eval_interval &gt; 0 and (round == 0 or round % eval_interval == 0):\n                x.append(round)\n            if self.data['option']['early_stop'] &gt; 0 and 'valid_loss' in self.data.keys() and len(x) &gt;= len(self.data['valid_loss']):\n                break\n        self.data['communication_round'] = x\n\n    def set_client_id(self):\n        with open(os.path.join(self.task, 'info')) as inf:\n            task_info = json.load(inf)\n            if 'num_clients' in task_info.keys():\n                N = int(task_info['num_clients'])\n            elif 'num_parties' in task_info.keys():\n                N = int(task_info['num_parties'])\n            else:\n                N = 0\n        self.data['client_id'] = [cid for cid in range(N)]\n\n    def set_legend(self, legend_with = []):\n        if len(legend_with)==0: self.data['label'] = []\n        self.data['label'] = [self.name[:self.name.find('_M')]]\n        for key in legend_with:\n            val = key + self.get_key_from_name(key)\n            self.data['label'].append(val)\n        self.data['label'] = ' '.join(self.data['label'])\n\n    def get_key_from_name(self, key):\n        if key == '': return ''\n        value_start = self.name.find('_' + key) + len(key) + 1\n        value_end = self.name.find('_', value_start)\n        return self.name[value_start:value_end]\n\n    @classmethod\n    def create_group(cls, rec_list: list):\nr\"\"\"\n        Organize the records in rec_list into a group-level Record,\n        where there will be a new attribute named Record.datas. And\n        the values in Record.data will be replaced by the mean values\n        of that in Record.datas\n\n        Args:\n            rec_list (list): a list of Record(...)\n\n        Returns:\n            a new group-level Record\n        \"\"\"\n        if len(rec_list) == 0: return None\n        r = copy.deepcopy(rec_list[0])\n        r.datas = [rec.data for rec in rec_list]\n        for key in r.data.keys():\n            if key == 'option': continue\n            try:\n                if type(r.data[key]) is list:\n                    ave_data = np.array([np.array(rdata[key]) for rdata in r.datas])\n                    r.data[key] = ave_data.mean(axis=0)\n            except:\n                continue\n        return r\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Record.create_group","title":"<code>create_group(rec_list)</code>  <code>classmethod</code>","text":"<p>Organize the records in rec_list into a group-level Record, where there will be a new attribute named Record.datas. And the values in Record.data will be replaced by the mean values of that in Record.datas</p> <p>Parameters:</p> Name Type Description Default <code>rec_list</code> <code>list</code> <p>a list of Record(...)</p> required <p>Returns:</p> Type Description <p>a new group-level Record</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>@classmethod\ndef create_group(cls, rec_list: list):\nr\"\"\"\n    Organize the records in rec_list into a group-level Record,\n    where there will be a new attribute named Record.datas. And\n    the values in Record.data will be replaced by the mean values\n    of that in Record.datas\n\n    Args:\n        rec_list (list): a list of Record(...)\n\n    Returns:\n        a new group-level Record\n    \"\"\"\n    if len(rec_list) == 0: return None\n    r = copy.deepcopy(rec_list[0])\n    r.datas = [rec.data for rec in rec_list]\n    for key in r.data.keys():\n        if key == 'option': continue\n        try:\n            if type(r.data[key]) is list:\n                ave_data = np.array([np.array(rdata[key]) for rdata in r.datas])\n                r.data[key] = ave_data.mean(axis=0)\n        except:\n            continue\n    return r\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Scatter","title":"<code>Scatter</code>","text":"<p>         Bases: <code>PaintObject</code></p> <p>Scatter Obejct</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Scatter(PaintObject):\n\"\"\"Scatter Obejct\"\"\"\n    def __init__(self, rec, args,  obj_option):\n        super(Scatter, self).__init__(rec, args, obj_option, 'scatter')\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Selector","title":"<code>Selector</code>","text":"<p>Filter the records and read them into memory accoring to customized settings</p> <p>Parameters:</p> Name Type Description Default <code>selector_config</code> <code>dict</code> <p>the dictionary that is used to filter records</p> required <p>Example:</p> <pre><code>    &gt;&gt;&gt; task='./my_task'\n    &gt;&gt;&gt; selector = Selector({'task':task, 'header':['fedavg'], 'filter':{'lr':0.1}})\n    &gt;&gt;&gt; selector.records[task]\n    &gt;&gt;&gt; # selector.records is a dict where selector.records[task] is a list\n    &gt;&gt;&gt; # of the records that pass the filter\n</code></pre> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Selector:\nr\"\"\"\n    Filter the records and read them into memory accoring to customized settings\n\n    Args:\n        selector_config (dict): the dictionary that is used to filter records\n\n    Example:\n    ```python\n        &gt;&gt;&gt; task='./my_task'\n        &gt;&gt;&gt; selector = Selector({'task':task, 'header':['fedavg'], 'filter':{'lr':0.1}})\n        &gt;&gt;&gt; selector.records[task]\n        &gt;&gt;&gt; # selector.records is a dict where selector.records[task] is a list\n        &gt;&gt;&gt; # of the records that pass the filter\n    ```\n    \"\"\"\n    def __init__(self, selector_config):\n        self.config = selector_config\n        self.tasks = [selector_config['task']] if type(selector_config['task']) is not list else selector_config['task']\n        self.headers = selector_config['header'] if type(selector_config['header']) is list else [selector_config['header']]\n        self.filter = selector_config['filter'] if 'filter' in selector_config.keys() else {}\n        self.legend_with = selector_config['legend_with'] if 'legend_with' in selector_config.keys() else []\n        self.rec_names = self.scan()\n        self.records = self.read_records(self.rec_names)\n        tmp = list(self.records.values())\n        self.all_records = []\n        for ti in tmp: self.all_records.extend(ti)\n        try:\n            self.grouped_records, self.group_names, = self.group_records()\n        except Exception() as e:\n            print(e)\n\n    def scan(self):\n        res = {}\n        for task in self.tasks:\n            path = os.path.join(task, 'record')\n            all_records = os.listdir(path)\n            tmp = []\n            # check headers\n            for header in self.headers:\n                tmp.extend([f for f in all_records if f.startswith(header) and f.endswith('.json')])\n            res[task] = self.filename_filter(tmp, self.filter)\n        return res\n\n    def filename_filter(self, fnames, filter):\n        if len(filter)==0: return fnames\n        for key in filter.keys():\n            condition = filter[key]\n            res = []\n            for f in fnames:\n                if f.find('_'+key)==-1: continue\n                fv = f[f.find('_' + key) + len(key) + 1:f.find('_', f.find('_' + key) + 1)]\n                if type(condition) is list:\n                    fv = float(fv) if ('0' &lt;= fv[0] &lt;= '9' or fv[0] == '.' or fv[0] == '-') else fv\n                    if fv in condition: res.append(f)\n                elif type(condition) is str:\n                    con = (fv+condition) if condition[0] in ['&lt;', '&gt;', '='] else (fv+'=='+condition)\n                    if eval(con): res.append(f)\n                else:\n                    if float(fv)==float(condition): res.append(f)\n            fnames = res\n        return fnames\n\n    def get_key_from_filename(self, filename, key):\n        if key == '': return ''\n        value_start = filename.find('_' + key) + len(key) + 1\n        value_end = filename.find('_', value_start)\n        return filename[value_start:value_end]\n\n    def read_records(self, rec_names):\n        res = {task: [] for task in rec_names}\n        for task in rec_names:\n            path = os.path.join(task, 'record')\n            files = os.listdir(path)\n            for record_name in rec_names[task]:\n                if record_name in files:\n                    record = Record(task, record_name)\n                    record.set_legend(self.legend_with)\n                    res[task].append(record)\n        return res\n\n    def group_records(self, key=['seed']):\n        if type(key) is not list: key=[key]\n        groups = collections.defaultdict(list)\n        for rec in self.all_records:\n            group_name = '.'.join([str(rec.data['option'][k]) for k in rec.data['option'].keys() if k not in key])\n            groups[group_name].append(rec)\n        res = []\n        for g in groups:\n            res.append(Record.create_group(groups[g]))\n        return res, list(groups.keys())\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Table","title":"<code>Table</code>","text":"<p>Organize the information in records into a table.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list</code> <p>a list of instances of Record(...)</p> required <code>save_text</code> <code>bool</code> <p>whether to store the table into the disk</p> <code>False</code> <code>path</code> <code>str</code> <p>the storing path</p> <code>'.'</code> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Table:\nr\"\"\"\n    Organize the information in records into a table.\n\n    Args:\n        records (list): a list of instances of Record(...)\n        save_text (bool): whether to store the table into the disk\n        path (str): the storing path\n    \"\"\"\n    def __init__(self, records:list, save_text:bool=False, path:str='.'):\n        self.records = records\n        self.save_text = save_text\n        self.path = path\n        self.tb = pt.PrettyTable()\n        self.tb.add_column('Task', [r.data['option']['task'] for r in self.records])\n        self.tb.add_column('Record', [r.data['label'] for r in self.records])\n        self.tb.float_format = \"3.4\"\n        self.sort_key = None\n\n    def add_column(self, func, col_option):\nr\"\"\"\n        Add a column to this table. For each record $Record_k$, its value $v_k$\n        in this column is v_k=func(Record_k, col_option), where func can be \n        arbitrarily customized.\n\n        Args:\n            func (func|str): the name of the function or the function\n            col_option (dict|str): the option of the column to index data in each record\n\n        Example:\n        ```python\n            &gt;&gt;&gt; tb = Table(records)\n            &gt;&gt;&gt; tb.add_column(min_value, col_option={'x':'valid_loss'})\n            &gt;&gt;&gt; tb.print()\n        ```\n        \"\"\"\n        func = eval(func) if type(func) is str else func\n        col_option = {'x': col_option} if type(col_option) is not dict else col_option\n        column = []\n        for rec in self.records:\n            column.append(func(rec, col_option))\n        if 'name' in col_option.keys():\n            fieldname = col_option['name']\n        else:\n            fieldname = '-'.join([str(v) for k,v in col_option.items() if k!='sort'])\n            fieldname = func.__name__ + '-' + fieldname\n        self.tb.add_column(fieldname=fieldname, column=column)\n        if 'sort' in col_option.keys(): self.tb.sortby = fieldname\n\n    def set_title(self, title):\n        self.tb.title = title\n\n    def print(self):\nr\"\"\"Print and store the table\"\"\"\n        if self.save_text:\n            with open(os.path.join(self.path, str(uuid.uuid4())+'.txt'), 'w') as outf:\n                outf.write(self.tb.__repr__())\n        print(self)\n\n    def __repr__(self):\n        return self.tb.__repr__()\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Table.add_column","title":"<code>add_column(func, col_option)</code>","text":"<p>Add a column to this table. For each record $Record_k$, its value $v_k$ in this column is v_k=func(Record_k, col_option), where func can be  arbitrarily customized.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>func | str</code> <p>the name of the function or the function</p> required <code>col_option</code> <code>dict | str</code> <p>the option of the column to index data in each record</p> required <p>Example:</p> <pre><code>    &gt;&gt;&gt; tb = Table(records)\n    &gt;&gt;&gt; tb.add_column(min_value, col_option={'x':'valid_loss'})\n    &gt;&gt;&gt; tb.print()\n</code></pre> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def add_column(self, func, col_option):\nr\"\"\"\n    Add a column to this table. For each record $Record_k$, its value $v_k$\n    in this column is v_k=func(Record_k, col_option), where func can be \n    arbitrarily customized.\n\n    Args:\n        func (func|str): the name of the function or the function\n        col_option (dict|str): the option of the column to index data in each record\n\n    Example:\n    ```python\n        &gt;&gt;&gt; tb = Table(records)\n        &gt;&gt;&gt; tb.add_column(min_value, col_option={'x':'valid_loss'})\n        &gt;&gt;&gt; tb.print()\n    ```\n    \"\"\"\n    func = eval(func) if type(func) is str else func\n    col_option = {'x': col_option} if type(col_option) is not dict else col_option\n    column = []\n    for rec in self.records:\n        column.append(func(rec, col_option))\n    if 'name' in col_option.keys():\n        fieldname = col_option['name']\n    else:\n        fieldname = '-'.join([str(v) for k,v in col_option.items() if k!='sort'])\n        fieldname = func.__name__ + '-' + fieldname\n    self.tb.add_column(fieldname=fieldname, column=column)\n    if 'sort' in col_option.keys(): self.tb.sortby = fieldname\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Table.print","title":"<code>print()</code>","text":"<p>Print and store the table</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def print(self):\nr\"\"\"Print and store the table\"\"\"\n    if self.save_text:\n        with open(os.path.join(self.path, str(uuid.uuid4())+'.txt'), 'w') as outf:\n            outf.write(self.tb.__repr__())\n    print(self)\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.Trace2D","title":"<code>Trace2D</code>","text":"<p>         Bases: <code>PaintObject</code></p> <p>Trace Object</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>class Trace2D(PaintObject):\n\"\"\"Trace Object\"\"\"\n    def __init__(self, rec, args,  obj_option):\n        super(Trace2D, self).__init__(rec, args, obj_option, '')\n\n    def draw(self, ax):\n        pass\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.final_value","title":"<code>final_value(record, col_option)</code>","text":"<p>Get final value. The col_option should be like     {'x': key of record.data}</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def final_value(record, col_option):\nr\"\"\"\n    Get final value. The col_option should be like\n        {'x': key of record.data}\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    return record.data[col_option['x']][-1]\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.group_optimal_value","title":"<code>group_optimal_value(record, col_option)</code>","text":"<p>Get the grouped optimal value. The col_option should be like     {     'x': key of record.data,     'flag': 'min' or 'max'     }</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def group_optimal_value(record, col_option):\nr\"\"\"\n    Get the grouped optimal value. The col_option should be like\n        {\n        'x': key of record.data,\n        'flag': 'min' or 'max'\n        }\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    if 'flag' not in col_option.keys(): col_option['flag'] = 'min'\n    if col_option['flag']=='min': f = np.min\n    else: f=np.max\n    minvs = np.array([f(rdata[col_option['x']]) for rdata in record.datas])\n    mean_v = np.mean(minvs)\n    std_v = np.std(minvs)\n    return \"{:.4f} \u00b1 {:.4f}\".format(mean_v, std_v)\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.group_optimal_x_by_y","title":"<code>group_optimal_x_by_y(record, col_option)</code>","text":"<p>Get the grouped value of y where the grouped value of x is the optimal. The col_option should be like     {     'x': key of record.data,     'y': key of record.data,     'flag': 'min' or 'max'     }</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def group_optimal_x_by_y(record, col_option):\nr\"\"\"\n    Get the grouped value of y where the grouped value of x is the optimal.\n    The col_option should be like\n        {\n        'x': key of record.data,\n        'y': key of record.data,\n        'flag': 'min' or 'max'\n        }\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    if 'flag' not in col_option.keys(): col_option['flag'] = 'min'\n    if col_option['flag']=='min': f = np.argmin\n    else: f=np.argmax\n    vs = []\n    for rdata in record.datas:\n        tmp = f(rdata[col_option['y']])\n        vs.append(rdata[col_option['x']][tmp])\n    mean_v = np.mean(vs)\n    std_v = np.std(vs)\n    return \"{:.4f} \u00b1 {:.4f}\".format(mean_v, std_v)\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.max_value","title":"<code>max_value(record, col_option)</code>","text":"<p>Get maximal value.The col_option should be like     {'x': key of record.data}</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def max_value(record,  col_option):\nr\"\"\"\n    Get maximal value.The col_option should be like\n        {'x': key of record.data}\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    return np.max(record.data[col_option['x']])\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.mean_value","title":"<code>mean_value(record, col_option)</code>","text":"<p>Get mean value. The col_option should be like     {'x': key of record.data}</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def mean_value(record, col_option):\nr\"\"\"\n    Get mean value. The col_option should be like\n        {'x': key of record.data}\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    return np.mean(record.data[col_option['x']])\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.min_value","title":"<code>min_value(record, col_option)</code>","text":"<p>Get minimal value. The col_option should be like     {'x': key of record.data}</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def min_value(record,  col_option):\nr\"\"\"\n    Get minimal value. The col_option should be like\n        {'x': key of record.data}\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    return np.min(record.data[col_option['x']])\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.optimal_x_by_y","title":"<code>optimal_x_by_y(record, col_option)</code>","text":"<p>Get the value of y where the value of x is the optimal. The col_option should be like     {     'x': key of record.data,     'y': key of record.data,     'flag': 'min' or 'max'     }</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def optimal_x_by_y(record, col_option):\nr\"\"\"\n    Get the value of y where the value of x is the optimal.\n    The col_option should be like\n        {\n        'x': key of record.data,\n        'y': key of record.data,\n        'flag': 'min' or 'max'\n        }\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    if 'flag' not in col_option.keys(): col_option['flag'] = 'min'\n    if col_option['flag']=='min': f = np.argmin\n    else: f=np.argmax\n    tmp = f(record.data[col_option['y']])\n    return record.data[col_option['x']][tmp]\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.show","title":"<code>show(config, save_figure=False, save_text=False, path='.', seed=0)</code>","text":"<p>Show the results according to analysis configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict | str</code> <p>the analysis plan</p> required <code>save_figure</code> <code>bool</code> <p>whether to save figures</p> <code>False</code> <code>save_text</code> <code>bool</code> <p>whether to save table as .txt file</p> <code>False</code> <code>path</code> <code>str</code> <p>the path to store the results</p> <code>'.'</code> <code>seed</code> <code>int</code> <p>random seed</p> <code>0</code> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.experiment.analyzer as al\n    &gt;&gt;&gt; # only records of fedavg running on the task 'my_task' with learning rate lr&lt;=0.01 will be selected\n    &gt;&gt;&gt; selector_config = {'task':'./my_task', 'header':['fedavg'], 'filter':['LR':'&lt;=0.1']}\n    &gt;&gt;&gt; # draw the learning curve on the validation dataset\n    &gt;&gt;&gt; painter_config = {'Curve':[{'args':{'x':'communication_round', 'y':'valid_loss'}}]}\n    &gt;&gt;&gt; # show the minimal value of validation loss\n    &gt;&gt;&gt; table_config = {'min_value':[{'x':'valid_loss'}]}\n    &gt;&gt;&gt; # create analysis plan\n    &gt;&gt;&gt; analysis_plan = {'Selector':selector_config, 'Painter':painter_config, 'Table':table_config}\n    &gt;&gt;&gt; # call this function\n    &gt;&gt;&gt; al.show(analysis_plan)\n</code></pre> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def show(config, save_figure=False, save_text=False, path='.', seed=0):\nr\"\"\"\n    Show the results according to analysis configuration.\n\n    Args:\n        config (dict|str): the analysis plan\n        save_figure (bool): whether to save figures\n        save_text (bool): whether to save table as .txt file\n        path (str): the path to store the results\n        seed (int): random seed\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.experiment.analyzer as al\n        &gt;&gt;&gt; # only records of fedavg running on the task 'my_task' with learning rate lr&lt;=0.01 will be selected\n        &gt;&gt;&gt; selector_config = {'task':'./my_task', 'header':['fedavg'], 'filter':['LR':'&lt;=0.1']}\n        &gt;&gt;&gt; # draw the learning curve on the validation dataset\n        &gt;&gt;&gt; painter_config = {'Curve':[{'args':{'x':'communication_round', 'y':'valid_loss'}}]}\n        &gt;&gt;&gt; # show the minimal value of validation loss\n        &gt;&gt;&gt; table_config = {'min_value':[{'x':'valid_loss'}]}\n        &gt;&gt;&gt; # create analysis plan\n        &gt;&gt;&gt; analysis_plan = {'Selector':selector_config, 'Painter':painter_config, 'Table':table_config}\n        &gt;&gt;&gt; # call this function\n        &gt;&gt;&gt; al.show(analysis_plan)\n    ```\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    option = load_configuration(config)\n    record_selector = Selector(option['Selector'])\n    if 'Painter' in option.keys():\n        painter = Painter(record_selector.all_records, save_figure=save_figure, path=path)\n        group_painter = Painter(record_selector.grouped_records, save_figure=save_figure, path=path)\n        for object_class_string in option['Painter'].keys():\n            figs = option['Painter'][object_class_string] if type(option['Painter'][object_class_string]) is list else [option['Painter'][object_class_string]]\n            grouped = ('Group' in object_class_string)\n            p = group_painter if grouped else painter\n            for fig_config in figs:\n                p.create_figure(object_class_string, fig_config)\n\n    if 'Table' in option.keys():\n        tb = Table(record_selector.all_records, save_text=save_text, path=path)\n        group_tb = Table(record_selector.grouped_records, save_text=save_text, path=path)\n        for funcname in option['Table']:\n            columns = option['Table'][funcname] if type(option['Table'][funcname]) is list else [option['Table'][funcname]]\n            grouped = ('group' in funcname)\n            ctb = group_tb if grouped else tb\n            for col_option in columns:\n                ctb.add_column(funcname, col_option)\n        tb.print()\n        group_tb.print()\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.std_value","title":"<code>std_value(record, col_option)</code>","text":"<p>Get standard deviation. The col_option should be like     {'x': key of record.data}</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def std_value(record, col_option):\nr\"\"\"\n    Get standard deviation. The col_option should be like\n        {'x': key of record.data}\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    return np.std(record.data[col_option['x']])\n</code></pre>"},{"location":"Docs/experiment/analyzer/#flgo.experiment.analyzer.variance","title":"<code>variance(record, col_option)</code>","text":"<p>Get variance. The col_option should be like     {'x': key of record.data}</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>the record</p> required <code>col_option</code> <code>dict</code> <p>column option</p> required <p>Returns:</p> Type Description <p>the column value</p> Source code in <code>flgo\\experiment\\analyzer.py</code> <pre><code>def variance(record, col_option):\nr\"\"\"\n    Get variance. The col_option should be like\n        {'x': key of record.data}\n\n    Args:\n        record (Record): the record\n        col_option (dict): column option\n\n    Returns:\n        the column value\n    \"\"\"\n    return np.var(record.data[col_option['x']])\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/","title":"flgo.experiment.device_scheduler","text":"<p>This module is for scheduling GPU devices to different runners. There are three pre-defined Schedulers: BasicScheduler, AutoScheduler, and RandomScheduler.</p> <p>When the number of runners is large and GPU memory is limited, we recommend to use AutoScheduler. Otherwise, BasicScheduler and RandomScheduler are both good choices.</p>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.AbstractScheduler","title":"<code>AbstractScheduler</code>","text":"<p>Abstract Scheduler</p> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>class AbstractScheduler(metaclass=ABCMeta):\nr\"\"\"Abstract Scheduler\"\"\"\n    @abstractmethod\n    def get_available_device(self, *args, **kwargs):\nr\"\"\"Search for a currently available device and return it\"\"\"\n        pass\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.AbstractScheduler.get_available_device","title":"<code>get_available_device(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Search for a currently available device and return it</p> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>@abstractmethod\ndef get_available_device(self, *args, **kwargs):\nr\"\"\"Search for a currently available device and return it\"\"\"\n    pass\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.AutoScheduler","title":"<code>AutoScheduler</code>","text":"<p>         Bases: <code>BasicScheduler</code></p> <p>Automatically schedule GPUs by dynamically esimating the GPU memory occupation for all the runners and checking availability according to real-time memory information.</p> <p>Parameters:</p> Name Type Description Default <code>devices</code> <code>list</code> <p>a list of the index numbers of GPUs</p> required <code>put_interval</code> <code>int</code> <p>the minimal time interval (i.e. seconds) to allocate the same device</p> <code>5</code> <code>mean_memory_occupated</code> <code>int</code> <p>the initial mean memory occupation (i.e. MB) for all the runners</p> <code>1000</code> <code>available_interval</code> <code>int</code> <p>a gpu will be returned only if it is kept available for a period longer than this term</p> <code>5</code> <code>dynamic_memory_occupated</code> <code>bool</code> <p>whether to dynamically estimate the memory occupation</p> <code>True</code> <code>dynamic_condition</code> <code>str</code> <p>'mean' or 'max'</p> <code>'mean'</code> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.experiment.device_scheduler\n    &gt;&gt;&gt; sc = flgo.experiment.device_scheduler.AutoScheduler([0,1])\n    &gt;&gt;&gt; import flgo\n    &gt;&gt;&gt; flgo.multi_init_and_run(runner_args, scheduler=sc)\n</code></pre> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>class AutoScheduler(BasicScheduler):\nr\"\"\"\n    Automatically schedule GPUs by dynamically esimating the GPU memory occupation\n    for all the runners and checking availability according to real-time memory information.\n\n    Args:\n        devices (list): a list of the index numbers of GPUs\n        put_interval (int, optional): the minimal time interval (i.e. seconds) to allocate the same device\n        mean_memory_occupated (int, optional): the initial mean memory occupation (i.e. MB) for all the runners\n        available_interval (int, optional): a gpu will be returned only if it is kept available for a period longer than this term\n        dynamic_memory_occupated (bool, optional): whether to dynamically estimate the memory occupation\n        dynamic_condition (str): 'mean' or 'max'\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.experiment.device_scheduler\n        &gt;&gt;&gt; sc = flgo.experiment.device_scheduler.AutoScheduler([0,1])\n        &gt;&gt;&gt; import flgo\n        &gt;&gt;&gt; flgo.multi_init_and_run(runner_args, scheduler=sc)\n    ```\n    \"\"\"\n    def __init__(self, devices:list, put_interval = 5, mean_memory_occupated = 1000, available_interval=5, dynamic_memory_occupated=True, dynamic_condition='mean'):\n        super(AutoScheduler, self).__init__(devices)\n        pynvml.nvmlInit()\n        crt_time = time.time()\n        self.dev_state = {\n            dev:{\n                'avl': True,\n                'time':crt_time,\n                'time_put':None,\n                'handle':pynvml.nvmlDeviceGetHandleByIndex(dev),\n                'total_memory':0,\n                'allocated_memory':0,\n                'free_memory':0,\n            }\n            for dev in self.devices\n        }\n        self.put_interval = put_interval\n        self.mean_memory_occupated = mean_memory_occupated\n        self.available_interval = available_interval\n        self.dynamic_condition = dynamic_condition\n        self.dynamic_memory_occupated = dynamic_memory_occupated\n\n    def get_available_device(self, option, *args, **kwargs):\n        for dev in self.devices:\n            self.flush(dev)\n        all_mems = []\n        for dev in self.devices:\n            dev_handle = self.dev_state[dev]['handle']\n            ps = pynvml.nvmlDeviceGetComputeRunningProcesses(dev_handle)\n            mems = [p.usedGpuMemory for p in ps if p.pid in self.process_set]\n            all_mems.extend(mems)\n        if self.dynamic_memory_occupated:\n            if len(all_mems)&gt;0:\n                mem = max(all_mems) if self.dynamic_condition=='max' else sum(all_mems)/len(all_mems)\n                self.mean_memory_occupated = self.byte2mb(mem)\n        tmp = copy.deepcopy(self.devices)\n        sorted(tmp, key=lambda x:self.dev_state[x]['free_memory'])\n        for dev in tmp:\n            if self.check_available(dev):\n                return dev\n        return None\n\n    def byte2mb(self, size):\n        return int(size/1024/1024)\n\n    def flush(self, dev):\n        if dev&gt;=0:\n            handle = self.dev_state[dev]['handle']\n            meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n            self.dev_state[dev]['total_memory'] = self.byte2mb(meminfo.total)\n            self.dev_state[dev]['allocated_memory'] = self.byte2mb(meminfo.used)\n            self.dev_state[dev]['free_memory'] = self.byte2mb(meminfo.free)\n\n    def check_available(self, dev):\n        if dev=='-1':return True\n        crt_time = time.time()\n        crt_free_memory = self.dev_state[dev]['free_memory']\n        target_memory = self.mean_memory_occupated\n        crt_avl = crt_free_memory&gt;target_memory\n        if crt_avl:\n            if self.dev_state[dev]['avl']:\n                if crt_time - self.dev_state[dev]['time']&gt;=self.available_interval:\n                    if self.dev_state[dev]['time_put'] is None or crt_time-self.dev_state[dev]['time_put']&gt;=self.put_interval:\n                        self.dev_state[dev]['time_put'] = crt_time\n                        return True\n        if crt_avl!=self.dev_state[dev]['avl']:\n            self.dev_state[dev]['avl'] = True\n            self.dev_state[dev]['time'] = crt_time\n        return False\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.BasicScheduler","title":"<code>BasicScheduler</code>","text":"<p>         Bases: <code>AbstractScheduler</code></p> <p>Basic gpu scheduler. Each device will be always considered available and will be returned in turn.</p> <p>Parameters:</p> Name Type Description Default <code>devices</code> <code>list</code> <p>a list of the index numbers of GPUs</p> required Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>class BasicScheduler(AbstractScheduler):\nr\"\"\"\n    Basic gpu scheduler. Each device will be always considered available\n    and will be returned in turn.\n\n    Args:\n        devices (list): a list of the index numbers of GPUs\n    \"\"\"\n    def __init__(self, devices:list, *args, **kwargs):\n        self.devices = devices if devices != [] else [-1]\n        self.dev_index = 0\n        self.process_set = set()\n\n    def get_available_device(self, *args, **kwargs):\n\"\"\"Return the next device\"\"\"\n        self.dev_index = (self.dev_index+1)%len(self.devices)\n        return self.devices[self.dev_index]\n\n    def set_devices(self, devices:list):\nr\"\"\"\n        Reset all the devices\n\n        Args:\n            devices (list): a list of the index numbers of GPUs\n        \"\"\"\n        self.devices=[-1] if devices==[] else devices\n        self.dev_index = self.dev_index%len(self.devices)\n\n    def add_process(self, pid=None):\nr\"\"\"\n        Record the running process that uses the gpu from the scheduler\n\n        Args:\n            pid (int): the process id\n        \"\"\"\n        if pid is not None:\n            self.process_set.add(pid)\n\n    def remove_process(self, pid=None):\nr\"\"\"\n        Remove the running process that uses the gpu from the scheduler\n\n        Args:\n            pid (int): the process id\n        \"\"\"\n        if pid is not None and pid in self.process_set:\n            self.process_set.remove(pid)\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.BasicScheduler.add_process","title":"<code>add_process(pid=None)</code>","text":"<p>Record the running process that uses the gpu from the scheduler</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>the process id</p> <code>None</code> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>def add_process(self, pid=None):\nr\"\"\"\n    Record the running process that uses the gpu from the scheduler\n\n    Args:\n        pid (int): the process id\n    \"\"\"\n    if pid is not None:\n        self.process_set.add(pid)\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.BasicScheduler.get_available_device","title":"<code>get_available_device(*args, **kwargs)</code>","text":"<p>Return the next device</p> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>def get_available_device(self, *args, **kwargs):\n\"\"\"Return the next device\"\"\"\n    self.dev_index = (self.dev_index+1)%len(self.devices)\n    return self.devices[self.dev_index]\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.BasicScheduler.remove_process","title":"<code>remove_process(pid=None)</code>","text":"<p>Remove the running process that uses the gpu from the scheduler</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>the process id</p> <code>None</code> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>def remove_process(self, pid=None):\nr\"\"\"\n    Remove the running process that uses the gpu from the scheduler\n\n    Args:\n        pid (int): the process id\n    \"\"\"\n    if pid is not None and pid in self.process_set:\n        self.process_set.remove(pid)\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.BasicScheduler.set_devices","title":"<code>set_devices(devices)</code>","text":"<p>Reset all the devices</p> <p>Parameters:</p> Name Type Description Default <code>devices</code> <code>list</code> <p>a list of the index numbers of GPUs</p> required Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>def set_devices(self, devices:list):\nr\"\"\"\n    Reset all the devices\n\n    Args:\n        devices (list): a list of the index numbers of GPUs\n    \"\"\"\n    self.devices=[-1] if devices==[] else devices\n    self.dev_index = self.dev_index%len(self.devices)\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.RandomScheduler","title":"<code>RandomScheduler</code>","text":"<p>         Bases: <code>BasicScheduler</code></p> <p>Random GPU Scheduler</p> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>class RandomScheduler(BasicScheduler):\n\"\"\"Random GPU Scheduler\"\"\"\n    def get_available_device(self, *args, **kwargs):\n\"\"\"Return a random device\"\"\"\n        return random.choice(self.devices)\n</code></pre>"},{"location":"Docs/experiment/device_scheduler/#flgo.experiment.device_scheduler.RandomScheduler.get_available_device","title":"<code>get_available_device(*args, **kwargs)</code>","text":"<p>Return a random device</p> Source code in <code>flgo\\experiment\\device_scheduler.py</code> <pre><code>def get_available_device(self, *args, **kwargs):\n\"\"\"Return a random device\"\"\"\n    return random.choice(self.devices)\n</code></pre>"},{"location":"Docs/experiment/logger/","title":"Index","text":"<p>         Bases: <code>Logger</code></p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>class BasicLogger(Logger):\n\n    _LEVEL = {\n        \"DEBUG\": DEBUG,\n\n        \"INFO\": INFO,\n\n        \"WARNING\": WARNING,\n\n        \"ERROR\": ERROR,\n\n        \"CRITICAL\": CRITICAL,\n    }\n\n    def __init__(self, task, option, *args, **kwargs):\n        self.task_path = task\n        self.option = option\n        super(BasicLogger, self).__init__(*args, **kwargs)\n        self.output = collections.defaultdict(list)\n        self.output['option'] = option\n        self.current_round = -1\n        self.objects = []\n        self.temp = \"{:&lt;30s}{:.4f}\"\n        self.time_costs = []\n        self.time_buf = {}\n        self.formatter = Formatter('%(asctime)s %(filename)s %(funcName)s [line:%(lineno)d] %(levelname)s %(message)s')\n        self.handler_list = []\n        self.overwrite = not self.option['no_overwrite']\n        if not self.option['no_log_console']:\n            self.streamhandler = StreamHandler()\n            self.streamhandler.setFormatter(self.formatter)\n            self.streamhandler.setLevel(self._LEVEL[self.option['log_level'].upper()])\n            self.addHandler(self.streamhandler)\n        if self.option['log_file']:\n            log_dir = self.get_log_path()\n            self.log_path = os.path.join(log_dir, self.get_time_string()+self.get_output_name('.log'))\n            if not os.path.exists(self.get_log_path()):\n                os.mkdir(log_dir)\n            self.filehandler = FileHandler(self.log_path)\n            self.filehandler.setFormatter(self.formatter)\n            self.filehandler.setLevel(self._LEVEL[self.option['log_level'].upper()])\n            self.addHandler(self.filehandler)\n        # options of early stopping\n        self._es_key = 'valid_loss'\n        self._es_patience = 20\n        self._es_counter = 0\n        self._es_best_score = None\n        self._es_best_round = 0\n\n    def check_if_log(self, round, eval_interval=-1):\n\"\"\"For evaluating every 'eval_interval' rounds, check whether to log at 'round'.\"\"\"\n        self.current_round = round\n        return eval_interval &gt; 0 and (round == 0 or round % eval_interval == 0)\n\n    def time_start(self, key=''):\n\"\"\"Create a timestamp of the event 'key' starting\"\"\"\n        if key not in [k for k in self.time_buf.keys()]:\n            self.time_buf[key] = []\n        self.time_buf[key].append(time.time())\n\n    def time_end(self, key=''):\n\"\"\"Create a timestamp that ends the event 'key' and print the time interval of the event.\"\"\"\n        if key not in [k for k in self.time_buf.keys()]:\n            raise RuntimeError(\"Timer end before start.\")\n        else:\n            self.time_buf[key][-1] = time.time() - self.time_buf[key][-1]\n            self.info(\"{:&lt;30s}{:.4f}\".format(key + \":\", self.time_buf[key][-1]) + 's')\n            return self.time_buf[key][-1]\n\n    def save_output_as_json(self, filepath=None):\n\"\"\"Save the self.output as .json file\"\"\"\n        if len(self.output) == 0: return\n        self.organize_output()\n        self.output_to_jsonable_dict()\n        if filepath is None:\n            filepath = os.path.join(self.get_output_path(),self.get_output_name())\n        if not self.overwrite:\n            if os.path.exists(filepath):\n                with open(filepath, 'r') as inf:\n                    original_record = json.loads(inf.read())\n                o_keys = set(original_record.keys())\n                output_keys = set(self.output.keys())\n                new_keys = list(output_keys.difference(o_keys))\n                for k in new_keys:\n                    original_record[k] = self.output[k]\n                self.output = original_record\n        try:\n            with open(filepath, 'w') as outf:\n                json.dump(dict(self.output), outf)\n        except:\n            self.error('Failed to save flw.logger.output as results')\n\n    def check_is_jsonable(self, x):\n        try:\n            json.dumps(x)\n            return True\n        except:\n            return False\n\n    def output_to_jsonable_dict(self):\n        for key, value in self.output.items():\n            if not self.check_is_jsonable(value):\n                try:\n                    self.output[key] = str(self.output[key])\n                    self.warning(\"flw.logger.output['{}'] is not jsonable, and is automatically converted to string.\".format(key))\n                except:\n                    del self.output[key]\n                    self.warning(\"Automatically remove flw.logger.output['{}'] from logger, because it is not jsonable and is failed to convert into string. \".format(key))\n        return\n\n    def write_var_into_output(self, var_name=None, var_value=None):\n\"\"\"Add variable 'var_name' and its value var_value to logger\"\"\"\n        if var_name == None: raise RuntimeError(\"Missing the name of the variable to be logged.\")\n        self.output[var_name].append(var_value)\n        return\n\n    def register_variable(self, **kwargs):\n\"\"\"Initialze the logger in utils.fflow.initialize()\"\"\"\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n        return\n\n    def show_current_output(self, yes_key=['train', 'test', 'valid'], no_key=['dist']):\n        for key, val in self.output.items():\n            a = [(yk in key) for yk in yes_key]\n            nf = [(nk not in key) for nk in no_key]\n            if np.all(nf) and np.any(a):\n                self.info(self.temp.format(key, val[-1]))\n\n    def get_output_name(self, suffix='.json'):\n        if not hasattr(self, 'option'): raise NotImplementedError('logger has no attr named \"option\"')\n        header = \"{}_\".format(self.option[\"algorithm\"])\n        if hasattr(self, 'coordinator'):\n            for para, pv in self.coordinator.algo_para.items():\n                header = header + para + \"{}_\".format(pv)\n        else:\n            if self.option['algo_para'] is not None:\n                header = header + 'algopara_'+'|'.join([str(p) for p in self.option['algo_para']])\n\n        output_name = header + \"M{}_R{}_B{}_\".format(self.option['model'], self.option['num_rounds'], self.option['batch_size'])\n        if self.option['num_steps']&lt;0:\n            output_name = output_name + (\"E{}_\".format(self.option['num_epochs']))\n        else:\n            output_name = output_name + (\"K{}_\".format(self.option['num_steps']))\n\n        output_name = output_name + \"LR{:.4f}_P{:.2f}_S{}_LD{:.3f}_WD{:.3f}_AVL{}_CN{}_CP{}_RS{}_LG{}\".format(\n                        self.option['learning_rate'],\n                        self.option['proportion'],\n                        self.option['seed'],\n                        self.option['lr_scheduler'] + self.option['learning_rate_decay'],\n                        self.option['weight_decay'],\n                        self.option['availability'],\n                        self.option['connectivity'],\n                        self.option['completeness'],\n                        self.option['responsiveness'],\n                        self.__class__.__name__,\n        ) + suffix\n        return output_name\n\n    def get_output_path(self):\n        if not hasattr(self, 'option'): raise NotImplementedError('logger has no attr named \"option\"')\n        return os.path.join(self.task_path, 'record')\n\n    def get_log_path(self):\n        return os.path.join(self.task_path, 'log')\n\n    def get_time_string(self):\n        return time.strftime('%Y-%m-%d-%H-%M-%S')\n\n    def early_stop(self):\n        # Early stopping when there is no improvement on the validation loss for more than self.option['early_stop'] rounds\n        if self.option['early_stop']&lt;0 or (self._es_key not in self.output): return False\n        score = -self.output[self._es_key][-1]\n        if np.isnan(score): return True\n        if self._es_best_score is None:\n            self._es_best_score = score\n            self._es_best_round = self.coordinator.current_round-1\n            self._es_patience = self.option['early_stop']\n        elif score&lt;self._es_best_score:\n            self._es_counter += 1\n            if self._es_counter &gt;= self._es_patience:\n                self.info('Early stopping after training for {} rounds.'.format(self.coordinator.current_round-1))\n                return True\n        else:\n            self._es_best_score = score\n            self._es_best_round = self.coordinator.current_round-1\n            self._es_counter = 0\n        return False\n\n    def initialize(self, *args, **kwargs):\n        return\n\n    def log_once(self, *args, **kwargs):\n\"\"\"This method is called at the beginning of each communication round of Server.\n        The round-wise operations of recording should be complemented here.\"\"\"\n        # calculate the testing metrics on testing dataset owned by coordinator\n        test_metric = self.coordinator.test()\n        for met_name, met_val in test_metric.items():\n            self.output['test_' + met_name].append(met_val)\n        # calculate weighted averaging of metrics on training datasets across participants\n        local_data_vols = [c.datavol for c in self.participants]\n        total_data_vol = sum(local_data_vols)\n        train_metrics = self.coordinator.global_test(flag='train')\n        for met_name, met_val in train_metrics.items():\n            self.output['train_' + met_name + '_dist'].append(met_val)\n            self.output['train_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n        # calculate weighted averaging and other statistics of metrics on validation datasets across clients\n        valid_metrics = self.coordinator.global_test(flag='valid')\n        for met_name, met_val in valid_metrics.items():\n            self.output['valid_'+met_name+'_dist'].append(met_val)\n            self.output['valid_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n            self.output['mean_valid_' + met_name].append(np.mean(met_val))\n            self.output['std_valid_' + met_name].append(np.std(met_val))\n        # output to stdout\n        self.show_current_output()\n\n    def organize_output(self, *args, **kwargs):\n\"\"\"This method will be called before saving self.output\"\"\"\n        for key in self.output.keys():\n            if '_dist' in key:\n                self.output[key] = self.output[key][-1]\n        return\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.check_if_log","title":"<code>check_if_log(round, eval_interval=-1)</code>","text":"<p>For evaluating every 'eval_interval' rounds, check whether to log at 'round'.</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def check_if_log(self, round, eval_interval=-1):\n\"\"\"For evaluating every 'eval_interval' rounds, check whether to log at 'round'.\"\"\"\n    self.current_round = round\n    return eval_interval &gt; 0 and (round == 0 or round % eval_interval == 0)\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.log_once","title":"<code>log_once(*args, **kwargs)</code>","text":"<p>This method is called at the beginning of each communication round of Server. The round-wise operations of recording should be complemented here.</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def log_once(self, *args, **kwargs):\n\"\"\"This method is called at the beginning of each communication round of Server.\n    The round-wise operations of recording should be complemented here.\"\"\"\n    # calculate the testing metrics on testing dataset owned by coordinator\n    test_metric = self.coordinator.test()\n    for met_name, met_val in test_metric.items():\n        self.output['test_' + met_name].append(met_val)\n    # calculate weighted averaging of metrics on training datasets across participants\n    local_data_vols = [c.datavol for c in self.participants]\n    total_data_vol = sum(local_data_vols)\n    train_metrics = self.coordinator.global_test(flag='train')\n    for met_name, met_val in train_metrics.items():\n        self.output['train_' + met_name + '_dist'].append(met_val)\n        self.output['train_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n    # calculate weighted averaging and other statistics of metrics on validation datasets across clients\n    valid_metrics = self.coordinator.global_test(flag='valid')\n    for met_name, met_val in valid_metrics.items():\n        self.output['valid_'+met_name+'_dist'].append(met_val)\n        self.output['valid_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n        self.output['mean_valid_' + met_name].append(np.mean(met_val))\n        self.output['std_valid_' + met_name].append(np.std(met_val))\n    # output to stdout\n    self.show_current_output()\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.organize_output","title":"<code>organize_output(*args, **kwargs)</code>","text":"<p>This method will be called before saving self.output</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def organize_output(self, *args, **kwargs):\n\"\"\"This method will be called before saving self.output\"\"\"\n    for key in self.output.keys():\n        if '_dist' in key:\n            self.output[key] = self.output[key][-1]\n    return\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.register_variable","title":"<code>register_variable(**kwargs)</code>","text":"<p>Initialze the logger in utils.fflow.initialize()</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def register_variable(self, **kwargs):\n\"\"\"Initialze the logger in utils.fflow.initialize()\"\"\"\n    for k, v in kwargs.items():\n        setattr(self, k, v)\n    return\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.save_output_as_json","title":"<code>save_output_as_json(filepath=None)</code>","text":"<p>Save the self.output as .json file</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def save_output_as_json(self, filepath=None):\n\"\"\"Save the self.output as .json file\"\"\"\n    if len(self.output) == 0: return\n    self.organize_output()\n    self.output_to_jsonable_dict()\n    if filepath is None:\n        filepath = os.path.join(self.get_output_path(),self.get_output_name())\n    if not self.overwrite:\n        if os.path.exists(filepath):\n            with open(filepath, 'r') as inf:\n                original_record = json.loads(inf.read())\n            o_keys = set(original_record.keys())\n            output_keys = set(self.output.keys())\n            new_keys = list(output_keys.difference(o_keys))\n            for k in new_keys:\n                original_record[k] = self.output[k]\n            self.output = original_record\n    try:\n        with open(filepath, 'w') as outf:\n            json.dump(dict(self.output), outf)\n    except:\n        self.error('Failed to save flw.logger.output as results')\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.time_end","title":"<code>time_end(key='')</code>","text":"<p>Create a timestamp that ends the event 'key' and print the time interval of the event.</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def time_end(self, key=''):\n\"\"\"Create a timestamp that ends the event 'key' and print the time interval of the event.\"\"\"\n    if key not in [k for k in self.time_buf.keys()]:\n        raise RuntimeError(\"Timer end before start.\")\n    else:\n        self.time_buf[key][-1] = time.time() - self.time_buf[key][-1]\n        self.info(\"{:&lt;30s}{:.4f}\".format(key + \":\", self.time_buf[key][-1]) + 's')\n        return self.time_buf[key][-1]\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.time_start","title":"<code>time_start(key='')</code>","text":"<p>Create a timestamp of the event 'key' starting</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def time_start(self, key=''):\n\"\"\"Create a timestamp of the event 'key' starting\"\"\"\n    if key not in [k for k in self.time_buf.keys()]:\n        self.time_buf[key] = []\n    self.time_buf[key].append(time.time())\n</code></pre>"},{"location":"Docs/experiment/logger/#flgo.experiment.logger.BasicLogger.write_var_into_output","title":"<code>write_var_into_output(var_name=None, var_value=None)</code>","text":"<p>Add variable 'var_name' and its value var_value to logger</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def write_var_into_output(self, var_name=None, var_value=None):\n\"\"\"Add variable 'var_name' and its value var_value to logger\"\"\"\n    if var_name == None: raise RuntimeError(\"Missing the name of the variable to be logged.\")\n    self.output[var_name].append(var_value)\n    return\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/","title":"BasicLogger","text":"<p>         Bases: <code>Logger</code></p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>class BasicLogger(Logger):\n\n    _LEVEL = {\n        \"DEBUG\": DEBUG,\n\n        \"INFO\": INFO,\n\n        \"WARNING\": WARNING,\n\n        \"ERROR\": ERROR,\n\n        \"CRITICAL\": CRITICAL,\n    }\n\n    def __init__(self, task, option, *args, **kwargs):\n        self.task_path = task\n        self.option = option\n        super(BasicLogger, self).__init__(*args, **kwargs)\n        self.output = collections.defaultdict(list)\n        self.output['option'] = option\n        self.current_round = -1\n        self.objects = []\n        self.temp = \"{:&lt;30s}{:.4f}\"\n        self.time_costs = []\n        self.time_buf = {}\n        self.formatter = Formatter('%(asctime)s %(filename)s %(funcName)s [line:%(lineno)d] %(levelname)s %(message)s')\n        self.handler_list = []\n        self.overwrite = not self.option['no_overwrite']\n        if not self.option['no_log_console']:\n            self.streamhandler = StreamHandler()\n            self.streamhandler.setFormatter(self.formatter)\n            self.streamhandler.setLevel(self._LEVEL[self.option['log_level'].upper()])\n            self.addHandler(self.streamhandler)\n        if self.option['log_file']:\n            log_dir = self.get_log_path()\n            self.log_path = os.path.join(log_dir, self.get_time_string()+self.get_output_name('.log'))\n            if not os.path.exists(self.get_log_path()):\n                os.mkdir(log_dir)\n            self.filehandler = FileHandler(self.log_path)\n            self.filehandler.setFormatter(self.formatter)\n            self.filehandler.setLevel(self._LEVEL[self.option['log_level'].upper()])\n            self.addHandler(self.filehandler)\n        # options of early stopping\n        self._es_key = 'valid_loss'\n        self._es_patience = 20\n        self._es_counter = 0\n        self._es_best_score = None\n        self._es_best_round = 0\n\n    def check_if_log(self, round, eval_interval=-1):\n\"\"\"For evaluating every 'eval_interval' rounds, check whether to log at 'round'.\"\"\"\n        self.current_round = round\n        return eval_interval &gt; 0 and (round == 0 or round % eval_interval == 0)\n\n    def time_start(self, key=''):\n\"\"\"Create a timestamp of the event 'key' starting\"\"\"\n        if key not in [k for k in self.time_buf.keys()]:\n            self.time_buf[key] = []\n        self.time_buf[key].append(time.time())\n\n    def time_end(self, key=''):\n\"\"\"Create a timestamp that ends the event 'key' and print the time interval of the event.\"\"\"\n        if key not in [k for k in self.time_buf.keys()]:\n            raise RuntimeError(\"Timer end before start.\")\n        else:\n            self.time_buf[key][-1] = time.time() - self.time_buf[key][-1]\n            self.info(\"{:&lt;30s}{:.4f}\".format(key + \":\", self.time_buf[key][-1]) + 's')\n            return self.time_buf[key][-1]\n\n    def save_output_as_json(self, filepath=None):\n\"\"\"Save the self.output as .json file\"\"\"\n        if len(self.output) == 0: return\n        self.organize_output()\n        self.output_to_jsonable_dict()\n        if filepath is None:\n            filepath = os.path.join(self.get_output_path(),self.get_output_name())\n        if not self.overwrite:\n            if os.path.exists(filepath):\n                with open(filepath, 'r') as inf:\n                    original_record = json.loads(inf.read())\n                o_keys = set(original_record.keys())\n                output_keys = set(self.output.keys())\n                new_keys = list(output_keys.difference(o_keys))\n                for k in new_keys:\n                    original_record[k] = self.output[k]\n                self.output = original_record\n        try:\n            with open(filepath, 'w') as outf:\n                json.dump(dict(self.output), outf)\n        except:\n            self.error('Failed to save flw.logger.output as results')\n\n    def check_is_jsonable(self, x):\n        try:\n            json.dumps(x)\n            return True\n        except:\n            return False\n\n    def output_to_jsonable_dict(self):\n        for key, value in self.output.items():\n            if not self.check_is_jsonable(value):\n                try:\n                    self.output[key] = str(self.output[key])\n                    self.warning(\"flw.logger.output['{}'] is not jsonable, and is automatically converted to string.\".format(key))\n                except:\n                    del self.output[key]\n                    self.warning(\"Automatically remove flw.logger.output['{}'] from logger, because it is not jsonable and is failed to convert into string. \".format(key))\n        return\n\n    def write_var_into_output(self, var_name=None, var_value=None):\n\"\"\"Add variable 'var_name' and its value var_value to logger\"\"\"\n        if var_name == None: raise RuntimeError(\"Missing the name of the variable to be logged.\")\n        self.output[var_name].append(var_value)\n        return\n\n    def register_variable(self, **kwargs):\n\"\"\"Initialze the logger in utils.fflow.initialize()\"\"\"\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n        return\n\n    def show_current_output(self, yes_key=['train', 'test', 'valid'], no_key=['dist']):\n        for key, val in self.output.items():\n            a = [(yk in key) for yk in yes_key]\n            nf = [(nk not in key) for nk in no_key]\n            if np.all(nf) and np.any(a):\n                self.info(self.temp.format(key, val[-1]))\n\n    def get_output_name(self, suffix='.json'):\n        if not hasattr(self, 'option'): raise NotImplementedError('logger has no attr named \"option\"')\n        header = \"{}_\".format(self.option[\"algorithm\"])\n        if hasattr(self, 'coordinator'):\n            for para, pv in self.coordinator.algo_para.items():\n                header = header + para + \"{}_\".format(pv)\n        else:\n            if self.option['algo_para'] is not None:\n                header = header + 'algopara_'+'|'.join([str(p) for p in self.option['algo_para']])\n\n        output_name = header + \"M{}_R{}_B{}_\".format(self.option['model'], self.option['num_rounds'], self.option['batch_size'])\n        if self.option['num_steps']&lt;0:\n            output_name = output_name + (\"E{}_\".format(self.option['num_epochs']))\n        else:\n            output_name = output_name + (\"K{}_\".format(self.option['num_steps']))\n\n        output_name = output_name + \"LR{:.4f}_P{:.2f}_S{}_LD{:.3f}_WD{:.3f}_AVL{}_CN{}_CP{}_RS{}_LG{}\".format(\n                        self.option['learning_rate'],\n                        self.option['proportion'],\n                        self.option['seed'],\n                        self.option['lr_scheduler'] + self.option['learning_rate_decay'],\n                        self.option['weight_decay'],\n                        self.option['availability'],\n                        self.option['connectivity'],\n                        self.option['completeness'],\n                        self.option['responsiveness'],\n                        self.__class__.__name__,\n        ) + suffix\n        return output_name\n\n    def get_output_path(self):\n        if not hasattr(self, 'option'): raise NotImplementedError('logger has no attr named \"option\"')\n        return os.path.join(self.task_path, 'record')\n\n    def get_log_path(self):\n        return os.path.join(self.task_path, 'log')\n\n    def get_time_string(self):\n        return time.strftime('%Y-%m-%d-%H-%M-%S')\n\n    def early_stop(self):\n        # Early stopping when there is no improvement on the validation loss for more than self.option['early_stop'] rounds\n        if self.option['early_stop']&lt;0 or (self._es_key not in self.output): return False\n        score = -self.output[self._es_key][-1]\n        if np.isnan(score): return True\n        if self._es_best_score is None:\n            self._es_best_score = score\n            self._es_best_round = self.coordinator.current_round-1\n            self._es_patience = self.option['early_stop']\n        elif score&lt;self._es_best_score:\n            self._es_counter += 1\n            if self._es_counter &gt;= self._es_patience:\n                self.info('Early stopping after training for {} rounds.'.format(self.coordinator.current_round-1))\n                return True\n        else:\n            self._es_best_score = score\n            self._es_best_round = self.coordinator.current_round-1\n            self._es_counter = 0\n        return False\n\n    def initialize(self, *args, **kwargs):\n        return\n\n    def log_once(self, *args, **kwargs):\n\"\"\"This method is called at the beginning of each communication round of Server.\n        The round-wise operations of recording should be complemented here.\"\"\"\n        # calculate the testing metrics on testing dataset owned by coordinator\n        test_metric = self.coordinator.test()\n        for met_name, met_val in test_metric.items():\n            self.output['test_' + met_name].append(met_val)\n        # calculate weighted averaging of metrics on training datasets across participants\n        local_data_vols = [c.datavol for c in self.participants]\n        total_data_vol = sum(local_data_vols)\n        train_metrics = self.coordinator.global_test(flag='train')\n        for met_name, met_val in train_metrics.items():\n            self.output['train_' + met_name + '_dist'].append(met_val)\n            self.output['train_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n        # calculate weighted averaging and other statistics of metrics on validation datasets across clients\n        valid_metrics = self.coordinator.global_test(flag='valid')\n        for met_name, met_val in valid_metrics.items():\n            self.output['valid_'+met_name+'_dist'].append(met_val)\n            self.output['valid_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n            self.output['mean_valid_' + met_name].append(np.mean(met_val))\n            self.output['std_valid_' + met_name].append(np.std(met_val))\n        # output to stdout\n        self.show_current_output()\n\n    def organize_output(self, *args, **kwargs):\n\"\"\"This method will be called before saving self.output\"\"\"\n        for key in self.output.keys():\n            if '_dist' in key:\n                self.output[key] = self.output[key][-1]\n        return\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.check_if_log","title":"<code>check_if_log(round, eval_interval=-1)</code>","text":"<p>For evaluating every 'eval_interval' rounds, check whether to log at 'round'.</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def check_if_log(self, round, eval_interval=-1):\n\"\"\"For evaluating every 'eval_interval' rounds, check whether to log at 'round'.\"\"\"\n    self.current_round = round\n    return eval_interval &gt; 0 and (round == 0 or round % eval_interval == 0)\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.log_once","title":"<code>log_once(*args, **kwargs)</code>","text":"<p>This method is called at the beginning of each communication round of Server. The round-wise operations of recording should be complemented here.</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def log_once(self, *args, **kwargs):\n\"\"\"This method is called at the beginning of each communication round of Server.\n    The round-wise operations of recording should be complemented here.\"\"\"\n    # calculate the testing metrics on testing dataset owned by coordinator\n    test_metric = self.coordinator.test()\n    for met_name, met_val in test_metric.items():\n        self.output['test_' + met_name].append(met_val)\n    # calculate weighted averaging of metrics on training datasets across participants\n    local_data_vols = [c.datavol for c in self.participants]\n    total_data_vol = sum(local_data_vols)\n    train_metrics = self.coordinator.global_test(flag='train')\n    for met_name, met_val in train_metrics.items():\n        self.output['train_' + met_name + '_dist'].append(met_val)\n        self.output['train_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n    # calculate weighted averaging and other statistics of metrics on validation datasets across clients\n    valid_metrics = self.coordinator.global_test(flag='valid')\n    for met_name, met_val in valid_metrics.items():\n        self.output['valid_'+met_name+'_dist'].append(met_val)\n        self.output['valid_' + met_name].append(1.0 * sum([client_vol * client_met for client_vol, client_met in zip(local_data_vols, met_val)]) / total_data_vol)\n        self.output['mean_valid_' + met_name].append(np.mean(met_val))\n        self.output['std_valid_' + met_name].append(np.std(met_val))\n    # output to stdout\n    self.show_current_output()\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.organize_output","title":"<code>organize_output(*args, **kwargs)</code>","text":"<p>This method will be called before saving self.output</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def organize_output(self, *args, **kwargs):\n\"\"\"This method will be called before saving self.output\"\"\"\n    for key in self.output.keys():\n        if '_dist' in key:\n            self.output[key] = self.output[key][-1]\n    return\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.register_variable","title":"<code>register_variable(**kwargs)</code>","text":"<p>Initialze the logger in utils.fflow.initialize()</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def register_variable(self, **kwargs):\n\"\"\"Initialze the logger in utils.fflow.initialize()\"\"\"\n    for k, v in kwargs.items():\n        setattr(self, k, v)\n    return\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.save_output_as_json","title":"<code>save_output_as_json(filepath=None)</code>","text":"<p>Save the self.output as .json file</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def save_output_as_json(self, filepath=None):\n\"\"\"Save the self.output as .json file\"\"\"\n    if len(self.output) == 0: return\n    self.organize_output()\n    self.output_to_jsonable_dict()\n    if filepath is None:\n        filepath = os.path.join(self.get_output_path(),self.get_output_name())\n    if not self.overwrite:\n        if os.path.exists(filepath):\n            with open(filepath, 'r') as inf:\n                original_record = json.loads(inf.read())\n            o_keys = set(original_record.keys())\n            output_keys = set(self.output.keys())\n            new_keys = list(output_keys.difference(o_keys))\n            for k in new_keys:\n                original_record[k] = self.output[k]\n            self.output = original_record\n    try:\n        with open(filepath, 'w') as outf:\n            json.dump(dict(self.output), outf)\n    except:\n        self.error('Failed to save flw.logger.output as results')\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.time_end","title":"<code>time_end(key='')</code>","text":"<p>Create a timestamp that ends the event 'key' and print the time interval of the event.</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def time_end(self, key=''):\n\"\"\"Create a timestamp that ends the event 'key' and print the time interval of the event.\"\"\"\n    if key not in [k for k in self.time_buf.keys()]:\n        raise RuntimeError(\"Timer end before start.\")\n    else:\n        self.time_buf[key][-1] = time.time() - self.time_buf[key][-1]\n        self.info(\"{:&lt;30s}{:.4f}\".format(key + \":\", self.time_buf[key][-1]) + 's')\n        return self.time_buf[key][-1]\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.time_start","title":"<code>time_start(key='')</code>","text":"<p>Create a timestamp of the event 'key' starting</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def time_start(self, key=''):\n\"\"\"Create a timestamp of the event 'key' starting\"\"\"\n    if key not in [k for k in self.time_buf.keys()]:\n        self.time_buf[key] = []\n    self.time_buf[key].append(time.time())\n</code></pre>"},{"location":"Docs/experiment/logger/BasicLogger/#flgo.experiment.logger.BasicLogger.write_var_into_output","title":"<code>write_var_into_output(var_name=None, var_value=None)</code>","text":"<p>Add variable 'var_name' and its value var_value to logger</p> Source code in <code>flgo\\experiment\\logger\\__init__.py</code> <pre><code>def write_var_into_output(self, var_name=None, var_value=None):\n\"\"\"Add variable 'var_name' and its value var_value to logger\"\"\"\n    if var_name == None: raise RuntimeError(\"Missing the name of the variable to be logged.\")\n    self.output[var_name].append(var_value)\n    return\n</code></pre>"},{"location":"Docs/simulator/","title":"Index","text":""},{"location":"Docs/simulator/#flgo.simulator","title":"<code>flgo.simulator</code>","text":"<p>This module is to simulate arbitrary system heterogeneity that may occur in practice. We conclude four types of system heterogeneity from existing works.</p> System Heterogeneity Description <ol> <li> <p>Availability: the devices will be either available or unavailable at each moment, where only the                 available devices can be selected to participate in training.</p> </li> <li> <p>Responsiveness: the responsiveness describes the length of the period from the server broadcasting the                 gloabl model to the server receiving the locally trained model from a particular client.</p> </li> <li> <p>Completeness: since the server cannot fully control the behavior of devices,it's possible for devices to                 upload imcomplete model updates (i.e. only training for a few steps).</p> </li> <li> <p>Connectivity: the clients who promise to complete training may suffer accidients so that the server may lose                 connections with these client who will never return the currently trained local model.</p> </li> </ol> <p>We build up a client state machine to simulate the four types of system heterogeneity, and provide high-level APIs to allow customized system heterogeneity simulation.</p> <p>Example: How to customize the system heterogeneity:</p> <pre><code>&gt;&gt;&gt; class MySimulator(flgo.simulator.base.BasicSimulator):\n...     def update_client_availability(self):\n...         # update the variable 'prob_available' and 'prob_unavailable' for all the clients\n...         self.set_variable(self.all_clients, 'prob_available', [0.9 for _ in self.all_clients])\n...         self.set_variable(self.all_clients, 'prob_unavailable', [0.1 for _ in self.all_clients])\n...\n...     def update_client_connectivity(self, client_ids):\n...         # update the variable 'prob_drop' for clients in client_ids\n...         self.set_variable(client_ids, 'prob_drop', [0.1 for _ in client_ids])\n...\n...     def update_client_responsiveness(self, client_ids, *args, **kwargs):\n...         # update the variable 'latency' for clients in client_ids\n...         self.set_variable(client_ids, 'latency', [np.random.randint(5,100) for _ in client_ids])\n...\n...     def update_client_completeness(self, client_ids, *args, **kwargs):\n...         # update the variable 'working_amount' for clients in client_ids\n...         self.set_variable(client_ids, 'working_amount',  [max(int(self.clients[cid].num_steps*np.random.rand()), 1) for cid in client_ids])\n&gt;&gt;&gt; r = flgo.init(task, algorithm=fedavg, Simulator=MySimulator)\n&gt;&gt;&gt; # The runner r will be runned under the customized system heterogeneity, where the clients' states will be flushed by\n&gt;&gt;&gt; # MySimulator.update_client_xxx at each moment of the virtual clock or particular events happen (i.e. a client was selected)\n</code></pre> <p>We also provide some preset Simulator like flgo.simulator.DefaultSimulator and flgo.simulator.</p>"},{"location":"Docs/simulator/base/","title":"flgo.simulator.base","text":""},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator","title":"<code>BasicSimulator</code>","text":"<p>         Bases: <code>AbstractSimulator</code></p> <p>Simulate the system heterogeneity with the client state machine.</p> <p>Parameters:</p> Name Type Description Default <code>object</code> <code>list</code> <p>a list of objects in the federated scenario</p> required Source code in <code>flgo\\simulator\\base.py</code> <pre><code>class BasicSimulator(AbstractSimulator):\nr\"\"\"\n    Simulate the system heterogeneity with the client state machine.\n\n    Args:\n        object (list): a list of objects in the federated scenario\n    \"\"\"\n    _STATE = ['offline', 'idle', 'selected', 'working', 'dropped']\n    _VAR_NAMES = ['prob_available', 'prob_unavailable', 'prob_drop', 'working_amount', 'latency']\n    def __init__(self, objects, *args, **kwargs):\n        if len(objects)&gt;0:\n            self.server = objects[0]\n            self.clients = objects[1:]\n        else:\n            self.server = None\n            self.clients = []\n        self.all_clients = list(range(len(self.clients)))\n        self.random_module = np.random.RandomState(0) if random_seed_gen is None else np.random.RandomState(next(random_seed_gen))\n        # client states and the variables\n        self.client_states = ['idle' for _ in self.clients]\n        self.roundwise_fixed_availability = False\n        self.availability_latest_round = -1\n        self.variables = [{\n            'prob_available': 1.,\n            'prob_unavailable': 0.,\n            'prob_drop': 0.,\n            'working_amount': c.num_steps,\n            'latency': 0,\n        } for c in self.clients]\n        for var in self._VAR_NAMES:\n            self.set_variable(self.all_clients, var, [self.variables[cid][var] for cid in self.all_clients])\n        self.state_counter = [{'dropped_counter': 0, 'latency_counter': 0, } for _ in self.clients]\n\n    def get_client_with_state(self, state='idle'):\nr\"\"\"\n        Get clients according to their states.\n\n        Args:\n            state (str): the state in ['offline', 'idle', 'selected', 'working', 'dropped']\n\n        Returns:\n            a list of clients whose states are state\n        \"\"\"\n        return [cid for cid, cstate in enumerate(self.client_states) if cstate == state]\n\n    def set_client_state(self, client_ids, state):\nr\"\"\"\n        Set the states of clients in client_ids to the state\n\n        Args:\n            client_ids (list): a list of clients' ids\n            state (str): the state in ['offline', 'idle', 'selected', 'working', 'dropped']\n\n        Returns:\n            a list of clients whose states are state\n        \"\"\"\n        if state not in self._STATE: raise RuntimeError('{} not in the default state'.format(state))\n        if type(client_ids) is not list: client_ids = [client_ids]\n        for cid in client_ids: self.client_states[cid] = state\n        if state == 'dropped':\n            self.set_client_dropped_counter(client_ids)\n        if state == 'working':\n            self.set_client_latency_counter(client_ids)\n        if state == 'idle':\n            self.reset_client_counter(client_ids)\n\n    def set_client_latency_counter(self, client_ids = []):\nr\"\"\"Set the latency_counter\"\"\"\n        if type(client_ids) is not list: client_ids = [client_ids]\n        for cid in client_ids:\n            self.state_counter[cid]['dropped_counter'] = 0\n            self.state_counter[cid]['latency_counter'] = self.variables[cid]['latency']\n\n    def set_client_dropped_counter(self, client_ids = []):\nr\"\"\"Set the dropped_counter\"\"\"\n        if type(client_ids) is not list: client_ids = [client_ids]\n        for cid in client_ids:\n            self.state_counter[cid]['latency_counter'] = 0\n            self.state_counter[cid]['dropped_counter'] = self.server.get_tolerance_for_latency()\n\n    def reset_client_counter(self, client_ids = []):\nr\"\"\"Reset the clients' counter\"\"\"\n        if type(client_ids) is not list: client_ids = [client_ids]\n        for cid in client_ids:\n            self.state_counter[cid]['dropped_counter'] = self.state_counter[cid]['latency_counter'] = 0\n        return\n\n    @property\n    def idle_clients(self):\n\"\"\"Return ideal clients\"\"\"\n        return self.get_client_with_state('idle')\n\n    @property\n    def working_clients(self):\n\"\"\"Return working clients\"\"\"\n        return self.get_client_with_state('working')\n\n    @property\n    def offline_clients(self):\n\"\"\"Return offline clients\"\"\"\n        return self.get_client_with_state('offline')\n\n    @property\n    def selected_clients(self):\n\"\"\"Return the selected clients\"\"\"\n        return self.get_client_with_state('selected')\n\n    @property\n    def dropped_clients(self):\n\"\"\"Return the dropped clients\"\"\"\n        return self.get_client_with_state('dropped')\n\n    def get_variable(self, client_ids, varname):\nr\"\"\"\n        Get the simulator-private variables of the clients in client_ids according to varname\n\n        Args:\n            client_ids (list): a list of clients' ids\n            varname (str): the name of the simulator-private variable\n\n        Returns:\n            the simulator-private variables of the clients in client_ids\n        \"\"\"\n        if len(self.variables) ==0: return None\n        if type(client_ids) is not list: client_ids = [client_ids]\n        return [self.variables[cid][varname] if varname in self.variables[cid].keys() else None for cid in client_ids]\n\n    def set_variable(self, client_ids, varname, values):\nr\"\"\"\n        Set the simulator-private variables of the clients in client_ids to values\n\n        Args:\n            client_ids (list): a list of clients' ids\n            varname (str): the name of the simulator-private variable\n            values (list): a list of things\n        \"\"\"\n        if type(client_ids) is not list: client_ids = [client_ids]\n        if type(values) is not list: values = [values]\n        assert len(client_ids) == len(values)\n        for cid, v in zip(client_ids, values):\n            self.variables[cid][varname] = v\n            setattr(self.clients[cid], '_'+varname, v)\n\n    def update_client_availability(self, *args, **kwargs):\n\"\"\"API to update client availability every time unit\"\"\"\n        return\n\n    def update_client_connectivity(self, client_ids, *args, **kwargs):\n\"\"\"API to update client connectivity every time unit\"\"\"\n        return\n\n    def update_client_completeness(self, client_ids, *args, **kwargs):\n\"\"\"API to update client completeness every time unit\"\"\"\n        return\n\n    def update_client_responsiveness(self, client_ids, *args, **kwargs):\n\"\"\"API to update client responsiveness every time unit\"\"\"\n        return\n\n    def flush(self):\n\"\"\"Flush the client state machine as time goes by\"\"\"\n        # +++++++++++++++++++ availability +++++++++++++++++++++\n        # change self.variables[cid]['prob_available'] and self.variables[cid]['prob_unavailable'] for each client `cid`\n        self.update_client_availability()\n        # update states for offline &amp; idle clients\n        if len(self.idle_clients)==0 or not self.roundwise_fixed_availability or self.server.current_round &gt; self.availability_latest_round:\n            self.availability_latest_round = self.server.current_round\n            offline_clients = {cid: 'offline' for cid in self.offline_clients}\n            idle_clients = {cid:'idle' for cid in self.idle_clients}\n            for cid in offline_clients:\n                if (self.random_module.rand() &lt;= self.variables[cid]['prob_available']): offline_clients[cid] = 'idle'\n            for cid in self.idle_clients:\n                if  (self.random_module.rand() &lt;= self.variables[cid]['prob_unavailable']): idle_clients[cid] = 'offline'\n            new_idle_clients = [cid for cid in offline_clients if offline_clients[cid] == 'idle']\n            new_offline_clients = [cid for cid in idle_clients if idle_clients[cid] == 'offline']\n            self.set_client_state(new_idle_clients, 'idle')\n            self.set_client_state(new_offline_clients, 'offline')\n        # update states for dropped clients\n        for cid in self.dropped_clients:\n            self.state_counter[cid]['dropped_counter'] -= 1\n            if self.state_counter[cid]['dropped_counter'] &lt; 0:\n                self.state_counter[cid]['dropped_counter'] = 0\n                self.client_states[cid] = 'offline'\n                if (self.random_module.rand() &lt; self.variables[cid]['prob_unavailable']):\n                    self.set_client_state([cid], 'offline')\n                else:\n                    self.set_client_state([cid], 'idle')\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.dropped_clients","title":"<code>dropped_clients</code>  <code>property</code>","text":"<p>Return the dropped clients</p>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.idle_clients","title":"<code>idle_clients</code>  <code>property</code>","text":"<p>Return ideal clients</p>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.offline_clients","title":"<code>offline_clients</code>  <code>property</code>","text":"<p>Return offline clients</p>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.selected_clients","title":"<code>selected_clients</code>  <code>property</code>","text":"<p>Return the selected clients</p>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.working_clients","title":"<code>working_clients</code>  <code>property</code>","text":"<p>Return working clients</p>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.flush","title":"<code>flush()</code>","text":"<p>Flush the client state machine as time goes by</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def flush(self):\n\"\"\"Flush the client state machine as time goes by\"\"\"\n    # +++++++++++++++++++ availability +++++++++++++++++++++\n    # change self.variables[cid]['prob_available'] and self.variables[cid]['prob_unavailable'] for each client `cid`\n    self.update_client_availability()\n    # update states for offline &amp; idle clients\n    if len(self.idle_clients)==0 or not self.roundwise_fixed_availability or self.server.current_round &gt; self.availability_latest_round:\n        self.availability_latest_round = self.server.current_round\n        offline_clients = {cid: 'offline' for cid in self.offline_clients}\n        idle_clients = {cid:'idle' for cid in self.idle_clients}\n        for cid in offline_clients:\n            if (self.random_module.rand() &lt;= self.variables[cid]['prob_available']): offline_clients[cid] = 'idle'\n        for cid in self.idle_clients:\n            if  (self.random_module.rand() &lt;= self.variables[cid]['prob_unavailable']): idle_clients[cid] = 'offline'\n        new_idle_clients = [cid for cid in offline_clients if offline_clients[cid] == 'idle']\n        new_offline_clients = [cid for cid in idle_clients if idle_clients[cid] == 'offline']\n        self.set_client_state(new_idle_clients, 'idle')\n        self.set_client_state(new_offline_clients, 'offline')\n    # update states for dropped clients\n    for cid in self.dropped_clients:\n        self.state_counter[cid]['dropped_counter'] -= 1\n        if self.state_counter[cid]['dropped_counter'] &lt; 0:\n            self.state_counter[cid]['dropped_counter'] = 0\n            self.client_states[cid] = 'offline'\n            if (self.random_module.rand() &lt; self.variables[cid]['prob_unavailable']):\n                self.set_client_state([cid], 'offline')\n            else:\n                self.set_client_state([cid], 'idle')\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.get_client_with_state","title":"<code>get_client_with_state(state='idle')</code>","text":"<p>Get clients according to their states.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>str</code> <p>the state in ['offline', 'idle', 'selected', 'working', 'dropped']</p> <code>'idle'</code> <p>Returns:</p> Type Description <p>a list of clients whose states are state</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def get_client_with_state(self, state='idle'):\nr\"\"\"\n    Get clients according to their states.\n\n    Args:\n        state (str): the state in ['offline', 'idle', 'selected', 'working', 'dropped']\n\n    Returns:\n        a list of clients whose states are state\n    \"\"\"\n    return [cid for cid, cstate in enumerate(self.client_states) if cstate == state]\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.get_variable","title":"<code>get_variable(client_ids, varname)</code>","text":"<p>Get the simulator-private variables of the clients in client_ids according to varname</p> <p>Parameters:</p> Name Type Description Default <code>client_ids</code> <code>list</code> <p>a list of clients' ids</p> required <code>varname</code> <code>str</code> <p>the name of the simulator-private variable</p> required <p>Returns:</p> Type Description <p>the simulator-private variables of the clients in client_ids</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def get_variable(self, client_ids, varname):\nr\"\"\"\n    Get the simulator-private variables of the clients in client_ids according to varname\n\n    Args:\n        client_ids (list): a list of clients' ids\n        varname (str): the name of the simulator-private variable\n\n    Returns:\n        the simulator-private variables of the clients in client_ids\n    \"\"\"\n    if len(self.variables) ==0: return None\n    if type(client_ids) is not list: client_ids = [client_ids]\n    return [self.variables[cid][varname] if varname in self.variables[cid].keys() else None for cid in client_ids]\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.reset_client_counter","title":"<code>reset_client_counter(client_ids=[])</code>","text":"<p>Reset the clients' counter</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def reset_client_counter(self, client_ids = []):\nr\"\"\"Reset the clients' counter\"\"\"\n    if type(client_ids) is not list: client_ids = [client_ids]\n    for cid in client_ids:\n        self.state_counter[cid]['dropped_counter'] = self.state_counter[cid]['latency_counter'] = 0\n    return\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.set_client_dropped_counter","title":"<code>set_client_dropped_counter(client_ids=[])</code>","text":"<p>Set the dropped_counter</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def set_client_dropped_counter(self, client_ids = []):\nr\"\"\"Set the dropped_counter\"\"\"\n    if type(client_ids) is not list: client_ids = [client_ids]\n    for cid in client_ids:\n        self.state_counter[cid]['latency_counter'] = 0\n        self.state_counter[cid]['dropped_counter'] = self.server.get_tolerance_for_latency()\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.set_client_latency_counter","title":"<code>set_client_latency_counter(client_ids=[])</code>","text":"<p>Set the latency_counter</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def set_client_latency_counter(self, client_ids = []):\nr\"\"\"Set the latency_counter\"\"\"\n    if type(client_ids) is not list: client_ids = [client_ids]\n    for cid in client_ids:\n        self.state_counter[cid]['dropped_counter'] = 0\n        self.state_counter[cid]['latency_counter'] = self.variables[cid]['latency']\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.set_client_state","title":"<code>set_client_state(client_ids, state)</code>","text":"<p>Set the states of clients in client_ids to the state</p> <p>Parameters:</p> Name Type Description Default <code>client_ids</code> <code>list</code> <p>a list of clients' ids</p> required <code>state</code> <code>str</code> <p>the state in ['offline', 'idle', 'selected', 'working', 'dropped']</p> required <p>Returns:</p> Type Description <p>a list of clients whose states are state</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def set_client_state(self, client_ids, state):\nr\"\"\"\n    Set the states of clients in client_ids to the state\n\n    Args:\n        client_ids (list): a list of clients' ids\n        state (str): the state in ['offline', 'idle', 'selected', 'working', 'dropped']\n\n    Returns:\n        a list of clients whose states are state\n    \"\"\"\n    if state not in self._STATE: raise RuntimeError('{} not in the default state'.format(state))\n    if type(client_ids) is not list: client_ids = [client_ids]\n    for cid in client_ids: self.client_states[cid] = state\n    if state == 'dropped':\n        self.set_client_dropped_counter(client_ids)\n    if state == 'working':\n        self.set_client_latency_counter(client_ids)\n    if state == 'idle':\n        self.reset_client_counter(client_ids)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.set_variable","title":"<code>set_variable(client_ids, varname, values)</code>","text":"<p>Set the simulator-private variables of the clients in client_ids to values</p> <p>Parameters:</p> Name Type Description Default <code>client_ids</code> <code>list</code> <p>a list of clients' ids</p> required <code>varname</code> <code>str</code> <p>the name of the simulator-private variable</p> required <code>values</code> <code>list</code> <p>a list of things</p> required Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def set_variable(self, client_ids, varname, values):\nr\"\"\"\n    Set the simulator-private variables of the clients in client_ids to values\n\n    Args:\n        client_ids (list): a list of clients' ids\n        varname (str): the name of the simulator-private variable\n        values (list): a list of things\n    \"\"\"\n    if type(client_ids) is not list: client_ids = [client_ids]\n    if type(values) is not list: values = [values]\n    assert len(client_ids) == len(values)\n    for cid, v in zip(client_ids, values):\n        self.variables[cid][varname] = v\n        setattr(self.clients[cid], '_'+varname, v)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.update_client_availability","title":"<code>update_client_availability(*args, **kwargs)</code>","text":"<p>API to update client availability every time unit</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def update_client_availability(self, *args, **kwargs):\n\"\"\"API to update client availability every time unit\"\"\"\n    return\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.update_client_completeness","title":"<code>update_client_completeness(client_ids, *args, **kwargs)</code>","text":"<p>API to update client completeness every time unit</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def update_client_completeness(self, client_ids, *args, **kwargs):\n\"\"\"API to update client completeness every time unit\"\"\"\n    return\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.update_client_connectivity","title":"<code>update_client_connectivity(client_ids, *args, **kwargs)</code>","text":"<p>API to update client connectivity every time unit</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def update_client_connectivity(self, client_ids, *args, **kwargs):\n\"\"\"API to update client connectivity every time unit\"\"\"\n    return\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.BasicSimulator.update_client_responsiveness","title":"<code>update_client_responsiveness(client_ids, *args, **kwargs)</code>","text":"<p>API to update client responsiveness every time unit</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def update_client_responsiveness(self, client_ids, *args, **kwargs):\n\"\"\"API to update client responsiveness every time unit\"\"\"\n    return\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock","title":"<code>ElemClock</code>","text":"<p>Simulate the clock by the timestamp of each Element</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>class ElemClock:\nr\"\"\"Simulate the clock by the timestamp of each Element\"\"\"\n    class Elem:\nr\"\"\"\n        Element with a timestamp\n\n        Args:\n            x: element\n            time (int): the timestamp\n        \"\"\"\n        def __init__(self, x, time):\n            self.x = x\n            self.time = time\n\n        def __str__(self):\n            return '{} at Time {}'.format(self.x, self.time)\n\n        def __lt__(self, other):\n            return self.time &lt; other.time\n\n    def __init__(self):\n        self.q = PriorityQueue()\n        self.time = 0\n        self.simulator = None\n\n    def step(self, delta_t=1):\nr\"\"\"\n        Step delta_t units of the virtual time\n\n        Args:\n            delta_t (int): the delta of time\n        \"\"\"\n        if delta_t &lt; 0: raise RuntimeError(\"Cannot inverse time of simulator.base.clock.\")\n        if self.simulator is not None:\n            for t in range(delta_t):\n                self.simulator.flush()\n        self.time += delta_t\n\n    def set_time(self, t):\nr\"\"\"\n        Set time\n\n        Args:\n            t (int): time\n        \"\"\"\n        if t &lt; self.time: raise RuntimeError(\"Cannot inverse time of simulator.base.clock.\")\n        self.time = t\n\n    def put(self, x, time):\nr\"\"\"\n        Put an element into the time queue with timestamp\n\n        Args:\n            x: element\n            time (int): the timestamp\n        \"\"\"\n        self.q.put(self.Elem(x, time))\n\n    def get(self):\nr\"\"\"\n        Get an element from the queue\n\n        Returns:\n            the element in the nearest coming time\n        \"\"\"\n        if self.q.empty(): return None\n        return self.q.get().x\n\n    def get_until(self, t):\nr\"\"\"\n        Get elements from the queue until time t\n\n        Args:\n            t (int): time\n\n        Returns:\n            a list of elements whose timestamps is no larger than t\n        \"\"\"\n        res = []\n        while not self.empty():\n            elem = self.q.get()\n            if elem.time &gt; t:\n                self.put(elem.x, elem.time)\n                break\n            pkg = elem.x\n            res.append(pkg)\n        return res\n\n    def get_sofar(self):\nr\"\"\"\n        Get elements from the queue until now\n\n        Returns:\n            a list of elements whose timestamps is no larger than the current time\n        \"\"\"\n        return self.get_until(self.current_time)\n\n    def gets(self):\nr\"\"\"\n        Get all the elements in the queue\n\n        Returns:\n            a list of elements in the queue\n        \"\"\"\n        if self.empty(): return []\n        res = []\n        while not self.empty(): res.append(self.q.get())\n        res = [rx.x for rx in res]\n        return res\n\n    def clear(self):\nr\"\"\"\n        Clear the queue\n        \"\"\"\n        while not self.empty():\n            self.get()\n\n    def conditionally_clear(self, f):\nr\"\"\"\n        Clear elements if f(element) is False\n\n        Args:\n            f (function): a function that receives element and returns bool variable\n        \"\"\"\n        buf = []\n        while not self.empty(): buf.append(self.q.get())\n        for elem in buf:\n            if not f(elem.x): self.q.put(elem)\n        return\n\n    def empty(self):\nr\"\"\"Return whether the queue is empty\"\"\"\n        return self.q.empty()\n\n    @ property\n    def current_time(self):\nr\"\"\"Return the current time\"\"\"\n        return self.time\n\n    def register_simulator(self, simulator):\nr\"\"\"Set self.simulator=simulator\"\"\"\n        self.simulator = simulator\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.current_time","title":"<code>current_time</code>  <code>property</code>","text":"<p>Return the current time</p>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.Elem","title":"<code>Elem</code>","text":"<p>Element with a timestamp</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>element</p> required <code>time</code> <code>int</code> <p>the timestamp</p> required Source code in <code>flgo\\simulator\\base.py</code> <pre><code>class Elem:\nr\"\"\"\n    Element with a timestamp\n\n    Args:\n        x: element\n        time (int): the timestamp\n    \"\"\"\n    def __init__(self, x, time):\n        self.x = x\n        self.time = time\n\n    def __str__(self):\n        return '{} at Time {}'.format(self.x, self.time)\n\n    def __lt__(self, other):\n        return self.time &lt; other.time\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.clear","title":"<code>clear()</code>","text":"<p>Clear the queue</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def clear(self):\nr\"\"\"\n    Clear the queue\n    \"\"\"\n    while not self.empty():\n        self.get()\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.conditionally_clear","title":"<code>conditionally_clear(f)</code>","text":"<p>Clear elements if f(element) is False</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>function</code> <p>a function that receives element and returns bool variable</p> required Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def conditionally_clear(self, f):\nr\"\"\"\n    Clear elements if f(element) is False\n\n    Args:\n        f (function): a function that receives element and returns bool variable\n    \"\"\"\n    buf = []\n    while not self.empty(): buf.append(self.q.get())\n    for elem in buf:\n        if not f(elem.x): self.q.put(elem)\n    return\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.empty","title":"<code>empty()</code>","text":"<p>Return whether the queue is empty</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def empty(self):\nr\"\"\"Return whether the queue is empty\"\"\"\n    return self.q.empty()\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.get","title":"<code>get()</code>","text":"<p>Get an element from the queue</p> <p>Returns:</p> Type Description <p>the element in the nearest coming time</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def get(self):\nr\"\"\"\n    Get an element from the queue\n\n    Returns:\n        the element in the nearest coming time\n    \"\"\"\n    if self.q.empty(): return None\n    return self.q.get().x\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.get_sofar","title":"<code>get_sofar()</code>","text":"<p>Get elements from the queue until now</p> <p>Returns:</p> Type Description <p>a list of elements whose timestamps is no larger than the current time</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def get_sofar(self):\nr\"\"\"\n    Get elements from the queue until now\n\n    Returns:\n        a list of elements whose timestamps is no larger than the current time\n    \"\"\"\n    return self.get_until(self.current_time)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.get_until","title":"<code>get_until(t)</code>","text":"<p>Get elements from the queue until time t</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>int</code> <p>time</p> required <p>Returns:</p> Type Description <p>a list of elements whose timestamps is no larger than t</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def get_until(self, t):\nr\"\"\"\n    Get elements from the queue until time t\n\n    Args:\n        t (int): time\n\n    Returns:\n        a list of elements whose timestamps is no larger than t\n    \"\"\"\n    res = []\n    while not self.empty():\n        elem = self.q.get()\n        if elem.time &gt; t:\n            self.put(elem.x, elem.time)\n            break\n        pkg = elem.x\n        res.append(pkg)\n    return res\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.gets","title":"<code>gets()</code>","text":"<p>Get all the elements in the queue</p> <p>Returns:</p> Type Description <p>a list of elements in the queue</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def gets(self):\nr\"\"\"\n    Get all the elements in the queue\n\n    Returns:\n        a list of elements in the queue\n    \"\"\"\n    if self.empty(): return []\n    res = []\n    while not self.empty(): res.append(self.q.get())\n    res = [rx.x for rx in res]\n    return res\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.put","title":"<code>put(x, time)</code>","text":"<p>Put an element into the time queue with timestamp</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>element</p> required <code>time</code> <code>int</code> <p>the timestamp</p> required Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def put(self, x, time):\nr\"\"\"\n    Put an element into the time queue with timestamp\n\n    Args:\n        x: element\n        time (int): the timestamp\n    \"\"\"\n    self.q.put(self.Elem(x, time))\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.register_simulator","title":"<code>register_simulator(simulator)</code>","text":"<p>Set self.simulator=simulator</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def register_simulator(self, simulator):\nr\"\"\"Set self.simulator=simulator\"\"\"\n    self.simulator = simulator\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.set_time","title":"<code>set_time(t)</code>","text":"<p>Set time</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>int</code> <p>time</p> required Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def set_time(self, t):\nr\"\"\"\n    Set time\n\n    Args:\n        t (int): time\n    \"\"\"\n    if t &lt; self.time: raise RuntimeError(\"Cannot inverse time of simulator.base.clock.\")\n    self.time = t\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.ElemClock.step","title":"<code>step(delta_t=1)</code>","text":"<p>Step delta_t units of the virtual time</p> <p>Parameters:</p> Name Type Description Default <code>delta_t</code> <code>int</code> <p>the delta of time</p> <code>1</code> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def step(self, delta_t=1):\nr\"\"\"\n    Step delta_t units of the virtual time\n\n    Args:\n        delta_t (int): the delta of time\n    \"\"\"\n    if delta_t &lt; 0: raise RuntimeError(\"Cannot inverse time of simulator.base.clock.\")\n    if self.simulator is not None:\n        for t in range(delta_t):\n            self.simulator.flush()\n    self.time += delta_t\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.PriorityQueue","title":"<code>PriorityQueue</code>","text":"<p>Priority Queue</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>class PriorityQueue:\nr\"\"\"Priority Queue\"\"\"\n    def __init__(self):\n        self.queue = []\n\n    def size(self):\nr\"\"\"The size of the queue\"\"\"\n        return len(self.queue)\n\n    def empty(self):\nr\"\"\"Return whether the queue is empty\"\"\"\n        return len(self.queue)==0\n\n    def put(self, item):\nr\"\"\"Put item into the queue\"\"\"\n        heapq.heappush(self.queue, item)\n\n    def get(self):\nr\"\"\"Get item from the queue\"\"\"\n        return heapq.heappop(self.queue)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.PriorityQueue.empty","title":"<code>empty()</code>","text":"<p>Return whether the queue is empty</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def empty(self):\nr\"\"\"Return whether the queue is empty\"\"\"\n    return len(self.queue)==0\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.PriorityQueue.get","title":"<code>get()</code>","text":"<p>Get item from the queue</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def get(self):\nr\"\"\"Get item from the queue\"\"\"\n    return heapq.heappop(self.queue)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.PriorityQueue.put","title":"<code>put(item)</code>","text":"<p>Put item into the queue</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def put(self, item):\nr\"\"\"Put item into the queue\"\"\"\n    heapq.heappush(self.queue, item)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.PriorityQueue.size","title":"<code>size()</code>","text":"<p>The size of the queue</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def size(self):\nr\"\"\"The size of the queue\"\"\"\n    return len(self.queue)\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.seed_generator","title":"<code>seed_generator(seed=0)</code>","text":"<p>Return an integer as the seed</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def seed_generator(seed=0):\n\"\"\"Return an integer as the seed\"\"\"\n    while True:\n        yield seed+1\n        seed+=1\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.size_of_package","title":"<code>size_of_package(package)</code>","text":"<p>Compute the size of the package</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>dict</code> <p>the pacakge</p> required <p>Returns:</p> Name Type Description <code>size</code> <code>int</code> <p>the size of the package</p> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def size_of_package(package):\nr\"\"\"\n    Compute the size of the package\n\n    Args:\n        package (dict): the pacakge\n\n    Returns:\n        size (int): the size of the package\n    \"\"\"\n    size = 0\n    for v in package.values():\n        if type(v) is torch.Tensor:\n            size += sys.getsizeof(v.storage())\n        else:\n            size += v.__sizeof__()\n    return size\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.with_availability","title":"<code>with_availability(sample)</code>","text":"<p>The decorator for sampling with client availability</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.algorithm.fedbase\n    &gt;&gt;&gt; import flgo.simulator.base as ss\n    &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n    ...     @ss.with_availability\n    ...     def sample(self):\n    ...         ...\n</code></pre> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def with_availability(sample):\nr\"\"\"\n    The decorator for sampling with client availability\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.algorithm.fedbase\n        &gt;&gt;&gt; import flgo.simulator.base as ss\n        &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n        ...     @ss.with_availability\n        ...     def sample(self):\n        ...         ...\n    ```\n    \"\"\"\n    @functools.wraps(sample)\n    def sample_with_availability(self):\n        available_clients = self.gv.simulator.idle_clients\n        # ensure that there is at least one client to be available at the current moment\n        # while len(available_clients) == 0:\n        #     self.gv.clock.step()\n        #     available_clients = self.gv.simulator.idle_clients\n        # call the original sampling function\n        selected_clients = sample(self)\n        # filter the selected but unavailable clients\n        effective_clients = set(selected_clients).intersection(set(available_clients))\n        # return the selected and available clients (e.g. sampling with replacement should be considered here)\n        self._unavailable_selected_clients = [cid for cid in selected_clients if cid not in effective_clients]\n        if len(self._unavailable_selected_clients)&gt;0:\n            self.gv.logger.info('The selected clients {} are not currently available.'.format(self._unavailable_selected_clients))\n        selected_clients = [cid for cid in selected_clients if cid in effective_clients]\n        self.gv.simulator.set_client_state(selected_clients, 'selected')\n        return selected_clients\n    return sample_with_availability\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.with_clock","title":"<code>with_clock(communicate)</code>","text":"<p>The decorator to simulate the scene where there is a virtual global clock</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.algorithm.fedbase\n    &gt;&gt;&gt; import flgo.simulator.base as ss\n    &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n    ...     @ss.with_clock\n    ...     def communicate(self,...):\n    ...         ...\n</code></pre> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def with_clock(communicate):\nr\"\"\"\n    The decorator to simulate the scene where there is a virtual global clock\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.algorithm.fedbase\n        &gt;&gt;&gt; import flgo.simulator.base as ss\n        &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n        ...     @ss.with_clock\n        ...     def communicate(self,...):\n        ...         ...\n    ```\n    \"\"\"\n    def communicate_with_clock(self, selected_clients, mtype=0, asynchronous=False):\n        self.gv.simulator.update_client_completeness(selected_clients)\n        res = communicate(self, selected_clients, mtype, asynchronous)\n        # If all the selected clients are unavailable, directly return the result without waiting.\n        # Else if all the available clients have dropped out and not using asynchronous communication,  waiting for `tolerance_for_latency` time units.\n        tolerance_for_latency = self.get_tolerance_for_latency()\n        if not asynchronous and len(selected_clients)==0:\n            if hasattr(self, '_dropped_selected_clients') and len(self._dropped_selected_clients)&gt;0:\n                self.gv.clock.step(tolerance_for_latency)\n            return res\n        # Convert the unpacked packages to a list of packages of each client.\n        pkgs = [{key: vi[id] for key, vi in res.items()} for id in range(len(list(res.values())[0]))] if len(selected_clients)&gt;0 else []\n        # Put the packages from selected clients into clock only if when there are effective selected clients\n        if len(selected_clients)&gt;0:\n            # Set selected clients' states as `working`\n            self.gv.simulator.set_client_state(selected_clients, 'working')\n            for pi in pkgs: self.gv.clock.put(pi, pi['__t'])\n        # Receiving packages in asynchronous\\synchronous way\n        # Wait for client packages. If communicating in asynchronous way, the waiting time is 0.\n        if asynchronous:\n            # Return the currently received packages to the server\n            eff_pkgs = self.gv.clock.get_until(self.gv.clock.current_time)\n            eff_cids = [pkg_i['__cid'] for pkg_i in eff_pkgs]\n        else:\n            # Wait all the selected clients for no more than `tolerance_for_latency` time units.\n            # Check if anyone had dropped out or will be overdue\n            max_latency = max(self.gv.simulator.get_variable(selected_clients, 'latency'))\n            any_drop, any_overdue = (len(self._dropped_selected_clients) &gt; 0), (max_latency &gt;  tolerance_for_latency)\n            # Compute delta of time for the communication.\n            delta_t = tolerance_for_latency if any_drop or any_overdue else max_latency\n            # Receive packages within due\n            eff_pkgs = self.gv.clock.get_until(self.gv.clock.current_time + delta_t)\n            self.gv.clock.step(delta_t)\n            # Drop the packages of overdue clients and reset their states to `idle`\n            eff_cids = [pkg_i['__cid'] for pkg_i in eff_pkgs]\n            self._overdue_clients = list(set([cid for cid in selected_clients if cid not in eff_cids]))\n            # no additional wait for the synchronous selected clients and preserve the later packages from asynchronous clients\n            if len(self._overdue_clients) &gt; 0:\n                self.gv.clock.conditionally_clear(lambda x: x['__cid'] in self._overdue_clients)\n                self.gv.simulator.set_client_state(self._overdue_clients, 'idle')\n            # Resort effective packages\n            pkg_map = {pkg_i['__cid']: pkg_i for pkg_i in eff_pkgs}\n            eff_pkgs = [pkg_map[cid] for cid in selected_clients if cid in eff_cids]\n        self.gv.simulator.set_client_state(eff_cids, 'offline')\n        self.received_clients = [pkg_i['__cid'] for pkg_i in eff_pkgs]\n        return self.unpack(eff_pkgs)\n    return communicate_with_clock\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.with_completeness","title":"<code>with_completeness(train)</code>","text":"<p>The decorator to simulate the scene where the clients may upload incomplete model updates</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.algorithm.fedbase\n    &gt;&gt;&gt; import flgo.simulator.base as ss\n    &gt;&gt;&gt; class Client(flgo.algorithm.fedbase.BasicClient):\n    ...     @ss.with_completeness\n    ...     def train(self,...):\n    ...         ...\n</code></pre> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def with_completeness(train):\nr\"\"\"\n    The decorator to simulate the scene where the clients may upload incomplete model updates\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.algorithm.fedbase\n        &gt;&gt;&gt; import flgo.simulator.base as ss\n        &gt;&gt;&gt; class Client(flgo.algorithm.fedbase.BasicClient):\n        ...     @ss.with_completeness\n        ...     def train(self,...):\n        ...         ...\n    ```\n    \"\"\"\n    @functools.wraps(train)\n    def train_with_incomplete_update(self, model, *args, **kwargs):\n        old_num_steps = self.num_steps\n        self.num_steps = self._working_amount\n        res = train(self, model, *args, **kwargs)\n        self.num_steps = old_num_steps\n        return res\n    return train_with_incomplete_update\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.with_dropout","title":"<code>with_dropout(communicate)</code>","text":"<p>The decorator for communicating to simulate the scene where clients may drop out</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.algorithm.fedbase\n    &gt;&gt;&gt; import flgo.simulator.base as ss\n    &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n    ...     @ss.with_dropout\n    ...     def communicate(self,...):\n    ...         ...\n</code></pre> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def with_dropout(communicate):\nr\"\"\"\n    The decorator for communicating to simulate the scene where clients may drop out\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.algorithm.fedbase\n        &gt;&gt;&gt; import flgo.simulator.base as ss\n        &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n        ...     @ss.with_dropout\n        ...     def communicate(self,...):\n        ...         ...\n    ```\n    \"\"\"\n    @functools.wraps(communicate)\n    def communicate_with_dropout(self, selected_clients, mtype=0, asynchronous=False):\n        if len(selected_clients) &gt; 0:\n            self.gv.simulator.update_client_connectivity(selected_clients)\n            probs_drop = self.gv.simulator.get_variable(selected_clients, 'prob_drop')\n            self._dropped_selected_clients = [cid for cid,prob in zip(selected_clients, probs_drop) if self.gv.simulator.random_module.rand() &lt;= prob]\n            self.gv.simulator.set_client_state(self._dropped_selected_clients, 'dropped')\n            return communicate(self, [cid for cid in selected_clients if cid not in self._dropped_selected_clients], mtype, asynchronous)\n        else:\n            return communicate(self, selected_clients, mtype, asynchronous)\n    return communicate_with_dropout\n</code></pre>"},{"location":"Docs/simulator/base/#flgo.simulator.base.with_latency","title":"<code>with_latency(communicate_with)</code>","text":"<p>The decorator to simulate the scene where there are network latencies during communication</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo.algorithm.fedbase\n    &gt;&gt;&gt; import flgo.simulator.base as ss\n    &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n    ...     @ss.with_latency\n    ...     def communicate_with(self,...):\n    ...         ...\n</code></pre> Source code in <code>flgo\\simulator\\base.py</code> <pre><code>def with_latency(communicate_with):\nr\"\"\"\n    The decorator to simulate the scene where there are network latencies during communication\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo.algorithm.fedbase\n        &gt;&gt;&gt; import flgo.simulator.base as ss\n        &gt;&gt;&gt; class Server(flgo.algorithm.fedbase.BasicServer):\n        ...     @ss.with_latency\n        ...     def communicate_with(self,...):\n        ...         ...\n    ```\n    \"\"\"\n    @functools.wraps(communicate_with)\n    def delayed_communicate_with(self, target_id, package):\n        # Calculate latency for the target client\n        # Set local model size of clients for computation cost estimation\n        model_size = package['model'].count_parameters(output=False) if 'model' in package.keys() else 0\n        self.gv.simulator.set_variable(target_id, '__model_size', model_size)\n        # Set downloading package sizes for clients for downloading cost estimation\n        self.gv.simulator.set_variable(target_id, '__download_package_size',size_of_package(package))\n        res = communicate_with(self, target_id, package)\n        # Set uploading package sizes for clients for uploading cost estimation\n        self.gv.simulator.set_variable(target_id, '__upload_package_size', size_of_package(res))\n        # update latency of the target client according to the communication cost and computation cost\n        self.gv.simulator.update_client_responsiveness([target_id])\n        # Record the size of the package that may influence the value of the latency\n        # Update the real-time latency of the client response\n        # Get the updated latency\n        latency = self.gv.simulator.get_variable(target_id, 'latency')[0]\n        self.clients[target_id]._latency = latency\n        res['__cid'] = target_id\n        # Compute the arrival time\n        res['__t'] = self.gv.clock.current_time + latency\n        return res\n    return delayed_communicate_with\n</code></pre>"},{"location":"Docs/simulator/default_simulator/","title":"flgo.simulator.default_simulator","text":"<p>This simulator supports for the following system heterogeneity:</p> <p>availability_modes = {     'IDL': ideal_client_availability,     'YMF': y_max_first_client_availability,     'MDF': more_data_first_client_availability,     'LDF': less_data_first_client_availability,     'YFF': y_fewer_first_client_availability,     'HOMO': homogeneous_client_availability,     'LN': lognormal_client_availability,     'SLN': sin_lognormal_client_availability,     'YC': y_cycle_client_availability, }</p> <p>connectivity_modes = {     'IDL': ideal_client_connectivity,     'HOMO': homogeneous_client_connectivity, }</p> <p>completeness_modes = {     'IDL': ideal_client_completeness,     'PDU': part_dynamic_uniform_client_completeness,     'FSU': full_static_unifrom_client_completeness,     'ADU': arbitrary_dynamic_unifrom_client_completeness,     'ASU': arbitrary_static_unifrom_client_completeness, }</p> <p>responsiveness_modes = {     'IDL': ideal_client_responsiveness,     'LN': lognormal_client_responsiveness,     'UNI': uniform_client_responsiveness, }</p>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.arbitrary_dynamic_unifrom_client_completeness","title":"<code>arbitrary_dynamic_unifrom_client_completeness(simulator, a=1, b=1)</code>","text":"<p>This setting follows the setting in the paper 'Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization' (http://arxiv.org/abs/2007.07481). The string <code>mode</code> should be like 'FEDNOVA-Uniform(a,b)' where <code>a</code> is the minimal value of the number of local epochs and <code>b</code> is the maximal value. If this mode is active, the <code>num_epochs</code> and <code>num_steps</code> of clients will be disable.</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def arbitrary_dynamic_unifrom_client_completeness(simulator, a=1, b=1):\n\"\"\"\n    This setting follows the setting in the paper 'Tackling the Objective Inconsistency Problem in\n    Heterogeneous Federated Optimization' (http://arxiv.org/abs/2007.07481). The string `mode` should be like\n    'FEDNOVA-Uniform(a,b)' where `a` is the minimal value of the number of local epochs and `b` is the maximal\n    value. If this mode is active, the `num_epochs` and `num_steps` of clients will be disable.\n    \"\"\"\n    simulator._incomplete_a = min(a, 1)\n    simulator._incomplete_b = max(b, simulator._incomplete_a)\n    def f(self, client_ids = []):\n        for cid in client_ids:\n            self.clients[cid].set_local_epochs(self.random_module.randint(low=self._incomplete_a, high=self._incomplete_b))\n        working_amounts = [self.clients[cid].num_steps for cid in self.all_clients]\n        self.set_variable(self.all_clients, 'working_amount', working_amounts)\n        return\n    return f\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.arbitrary_static_unifrom_client_completeness","title":"<code>arbitrary_static_unifrom_client_completeness(simulator, a=1, b=1)</code>","text":"<p>This setting follows the setting in the paper 'Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization' (http://arxiv.org/abs/2007.07481). The string <code>mode</code> should be like 'FEDNOVA-Uniform(a,b)' where <code>a</code> is the minimal value of the number of local epochs and <code>b</code> is the maximal value. If this mode is active, the <code>num_epochs</code> and <code>num_steps</code> of clients will be disable.</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def arbitrary_static_unifrom_client_completeness(simulator, a=1, b=1):\n\"\"\"\n    This setting follows the setting in the paper 'Tackling the Objective Inconsistency Problem in\n    Heterogeneous Federated Optimization' (http://arxiv.org/abs/2007.07481). The string `mode` should be like\n    'FEDNOVA-Uniform(a,b)' where `a` is the minimal value of the number of local epochs and `b` is the maximal\n    value. If this mode is active, the `num_epochs` and `num_steps` of clients will be disable.\n    \"\"\"\n    a = min(a, 1)\n    b = max(b, a)\n    for cid in simulator.clients:\n        simulator.clients[cid].set_local_epochs(np.random.randint(low=a, high=b))\n    working_amounts = [simulator.clients[cid].num_steps for cid in simulator.all_clients]\n    simulator.set_variable(simulator.all_clients, 'working_amount', working_amounts)\n    return\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.homogeneous_client_availability","title":"<code>homogeneous_client_availability(simulator, beta=0.2)</code>","text":"<p>All the clients share a homogeneous active rate <code>1-beta</code> where beta \u2208 [0,1)</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def homogeneous_client_availability(simulator, beta=0.2):\n\"\"\"\n    All the clients share a homogeneous active rate `1-beta` where beta \u2208 [0,1)\n    \"\"\"\n    # alpha = float(mode[mode.find('-') + 1:]) if mode.find('-') != -1 else 0.8\n    probs = [1.-beta for _ in simulator.clients]\n    simulator.set_variable(simulator.all_clients, 'prob_available', probs)\n    simulator.set_variable(simulator.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.less_data_first_client_availability","title":"<code>less_data_first_client_availability(simulator, beta=0.5)</code>","text":"<p>Clients with less data will have a larger active rate at each round.         ci=(1-beta)^(-|Di|), pi=ci/cmax, beta \u2208 [0,1)</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def less_data_first_client_availability(simulator, beta=0.5):\n\"\"\"\n    Clients with less data will have a larger active rate at each round.\n            ci=(1-beta)^(-|Di|), pi=ci/cmax, beta \u2208 [0,1)\n    \"\"\"\n    # alpha = float(mode[mode.find('-') + 1:]) if mode.find('-') != -1 else 0.1\n    prop = np.array([len(c.train_data) for c in simulator.server.clients])\n    prop = prop ** (-beta)\n    maxp = np.max(prop)\n    probs = prop/maxp\n    simulator.set_variable(simulator.all_clients, 'prob_available', probs)\n    simulator.set_variable(simulator.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.lognormal_client_availability","title":"<code>lognormal_client_availability(simulator, beta=0.1)</code>","text":"<p>The following two settings are from 'Federated Learning Under Intermittent Client Availability and Time-Varying Communication Constraints' (http://arxiv.org/abs/2205.06730).     ci ~ logmal(0, lognormal(0, -ln(1-beta)), pi=ci/cmax</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def lognormal_client_availability(simulator, beta=0.1):\n\"\"\"The following two settings are from 'Federated Learning Under Intermittent\n    Client Availability and Time-Varying Communication Constraints' (http://arxiv.org/abs/2205.06730).\n        ci ~ logmal(0, lognormal(0, -ln(1-beta)), pi=ci/cmax\n    \"\"\"\n    epsilon = 0.000001\n    Tks = [np.random.lognormal(0, -np.log(1 - beta - epsilon)) for _ in simulator.clients]\n    max_Tk = max(Tks)\n    probs = np.array(Tks)/max_Tk\n    simulator.set_variable(simulator.all_clients, 'prob_available', probs)\n    simulator.set_variable(simulator.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.more_data_first_client_availability","title":"<code>more_data_first_client_availability(simulator, beta=0.0001)</code>","text":"<p>Clients with more data will have a larger active rate at each round. e.g. ci=tanh(-|Di| ln(beta+epsilon)), pi=ci/cmax, beta \u2208 [0,1)</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def more_data_first_client_availability(simulator, beta=0.0001):\n\"\"\"\n    Clients with more data will have a larger active rate at each round.\n    e.g. ci=tanh(-|Di| ln(beta+epsilon)), pi=ci/cmax, beta \u2208 [0,1)\n    \"\"\"\n    p = np.array([len(c.train_data) for c in simulator.server.clients])\n    p = p ** beta\n    maxp = np.max(p)\n    probs = p/maxp\n    simulator.set_variable(simulator.all_clients, 'prob_available', probs)\n    simulator.set_variable(simulator.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.part_dynamic_uniform_client_completeness","title":"<code>part_dynamic_uniform_client_completeness(simulator, p=0.5)</code>","text":"<p>This setting follows the setting in the paper 'Federated Optimization in Heterogeneous Networks' (http://arxiv.org/abs/1812.06127). The <code>p</code> specifies the number of selected clients with incomplete updates.</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def part_dynamic_uniform_client_completeness(simulator, p=0.5):\n\"\"\"\n    This setting follows the setting in the paper 'Federated Optimization in Heterogeneous Networks'\n    (http://arxiv.org/abs/1812.06127). The `p` specifies the number of selected clients with\n    incomplete updates.\n    \"\"\"\n    simulator.prob_incomplete = p\n    def f(self, client_ids = []):\n        was = []\n        for cid in client_ids:\n            wa = self.random_module.randint(low=0, high=self.clients[cid].num_steps) if self.random_module.rand() &lt; self.prob_incomplete else self.clients[cid].num_steps\n            wa = max(1, wa)\n            was.append(wa)\n            self.clients[cid].num_steps = wa\n        self.set_variable(client_ids, 'working_amount', was)\n        return\n    return f\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.sin_lognormal_client_availability","title":"<code>sin_lognormal_client_availability(simulator, beta=0.1)</code>","text":"<p>This setting shares the same active rate distribution with LogNormal, however, the active rates are also influenced by the time (i.e. communication round). The active rates obey a sin wave according to the time with period T.     ci ~ logmal(0, lognormal(0, -ln(1-beta)), pi=ci/cmax, p(i,t)=(0.4sin((1+R%T)/T*2pi)+0.5) * pi</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def sin_lognormal_client_availability(simulator, beta=0.1):\n\"\"\"This setting shares the same active rate distribution with LogNormal, however, the active rates are\n    also influenced by the time (i.e. communication round). The active rates obey a sin wave according to the\n    time with period T.\n        ci ~ logmal(0, lognormal(0, -ln(1-beta)), pi=ci/cmax, p(i,t)=(0.4sin((1+R%T)/T*2pi)+0.5) * pi\n    \"\"\"\n    # beta = float(mode[mode.find('-') + 1:]) if mode.find('-') != -1 else 0.1\n    epsilon = 0.000001\n    Tks = [np.random.lognormal(0, -np.log(1 - beta - epsilon)) for _ in simulator.clients]\n    max_Tk = max(Tks)\n    q = np.array(Tks)/max_Tk\n    simulator.set_variable(simulator.all_clients, 'q', q)\n    simulator.set_variable(simulator.all_clients, 'prob_available', q)\n    def f(self):\n        T = 24\n        times = np.linspace(start=0, stop=2 * np.pi, num=T)\n        fts = 0.4 * np.sin(times) + 0.5\n        t = self.server.current_round % T\n        q = self.get_variable(self.all_clients, 'q')\n        probs = [fts[t]*qi for qi in q]\n        self.set_variable(self.all_clients, 'prob_available', probs)\n        self.set_variable(self.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n    return f\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.y_fewer_first_client_availability","title":"<code>y_fewer_first_client_availability(simulator, beta=0.2)</code>","text":"<p>Clients with fewer kinds of labels will owe a larger active rate.     ci = |set(Yi)|/|set(Y)|, pi = beta*ci + (1-beta)</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def y_fewer_first_client_availability(simulator, beta=0.2):\n\"\"\"\n    Clients with fewer kinds of labels will owe a larger active rate.\n        ci = |set(Yi)|/|set(Y)|, pi = beta*ci + (1-beta)\n    \"\"\"\n    label_num = len(set([int(simulator.server.test_data[di][-1]) for di in range(len(simulator.server.test_data))]))\n    probs = []\n    for c in simulator.server.clients:\n        train_set = set([int(c.train_data[di][-1]) for di in range(len(c.train_data))])\n        valid_set = set([int(c.valid_data[di][-1]) for di in range(len(c.valid_data))])\n        label_set = train_set.union(valid_set)\n        probs.append(beta * len(label_set) / label_num + (1 - beta))\n    simulator.set_variable(simulator.all_clients, 'prob_available', probs)\n    simulator.set_variable(simulator.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n</code></pre>"},{"location":"Docs/simulator/default_simulator/#flgo.simulator.default_simulator.y_max_first_client_availability","title":"<code>y_max_first_client_availability(simulator, beta=0.1)</code>","text":"<p>This setting follows the activity mode in 'Fast Federated Learning in the Presence of Arbitrary Device Unavailability' , where each client ci will be ready</p> for joining in a round with a static probability <p>pi = alpha * min({label kept by ci}) / max({all labels}) + ( 1 - alpha )</p> <p>and the participation of client is independent across rounds. The string mode should be like 'YMaxFirst-x' where x should be replaced by a float number.</p> Source code in <code>flgo\\simulator\\default_simulator.py</code> <pre><code>def y_max_first_client_availability(simulator, beta=0.1):\n\"\"\"\n    This setting follows the activity mode in 'Fast Federated Learning in the\n    Presence of Arbitrary Device Unavailability' , where each client ci will be ready\n    for joining in a round with a static probability:\n        pi = alpha * min({label kept by ci}) / max({all labels}) + ( 1 - alpha )\n    and the participation of client is independent across rounds. The string mode\n    should be like 'YMaxFirst-x' where x should be replaced by a float number.\n    \"\"\"\n    # alpha = float(mode[mode.find('-') + 1:]) if mode.find('-') != -1 else 0.1\n    def label_counter(dataset):\n        return collections.Counter([int(dataset[di][-1]) for di in range(len(dataset))])\n    label_num = len(label_counter(simulator.server.test_data))\n    probs = []\n    for c in simulator.clients:\n        c_counter = label_counter(c.train_data + c.valid_data)\n        c_label = [lb for lb in c_counter.keys()]\n        probs.append((beta * min(c_label) / max(1, label_num - 1)) + (1 - beta))\n    simulator.set_variable(simulator.all_clients, 'prob_available', probs)\n    simulator.set_variable(simulator.all_clients, 'prob_unavailable', [1 - p for p in probs])\n    simulator.roundwise_fixed_availability = True\n    return\n</code></pre>"},{"location":"Docs/utils/fflow/","title":"flgo.utils.fflow","text":""},{"location":"Docs/utils/fflow/#flgo.utils.fflow.GlobalVariable","title":"<code>GlobalVariable</code>","text":"<p>This class is to create a shared space for sharing variables across different parties for each runner</p> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>class GlobalVariable:\n\"\"\"This class is to create a shared space for sharing variables across\n    different parties for each runner\"\"\"\n\n    def __init__(self):\n        self.logger = None\n        self.simulator = None\n        self.clock = None\n        self.dev_list = None\n        self.TaskCalculator = None\n        self.TaskPipe = None\n        self.crt_dev = 0\n\n    def apply_for_device(self):\nr\"\"\"\n        Apply for a new device from currently available ones (i.e. devices in self.dev_list)\n\n        Returns:\n            GPU device (i.e. torch.device)\n        \"\"\"\n        if self.dev_list is None: return None\n        dev = self.dev_list[self.crt_dev]\n        self.crt_dev = (self.crt_dev + 1) % len(self.dev_list)\n        return dev\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.GlobalVariable.apply_for_device","title":"<code>apply_for_device()</code>","text":"<p>Apply for a new device from currently available ones (i.e. devices in self.dev_list)</p> <p>Returns:</p> Type Description <p>GPU device (i.e. torch.device)</p> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def apply_for_device(self):\nr\"\"\"\n    Apply for a new device from currently available ones (i.e. devices in self.dev_list)\n\n    Returns:\n        GPU device (i.e. torch.device)\n    \"\"\"\n    if self.dev_list is None: return None\n    dev = self.dev_list[self.crt_dev]\n    self.crt_dev = (self.crt_dev + 1) % len(self.dev_list)\n    return dev\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.gen_task_by_config","title":"<code>gen_task_by_config(config={}, task_path='', rawdata_path='', seed=0)</code>","text":"<p>Generate a federated task that is specified by the benchmark information and the partition information, where the generated task will be stored in the task_path and the raw data will be downloaded into the rawdata_path.</p> <pre><code>config (dict || str): configuration is either a dict contains parameters or a filename of a .yml file\ntask_path (str): where the generated task will be stored\nrawdata_path (str): where the raw data will be downloaded\\stored\nseed (int): the random seed used to generate the task\n</code></pre> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo\n    &gt;&gt;&gt; config = {'benchmark':{'name':'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDParitioner', 'para':{'num_clients':100}}}\n    &gt;&gt;&gt; flgo.gen_task(config, './my_mnist_iid')\n    &gt;&gt;&gt; # The task will be stored as `my_mnist_iid` in the current working dictionary\n</code></pre> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def gen_task_by_config(config={}, task_path:str='', rawdata_path:str='', seed:int=0):\nr\"\"\"\n    Generate a federated task that is specified by the benchmark information and the partition information, where the generated task will be stored in the task_path and the raw data will be downloaded into the rawdata_path.\n\n        config (dict || str): configuration is either a dict contains parameters or a filename of a .yml file\n        task_path (str): where the generated task will be stored\n        rawdata_path (str): where the raw data will be downloaded\\stored\n        seed (int): the random seed used to generate the task\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo\n        &gt;&gt;&gt; config = {'benchmark':{'name':'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDParitioner', 'para':{'num_clients':100}}}\n        &gt;&gt;&gt; flgo.gen_task(config, './my_mnist_iid')\n        &gt;&gt;&gt; # The task will be stored as `my_mnist_iid` in the current working dictionary\n    ```\n    \"\"\"\n    # setup random seed\n    random.seed(3 + seed)\n    np.random.seed(97 + seed)\n    torch.manual_seed(12+seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    # load configuration\n    gen_option = load_configuration(config)\n    if 'para' not in gen_option['benchmark'].keys(): gen_option['benchmark']['para'] = {}\n    if 'partitioner' in gen_option.keys():\n        # update parameters of partitioner\n        if 'para' not in gen_option['partitioner'].keys():\n            gen_option['partitioner']['para'] = {}\n        else:\n            if 'name' not in gen_option['partitioner'].keys():\n                gen_option['benchmark']['para'].update(gen_option['partitioner']['para'])\n    # init generator\n    if rawdata_path!='': gen_option['benchmark']['para']['rawdata_path']=rawdata_path\n    if type(gen_option['benchmark']['name']) is str:\n        bmk_core = importlib.import_module('.'.join([gen_option['benchmark']['name'], 'core']))\n    elif hasattr(gen_option['benchmark']['name'], '__path__'):\n        bmk_core = getattr(gen_option['benchmark']['name'],'core')\n    else:\n        raise RuntimeError(\"The value of parameter config['benchmark']['name'] should be either a string or a python package.\")\n    task_generator = getattr(bmk_core, 'TaskGenerator')(**gen_option['benchmark']['para'])\n    # create partitioner for generator if specified\n    if 'partitioner' in gen_option.keys() and 'name' in gen_option['partitioner'].keys():\n        Partitioner = gen_option['partitioner']['name']\n        if type(Partitioner) is str:\n            if Partitioner in globals().keys(): Partitioner = eval(Partitioner)\n            else: Partitioner = getattr(flgo.benchmark.toolkits.partition, Partitioner)\n        partitioner = Partitioner(**gen_option['partitioner']['para'])\n        task_generator.register_partitioner(partitioner)\n        partitioner.register_generator(task_generator)\n    else:\n        partitioner = None\n    # generate federated task\n    task_generator.generate()\n    # save the generated federated benchmark\n    # initialize task pipe\n    if task_path=='': task_path = os.path.join('.', task_generator.task_name)\n    task_pipe = getattr(bmk_core, 'TaskPipe')(task_path)\n    # check if task already exists\n    if task_pipe.task_exists():\n        raise FileExistsError('Task {} already exists.'.format(task_path))\n    try:\n        # create task architecture\n        task_pipe.create_task_architecture()\n        # save meta infomation\n        task_pipe.save_info(task_generator)\n        # save task\n        task_pipe.save_task(task_generator)\n        print('Task {} has been successfully generated.'.format(task_generator.task_name))\n    except Exception as e:\n        print(e)\n        task_pipe.remove_task()\n        print(\"Failed to saving splited dataset.\")\n    # save visualization\n    try:\n        visualize_func = getattr(importlib.import_module(gen_option['benchmark']['name']),'visualize')\n        visualize_func(task_generator, partitioner, task_path)\n    except:\n        pass\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.gen_task_by_para","title":"<code>gen_task_by_para(benchmark, bmk_para={}, Partitioner=None, par_para={}, task_path='', rawdata_path='', seed=0)</code>","text":"<p>Generate a federated task according to the parameters of this function. The formats and meanings of the inputs are listed as below:</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>package | str</code> <p>the benchmark package or the module path of it</p> required <code>bmk_para</code> <code>dict</code> <p>the customized parameter dict of the method TaskGenerator.init() of the benchmark</p> <code>{}</code> <code>Partitioner</code> <code>flgo.benchmark.toolkits.partition.BasicPartitioner | str</code> <p>the class of the Partitioner or the name of the Partitioner that was realized in flgo.benchmark.toolkits.partition</p> <code>None</code> <code>par_para</code> <code>dict</code> <p>the customized parameter dict of the method Partitioner.init()</p> <code>{}</code> <code>task_path</code> <code>str</code> <p>the path to store the generated task</p> <code>''</code> <code>rawdata_path</code> <code>str</code> <p>where the raw data will be downloaded\\stored</p> <code>''</code> <code>seed</code> <code>int</code> <p>the random seed used to generate the task</p> <code>0</code> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo\n    &gt;&gt;&gt; import flgo.benchmark.mnist_classification as mnist\n    &gt;&gt;&gt; from flgo.benchmark.toolkits.partition import IIDPartitioner\n    &gt;&gt;&gt; # GENERATE TASK BY PASSING THE MODULE OF BENCHMARK AND THE CLASS OF THE PARTITIOENR\n    &gt;&gt;&gt; flgo.gen_task_by_para(benchmark=mnist, Partitioner = IIDPartitioner, par_para={'num_clients':100}, task_path='./mnist_gen_by_para1')\n    &gt;&gt;&gt; # GENERATE THE SAME TASK BY PASSING THE STRING\n    &gt;&gt;&gt; flgo.gen_task_by_para(benchmark='flgo.benchmark.mnist_classification', Partitioner='IIDPartitioner', par_para={'num_clients':100}, task_path='./mnist_gen_by_para2')\n</code></pre> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def gen_task_by_para(benchmark, bmk_para:dict={}, Partitioner=None, par_para:dict={}, task_path: str='', rawdata_path:str='', seed:int=0):\nr\"\"\"\n    Generate a federated task according to the parameters of this function. The formats and meanings of the inputs are listed as below:\n\n    Args:\n        benchmark (package|str): the benchmark package or the module path of it\n        bmk_para (dict): the customized parameter dict of the method TaskGenerator.__init__() of the benchmark\n        Partitioner (flgo.benchmark.toolkits.partition.BasicPartitioner|str): the class of the Partitioner or the name of the Partitioner that was realized in flgo.benchmark.toolkits.partition\n        par_para (dict): the customized parameter dict of the method Partitioner.__init__()\n        task_path (str): the path to store the generated task\n        rawdata_path (str): where the raw data will be downloaded\\stored\n        seed (int): the random seed used to generate the task\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo\n        &gt;&gt;&gt; import flgo.benchmark.mnist_classification as mnist\n        &gt;&gt;&gt; from flgo.benchmark.toolkits.partition import IIDPartitioner\n        &gt;&gt;&gt; # GENERATE TASK BY PASSING THE MODULE OF BENCHMARK AND THE CLASS OF THE PARTITIOENR\n        &gt;&gt;&gt; flgo.gen_task_by_para(benchmark=mnist, Partitioner = IIDPartitioner, par_para={'num_clients':100}, task_path='./mnist_gen_by_para1')\n        &gt;&gt;&gt; # GENERATE THE SAME TASK BY PASSING THE STRING\n        &gt;&gt;&gt; flgo.gen_task_by_para(benchmark='flgo.benchmark.mnist_classification', Partitioner='IIDPartitioner', par_para={'num_clients':100}, task_path='./mnist_gen_by_para2')\n    ```\n    \"\"\"\n    random.seed(3 + seed)\n    np.random.seed(97 + seed)\n    torch.manual_seed(12+seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    if type(benchmark) is str: benchmark = importlib.import_module(benchmark)\n    if not hasattr(benchmark, '__path__'): raise RuntimeError(\"benchmark should be a package or the path of a package\")\n    if Partitioner is not None:\n        if type(Partitioner) is str:\n            if Partitioner in globals().keys(): Partitioner = eval(Partitioner)\n            else: Partitioner = getattr(flgo.benchmark.toolkits.partition, Partitioner)\n        partitioner = Partitioner(**par_para)\n    else: partitioner = None\n    if rawdata_path!='': bmk_para['rawdata_path']=rawdata_path\n    bmk_core = benchmark.core\n    task_generator = getattr(bmk_core, 'TaskGenerator')(**bmk_para)\n    if partitioner is not None:\n        task_generator.register_partitioner(partitioner)\n        partitioner.register_generator(task_generator)\n    task_generator.generate()\n    # save the generated federated benchmark\n    # initialize task pipe\n    if task_path=='': task_path = os.path.join('.', task_generator.task_name)\n    task_pipe = getattr(bmk_core, 'TaskPipe')(task_path)\n    # check if task already exists\n    if task_pipe.task_exists():\n        raise FileExistsError('Task {} already exists.'.format(task_path))\n    try:\n        # create task architecture\n        task_pipe.create_task_architecture()\n        # save meta infomation\n        task_pipe.save_info(task_generator)\n        # save task\n        task_pipe.save_task(task_generator)\n        print('Task {} has been successfully generated.'.format(task_generator.task_name))\n    except Exception as e:\n        print(e)\n        task_pipe.remove_task()\n        print(\"Failed to saving splited dataset.\")\n    # save visualization\n    try:\n        visualize_func = getattr(benchmark,'visualize')\n        visualize_func(task_generator, partitioner, task_path)\n    except:\n        pass\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.init","title":"<code>init(task, algorithm, option={}, model=None, Logger=None, Simulator=flgo.simulator.DefaultSimulator, scene='horizontal')</code>","text":"<p>Initialize a runner in FLGo, which is to optimize a model on a specific task (i.e. IID-mnist-of-100-clients) by the selected federated algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>the dictionary of the federated task</p> required <code>algorithm</code> <code>module|class</code> <p>the algorithm will be used to optimize the model in federated manner, which must contain pre-defined attributions (e.g. algorithm.Server and algorithm.Client for horizontal federated learning)</p> required <code>option</code> <code>dict | str</code> <p>the configurations of training, environment, algorithm, logger and simulator</p> <code>{}</code> <code>model</code> <code>module|class</code> <p>the model module that contains two methods: model.init_local_module(object) and model.init_global_module(object)</p> <code>None</code> <code>Logger</code> <code>flgo.experiment.logger.BasicLogger</code> <p>the class of the logger inherited from flgo.experiment.logger.BasicLogger</p> <code>None</code> <code>Simulator</code> <code>flgo.simulator.base.BasicSimulator</code> <p>the class of the simulator inherited from flgo.simulator.BasicSimulator</p> <code>flgo.simulator.DefaultSimulator</code> <code>scene</code> <code>str</code> <p>'horizontal' or 'vertical' in current version of FLGo</p> <code>'horizontal'</code> <p>Returns:</p> Name Type Description <code>runner</code> <p>the object instance that has the method runner.run()</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; import flgo\n    &gt;&gt;&gt; from flgo.algorithm import fedavg\n    &gt;&gt;&gt; from flgo.experiment.logger.simple_logger import SimpleLogger\n    &gt;&gt;&gt; # create task 'mnist_iid' by flgo.gen_task('gen_config.yml', 'mnist_iid') if there exists no such task\n    &gt;&gt;&gt; if os.path.exists('mnist_iid'): flgo.gen_task({'benchmark':{'name':'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDPartitioner','para':{'num_clients':100}}}, 'mnist_iid')\n    &gt;&gt;&gt; # create runner\n    &gt;&gt;&gt; fedavg_runner = flgo.init('mnist_iid', algorithm=fedavg, option = {'num_rounds':20, 'gpu':[0], 'learning_rate':0.1})\n    &gt;&gt;&gt; fedavg_runner.run()\n    ... # the training will start after runner.run() was called, and the running-time results will be recorded by Logger into the task dictionary\n</code></pre> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def init(task: str, algorithm, option = {}, model=None, Logger: flgo.experiment.logger.BasicLogger = None, Simulator: BasicSimulator=flgo.simulator.DefaultSimulator, scene='horizontal'):\nr\"\"\"\n    Initialize a runner in FLGo, which is to optimize a model on a specific task (i.e. IID-mnist-of-100-clients) by the selected federated algorithm.\n\n    Args:\n        task (str): the dictionary of the federated task\n        algorithm (module|class): the algorithm will be used to optimize the model in federated manner, which must contain pre-defined attributions (e.g. algorithm.Server and algorithm.Client for horizontal federated learning)\n        option (dict|str): the configurations of training, environment, algorithm, logger and simulator\n        model (module|class): the model module that contains two methods: model.init_local_module(object) and model.init_global_module(object)\n        Logger (flgo.experiment.logger.BasicLogger): the class of the logger inherited from flgo.experiment.logger.BasicLogger\n        Simulator (flgo.simulator.base.BasicSimulator): the class of the simulator inherited from flgo.simulator.BasicSimulator\n        scene (str): 'horizontal' or 'vertical' in current version of FLGo\n\n    Returns:\n        runner: the object instance that has the method runner.run()\n\n    Example:\n    ```python\n        &gt;&gt;&gt; import flgo\n        &gt;&gt;&gt; from flgo.algorithm import fedavg\n        &gt;&gt;&gt; from flgo.experiment.logger.simple_logger import SimpleLogger\n        &gt;&gt;&gt; # create task 'mnist_iid' by flgo.gen_task('gen_config.yml', 'mnist_iid') if there exists no such task\n        &gt;&gt;&gt; if os.path.exists('mnist_iid'): flgo.gen_task({'benchmark':{'name':'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDPartitioner','para':{'num_clients':100}}}, 'mnist_iid')\n        &gt;&gt;&gt; # create runner\n        &gt;&gt;&gt; fedavg_runner = flgo.init('mnist_iid', algorithm=fedavg, option = {'num_rounds':20, 'gpu':[0], 'learning_rate':0.1})\n        &gt;&gt;&gt; fedavg_runner.run()\n        ... # the training will start after runner.run() was called, and the running-time results will be recorded by Logger into the task dictionary\n    ```\n    \"\"\"\n\n    # init option\n    option = load_configuration(option)\n    default_option = read_option_from_command()\n    for op_key in option:\n        if op_key in default_option.keys():\n            op_type = type(default_option[op_key])\n            if op_type == type(option[op_key]):\n                default_option[op_key] = option[op_key]\n            else:\n                if op_type is list:\n                    default_option[op_key]=list(option[op_key]) if hasattr(option[op_key], '__iter__') else [option[op_key]]\n                elif op_type is tuple:\n                    default_option[op_key] = tuple(option[op_key]) if hasattr(option[op_key], '__iter__') else (option[op_key])\n                else:\n                    default_option[op_key] = op_type(option[op_key])\n        else:\n            default_option[op_key] = option[op_key]\n    option = default_option\n    setup_seed(seed=option['seed'])\n    option['task'] = task\n    option['algorithm'] = (algorithm.__name__).split('.')[-1]\n    option['server_with_cpu'] = True if option['num_parallels']&gt;1 else option['server_with_cpu']\n    # init task info\n    if not os.path.exists(task):\n        raise FileExistsError(\"Fedtask '{}' doesn't exist. Please generate the specified task by flgo.gen_task().\")\n    with open(os.path.join(task, 'info'), 'r') as inf:\n        task_info = json.load(inf)\n    benchmark = task_info['benchmark']\n    if model== None: model = getattr(importlib.import_module(benchmark), 'default_model')\n    option['model'] = (model.__name__).split('.')[-1]\n\n    # create global variable\n    gv = GlobalVariable()\n    # init logger\n    if Logger is None:\n        if scene=='horizontal':\n            Logger = flgo.experiment.logger.simple_logger.SimpleLogger\n        elif scene=='vertical':\n            Logger = flgo.experiment.logger.vertical_logger.VerticalLogger\n    gv.logger = Logger(task=task, option=option, name=str(id(gv))+str(Logger), level=option['log_level'])\n\n    # init device\n    gv.dev_list = [torch.device('cpu')] if (option['gpu'] is None or len(option['gpu'])==0) else [torch.device('cuda:{}'.format(gpu_id)) for gpu_id in option['gpu']]\n    gv.logger.info('Initializing devices: '+','.join([str(dev) for dev in gv.dev_list])+' will be used for this running.')\n    # init task\n    core_module = '.'.join([benchmark, 'core'])\n    gv.TaskPipe = getattr(importlib.import_module(core_module), 'TaskPipe')\n    task_pipe = gv.TaskPipe(task)\n    gv.TaskCalculator = getattr(importlib.import_module(core_module), 'TaskCalculator')\n    task_data = task_pipe.load_data(option)\n\n    # init objects\n    obj_class = [c for c in dir(algorithm) if not c.startswith('__')]\n    tmp = []\n    for c in obj_class:\n        try:\n            C = getattr(algorithm, c)\n            setattr(C, 'gv', gv)\n            tmp.append(c)\n        except:\n            continue\n    objects = task_pipe.generate_objects(option, algorithm, scene=scene)\n    task_pipe.distribute(task_data, objects)\n\n    # init model\n    if hasattr(model, 'init_local_module'):\n        for object in objects:\n            model.init_local_module(object)\n    if hasattr(model, 'init_global_module'):\n        for object in objects:\n            model.init_global_module(object)\n\n    # init communicator\n    gv.communicator = flgo.VirtualCommunicator(objects)\n\n    for ob in objects: ob.initialize()\n\n    # init virtual system environment\n    gv.logger.info('Use `{}` as the system simulator'.format(str(Simulator)))\n    flgo.simulator.base.random_seed_gen = flgo.simulator.base.seed_generator(option['seed'])\n    gv.clock = flgo.simulator.base.ElemClock()\n    gv.simulator = Simulator(objects, option)\n    gv.clock.register_simulator(simulator=gv.simulator)\n\n    gv.logger.register_variable(coordinator=objects[0], participants=objects[1:], option=option, clock=gv.clock, scene=scene, objects = objects)\n    gv.logger.initialize()\n    gv.logger.info('Ready to start.')\n\n    # register global variables for objects\n    for c in tmp:\n        try:\n            C = getattr(algorithm, c)\n            delattr(C, 'gv')\n        except:\n            continue\n    for ob in objects:\n        ob.gv = gv\n    gv.simulator.gv = gv\n    gv.clock.gv = gv\n    gv.logger.gv = gv\n    return objects[0]\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.load_configuration","title":"<code>load_configuration(config={})</code>","text":"<p>Load configurations from .yml file or dict.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict | str</code> <p>the configurations</p> <code>{}</code> <p>Returns:</p> Type Description <p>a dict of option (i.e. configuration)</p> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def load_configuration(config={}):\nr\"\"\"\n    Load configurations from .yml file or dict.\n\n    Args:\n        config (dict|str): the configurations\n\n    Returns:\n        a dict of option (i.e. configuration)\n    \"\"\"\n    if type(config) is str and config.endswith('.yml'):\n        with open(config) as f:\n            option = yaml.load(f, Loader=yaml.FullLoader)\n        return option\n    elif type(config) is dict:\n        return config\n    else:\n        raise TypeError('The input config should be either a dict or a filename.')\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.multi_init_and_run","title":"<code>multi_init_and_run(runner_args, devices=[], scheduler=None)</code>","text":"<p>Create multiple runners and run in parallel</p> <p>Parameters:</p> Name Type Description Default <code>runner_args</code> <code>list</code> <p>each element in runner_args should be either a dict or a tuple or parameters</p> required <code>devices</code> <code>list</code> <p>a list of gpu id</p> <code>[]</code> <code>scheduler</code> <code>flgo.experiment.device_scheduler.BasicScheduler(...</code> <p>GPU scheduler</p> <code>None</code> <p>Returns:</p> Type Description <p>a list of output results of runners</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; from flgo.algorithm import fedavg, fedprox, scaffold\n    &gt;&gt;&gt; # create task 'mnist_iid' by flgo.gen_task if there exists no such task\n    &gt;&gt;&gt; task='./mnist_iid'\n    &gt;&gt;&gt; if os.path.exists(task): flgo.gen_task({'benchmark':{'name':'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDPartitioner','para':{'num_clients':100}}}, task)\n    &gt;&gt;&gt; algos = [fedavg, fedprox, scaffold]\n    &gt;&gt;&gt; flgo.multi_init_and_run([{'task':task, 'algorithm':algo} for algo in algos], devices=[0])\n</code></pre> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def multi_init_and_run(runner_args:list, devices = [], scheduler=None):\nr\"\"\"\n    Create multiple runners and run in parallel\n\n    Args:\n        runner_args (list): each element in runner_args should be either a dict or a tuple or parameters\n        devices (list): a list of gpu id\n        scheduler (flgo.experiment.device_scheduler.BasicScheduler(...)): GPU scheduler\n\n    Returns:\n        a list of output results of runners\n\n    Example:\n    ```python\n        &gt;&gt;&gt; from flgo.algorithm import fedavg, fedprox, scaffold\n        &gt;&gt;&gt; # create task 'mnist_iid' by flgo.gen_task if there exists no such task\n        &gt;&gt;&gt; task='./mnist_iid'\n        &gt;&gt;&gt; if os.path.exists(task): flgo.gen_task({'benchmark':{'name':'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDPartitioner','para':{'num_clients':100}}}, task)\n        &gt;&gt;&gt; algos = [fedavg, fedprox, scaffold]\n        &gt;&gt;&gt; flgo.multi_init_and_run([{'task':task, 'algorithm':algo} for algo in algos], devices=[0])\n    ```\n    \"\"\"\n    if len(runner_args)==0:return\n    args = []\n    if type(runner_args[0]) is dict:\n        for a in runner_args:\n            tmp = collections.defaultdict(lambda:None, a)\n            if tmp['task'] is None or tmp['algorithm'] is None:\n                raise RuntimeError(\"keyword 'task' or 'algorithm' is of NoneType\")\n            algorithm = tmp['algorithm']\n            tmp['algorithm'] = algorithm.__name__ if (not hasattr(algorithm, '__module__') and hasattr(algorithm, '__name__')) else algorithm\n            if tmp['option'] is None:\n                tmp['option'] = default_option_dict\n            else:\n                option = tmp['option']\n                default_option = read_option_from_command()\n                for op_key in option:\n                    if op_key in default_option.keys():\n                        op_type = type(default_option[op_key])\n                        if op_type == type(option[op_key]):\n                            default_option[op_key] = option[op_key]\n                        else:\n                            if op_type is list:\n                                default_option[op_key] = list(option[op_key]) if hasattr(option[op_key],\n                                                                                         '__iter__') else [\n                                    option[op_key]]\n                            elif op_type is tuple:\n                                default_option[op_key] = tuple(option[op_key]) if hasattr(option[op_key],\n                                                                                          '__iter__') else (\n                                option[op_key])\n                            else:\n                                default_option[op_key] = op_type(option[op_key])\n                tmp['option'] = default_option\n            if tmp['model'] is None:\n                model_name = None\n            else:\n                if not hasattr(tmp['model'], '__module__') and hasattr(tmp['model'], '__name__'):\n                    model_name = tmp['model'].__name__\n                else:\n                    model_name = tmp['model']\n            tmp['model'] = model_name\n            if tmp['Logger'] is None:\n                tmp['Logger'] = flgo.experiment.logger.simple_logger.SimpleLogger\n            algorithm_name = tmp['algorithm'].__name__ if (not hasattr(tmp['algorithm'], '__module__') and hasattr(tmp['algorithm'], '__name__')) else tmp['algorithm']\n            if tmp['Simulator'] is None:\n                tmp['Simulator'] = flgo.simulator.DefaultSimulator\n            if tmp['scene'] is None:\n                tmp['scene'] = 'horizontal'\n            args.append(list(tmp.values()))\n    elif type(runner_args[0]) is tuple or type(runner_args[0]) is list:\n        for a in runner_args:\n            if len(a)&lt;2: raise RuntimeError('the args of runner should at least contain task and algorithm.')\n            default_args = [None, None, default_option_dict, None, flgo.experiment.logger.simple_logger.SimpleLogger, flgo.simulator.DefaultSimulator, 'horizontal']\n            for aid in range(len(a)):\n                if aid==0:\n                    default_args[aid] = a[aid]\n                if aid==1:\n                    algorithm = a[aid]\n                    algorithm_name = algorithm.__name__ if (not hasattr(algorithm, '__module__') and hasattr(algorithm, '__name__')) else algorithm\n                    default_args[aid] = algorithm_name\n                elif aid==2:\n                    option = a[aid]\n                    default_option = read_option_from_command()\n                    for op_key in option:\n                        if op_key in default_option.keys():\n                            op_type = type(default_option[op_key])\n                            if op_type == type(option[op_key]):\n                                default_option[op_key] = option[op_key]\n                            else:\n                                if op_type is list:\n                                    default_option[op_key] = list(option[op_key]) if hasattr(option[op_key],\n                                                                                             '__iter__') else [\n                                        option[op_key]]\n                                elif op_type is tuple:\n                                    default_option[op_key] = tuple(option[op_key]) if hasattr(option[op_key],\n                                                                                              '__iter__') else (\n                                        option[op_key])\n                                else:\n                                    default_option[op_key] = op_type(option[op_key])\n                    default_args[aid] = default_option\n                elif aid==3:\n                    model = a[aid]\n                    if model is None:\n                        model_name = None\n                    else:\n                        if not hasattr(model, '__module__') and hasattr(model, '__name__'):\n                            model_name = model.__name__\n                        else:\n                            model_name = model\n                    default_args[aid] = model_name\n                else:\n                    default_args[aid] = a[aid]\n\n    runner_state = {rid: {'p': None, 'completed': False, 'output': None, 'runner_in_queue': False, 'recv': None, } for\n                    rid in range(len(args))}\n    if scheduler is None: scheduler = flgo.experiment.device_scheduler.BasicScheduler(devices)\n    while True:\n        for rid in range(len(args)):\n            current_arg = args[rid]\n            if runner_state[rid]['p'] is None:\n                if not runner_state[rid]['completed']:\n                    available_device = scheduler.get_available_device(current_arg)\n                    if available_device is None:\n                        continue\n                    else:\n                        list_current_arg = copy.deepcopy(current_arg)\n                        list_current_arg[2]['gpu'] = available_device\n                        recv_end, send_end = multiprocessing.Pipe(False)\n                        list_current_arg.append(send_end)\n                        runner_state[rid]['p'] = multiprocessing.Process(target=_call_by_process, args=tuple(list_current_arg))\n                        runner_state[rid]['recv'] = recv_end\n                        runner_state[rid]['p'].start()\n                        scheduler.add_process(runner_state[rid]['p'].pid)\n                        print('Process {} was created for args {}'.format(runner_state[rid]['p'].pid,current_arg))\n            else:\n                if runner_state[rid]['p'].exitcode is not None:\n                    tmp = runner_state[rid]['recv'].recv()\n                    scheduler.remove_process(tmp[-1])\n                    try:\n                        runner_state[rid]['p'].terminate()\n                    except:\n                        pass\n                    runner_state[rid]['p'] = None\n                    if len(tmp) == 2:\n                        runner_state[rid]['completed'] = True\n                        runner_state[rid]['output'] = tmp[0]\n                    else:\n                        print(tmp[1])\n        if all([v['completed'] for v in runner_state.values()]): break\n        time.sleep(1)\n    res = []\n    for rid in range(len(runner_state)):\n        rec_path = runner_state[rid]['output']\n        with open(rec_path, 'r') as inf:\n            s_inf = inf.read()\n            rec = json.loads(s_inf)\n        res.append(rec)\n    return res\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.read_option_from_command","title":"<code>read_option_from_command()</code>","text":"<p>Generate running-time configurations for flgo.init with default values from command lines</p> <p>Returns:</p> Type Description <p>a dict of option (i.e. configuration)</p> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def read_option_from_command():\nr\"\"\"\n    Generate running-time configurations for flgo.init with default values from command lines\n\n    Returns:\n        a dict of option (i.e. configuration)\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n\"\"\"Training Options\"\"\"\n    # basic settings\n    # methods of server side for sampling and aggregating\n    parser.add_argument('--sample', help='methods for sampling clients', type=str, choices=sample_list, default='md')\n    parser.add_argument('--aggregate', help='methods for aggregating models', type=str, choices=agg_list, default='uniform')\n    # hyper-parameters of training in server side\n    parser.add_argument('--num_rounds', help='number of communication rounds', type=int, default=20)\n    parser.add_argument('--proportion', help='proportion of clients sampled per round', type=float, default=0.2)\n    parser.add_argument('--learning_rate_decay', help='learning rate decay for the training process;', type=float, default=0.998)\n    parser.add_argument('--lr_scheduler', help='type of the global learning rate scheduler', type=int, default=-1)\n    parser.add_argument('--early_stop', help='stop training if there is no improvement for no smaller than the maximum rounds', type=int, default=-1)\n    # hyper-parameters of local training\n    parser.add_argument('--num_epochs', help='number of epochs when clients trainset on data;', type=int, default=5)\n    parser.add_argument('--num_steps', help='the number of local steps, which dominate num_epochs when setting num_steps&gt;0', type=int, default=-1)\n    parser.add_argument('--learning_rate', help='learning rate for inner solver;', type=float, default=0.1)\n    parser.add_argument('--batch_size', help='batch size when clients trainset on data;', type=float, default='64')\n    parser.add_argument('--optimizer', help='select the optimizer for gd', type=str, choices=optimizer_list, default='SGD')\n    parser.add_argument('--momentum', help='momentum of local update', type=float, default=0)\n    parser.add_argument('--weight_decay', help='weight decay for the training process', type=float, default=0)\n    # algorithm-dependent hyper-parameters\n    parser.add_argument('--algo_para', help='algorithm-dependent hyper-parameters', nargs='*', type=float)\n\n\"\"\"Environment Options\"\"\"\n    # the ratio of the amount of the data used to train\n    parser.add_argument('--train_holdout', help='the rate of holding out the validation dataset from all the local training datasets', type=float, default=0.1)\n    parser.add_argument('--test_holdout', help='the rate of holding out the validation dataset from the training datasets', type=float, default=0.0)\n    parser.add_argument('--local_test', help='if this term is set True and train_holdout&gt;0, (0.5*train_holdout) of data will be set as client.test_data.', action=\"store_true\", default=False)\n    # realistic machine config\n    parser.add_argument('--seed', help='seed for random initialization;', type=int, default=0)\n    parser.add_argument('--gpu', nargs='*', help='GPU IDs and empty input is equal to using CPU', type=int)\n    parser.add_argument('--server_with_cpu', help='seed for random initialization;', action=\"store_true\", default=False)\n    parser.add_argument('--num_parallels', help=\"the number of parallels in the clients computing session\", type=int, default=1)\n    parser.add_argument('--num_workers', help='the number of workers of DataLoader', type=int, default=0)\n    parser.add_argument('--pin_memory', help='pin_memory of DataLoader', action=\"store_true\", default=False)\n    parser.add_argument('--test_batch_size', help='the batch_size used in testing phase;', type=int, default=512)\n\n\"\"\"Simulator Options\"\"\"\n    # the simulating systemic configuration of clients and the server that helps constructing the heterogeity in the network condition &amp; computing power\n    parser.add_argument('--availability', help=\"client availability mode\", type=str, default = 'IDL')\n    parser.add_argument('--connectivity', help=\"client connectivity mode\", type=str, default = 'IDL')\n    parser.add_argument('--completeness', help=\"client completeness mode\", type=str, default = 'IDL')\n    parser.add_argument('--responsiveness', help=\"client responsiveness mode\", type=str, default='IDL')\n\n\"\"\"Logger Options\"\"\"\n    # logger setting\n    parser.add_argument('--log_level', help='the level of logger', type=str, default='INFO')\n    parser.add_argument('--log_file', help='bool controls whether log to file and default value is False', action=\"store_true\", default=False)\n    parser.add_argument('--no_log_console', help='bool controls whether log to screen and default value is True', action=\"store_true\", default=False)\n    parser.add_argument('--no_overwrite', help='bool controls whether to overwrite the old result', action=\"store_true\", default=False)\n    parser.add_argument('--eval_interval', help='evaluate every __ rounds;', type=int, default=1)\n    try: option = vars(parser.parse_known_args()[0])\n    except IOError as msg: parser.error(str(msg))\n    for key in option.keys():\n        if option[key] is None:\n            option[key]=[]\n    return option\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.run_in_parallel","title":"<code>run_in_parallel(task, algorithm, options=[], model=None, devices=[], Logger=flgo.experiment.logger.simple_logger.SimpleLogger, Simulator=flgo.simulator.DefaultSimulator, scene='horizontal', scheduler=None)</code>","text":"<p>Run different groups of hyper-parameters for one task and one algorithm in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>the dictionary of the federated task</p> required <code>algorithm</code> <code>module|class</code> <p>the algorithm will be used to optimize the model in federated manner, which must contain pre-defined attributions (e.g. algorithm.Server and algorithm.Client for horizontal federated learning)</p> required <code>options</code> <code>list</code> <p>the configurations of different groups of hyper-parameters</p> <code>[]</code> <code>model</code> <code>module|class</code> <p>the model module that contains two methods: model.init_local_module(object) and model.init_global_module(object)</p> <code>None</code> <code>devices</code> <code>list</code> <p>the list of IDs of devices</p> <code>[]</code> <code>Logger</code> <code>class</code> <p>the class of the logger inherited from flgo.experiment.logger.BasicLogger</p> <code>flgo.experiment.logger.simple_logger.SimpleLogger</code> <code>Simulator</code> <code>class</code> <p>the class of the simulator inherited from flgo.simulator.BasicSimulator</p> <code>flgo.simulator.DefaultSimulator</code> <code>scene</code> <code>str</code> <p>'horizontal' or 'vertical' in current version of FLGo</p> <code>'horizontal'</code> <code>scheduler</code> <code>instance of flgo.experiment.device_scheduler.BasicScheduler</code> <p>GPU scheduler that schedules GPU by checking their availability</p> <code>None</code> <p>Returns:</p> Type Description <p>the returns of _call_by_process</p> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def run_in_parallel(task: str, algorithm, options:list = [], model=None, devices = [], Logger:flgo.experiment.logger.BasicLogger = flgo.experiment.logger.simple_logger.SimpleLogger, Simulator=flgo.simulator.DefaultSimulator, scene='horizontal', scheduler = None):\n\"\"\"\n    Run different groups of hyper-parameters for one task and one algorithm in parallel.\n\n    Args:\n        task (str): the dictionary of the federated task\n        algorithm (module|class): the algorithm will be used to optimize the model in federated manner, which must contain pre-defined attributions (e.g. algorithm.Server and algorithm.Client for horizontal federated learning)\n        options (list): the configurations of different groups of hyper-parameters\n        model (module|class): the model module that contains two methods: model.init_local_module(object) and model.init_global_module(object)\n        devices (list): the list of IDs of devices\n        Logger (class): the class of the logger inherited from flgo.experiment.logger.BasicLogger\n        Simulator (class): the class of the simulator inherited from flgo.simulator.BasicSimulator\n        scene (str): 'horizontal' or 'vertical' in current version of FLGo\n        scheduler (instance of flgo.experiment.device_scheduler.BasicScheduler): GPU scheduler that schedules GPU by checking their availability\n\n    Returns:\n        the returns of _call_by_process\n    \"\"\"\n    try:\n        # init multiprocess\n        torch.multiprocessing.set_start_method('spawn', force=True)\n        torch.multiprocessing.set_sharing_strategy('file_system')\n    except:\n        pass\n    if model is None:\n        model_name = None\n    else:\n        if not hasattr(model, '__module__') and hasattr(model, '__name__'):\n            model_name = model.__name__\n        else:\n            model_name = model\n    algorithm_name = algorithm.__name__ if (not hasattr(algorithm, '__module__') and hasattr(algorithm, '__name__')) else algorithm\n    option_state = {oid:{'p':None, 'completed':False, 'output':None, 'option_in_queue':False, 'recv':None, } for oid in range(len(options))}\n    if scheduler is None: scheduler = flgo.experiment.device_scheduler.BasicScheduler(devices)\n    while True:\n        for oid in range(len(options)):\n            opt = options[oid]\n            if option_state[oid]['p'] is None:\n                if not option_state[oid]['completed']:\n                    available_device = scheduler.get_available_device(opt)\n                    if available_device is None: continue\n                    else:\n                        opt['gpu'] = available_device\n                        recv_end, send_end = multiprocessing.Pipe(False)\n                        option_state[oid]['p'] = multiprocessing.Process(target=_call_by_process, args=(task, algorithm_name, opt, model_name, Logger, Simulator, scene, send_end))\n                        option_state[oid]['recv'] = recv_end\n                        option_state[oid]['p'].start()\n                        scheduler.add_process(option_state[oid]['p'].pid)\n                        print('Process {} was created for args {}'.format(option_state[oid]['p'].pid,(task, algorithm_name, opt, model_name, Logger, Simulator, scene)))\n            else:\n                if option_state[oid]['p'].exitcode is not None:\n                    tmp = option_state[oid]['recv'].recv()\n                    scheduler.remove_process(tmp[-1])\n                    try:\n                        option_state[oid]['p'].terminate()\n                    except:\n                        pass\n                    option_state[oid]['p'] = None\n                    if len(tmp)==2:\n                        option_state[oid]['completed'] = True\n                        option_state[oid]['output'] = tmp[0]\n                    else:\n                        print(tmp[1])\n        if all([v['completed'] for v in option_state.values()]):break\n        time.sleep(1)\n    res = []\n    for oid in range(len(options)):\n        rec_path = option_state[oid]['output']\n        with open(rec_path, 'r') as inf:\n            s_inf = inf.read()\n            rec = json.loads(s_inf)\n        res.append(rec)\n    return res\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.setup_seed","title":"<code>setup_seed(seed)</code>","text":"<p>Fix all the random seed used in numpy, torch and random module</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>the random seed</p> required Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def setup_seed(seed):\nr\"\"\"\n    Fix all the random seed used in numpy, torch and random module\n\n    Args:\n        seed (int): the random seed\n    \"\"\"\n    random.seed(1+seed)\n    np.random.seed(21+seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(12+seed)\n    torch.cuda.manual_seed_all(123+seed)\n    torch.backends.cudnn.enabled = False\n    torch.backends.cudnn.deterministic = True\n</code></pre>"},{"location":"Docs/utils/fflow/#flgo.utils.fflow.tune","title":"<code>tune(task, algorithm, option={}, model=None, Logger=flgo.experiment.logger.tune_logger.TuneLogger, Simulator=flgo.simulator.DefaultSimulator, scene='horizontal', scheduler=None)</code>","text":"<p>Tune hyper-parameters for the specific (task, algorithm, model) in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>str</code> <p>the dictionary of the federated task</p> required <code>algorithm</code> <code>module|class</code> <p>the algorithm will be used to optimize the model in federated manner, which must contain pre-defined attributions (e.g. algorithm.Server and algorithm.Client for horizontal federated learning)</p> required <code>option</code> <code>dict</code> <p>the dict whose values should be of type list to construct the combinations</p> <code>{}</code> <code>model</code> <code>module|class</code> <p>the model module that contains two methods: model.init_local_module(object) and model.init_global_module(object)</p> <code>None</code> <code>Logger</code> <code>class</code> <p>the class of the logger inherited from flgo.experiment.logger.BasicLogger</p> <code>flgo.experiment.logger.tune_logger.TuneLogger</code> <code>Simulator</code> <code>class</code> <p>the class of the simulator inherited from flgo.simulator.BasicSimulator</p> <code>flgo.simulator.DefaultSimulator</code> <code>scene</code> <code>str</code> <p>'horizontal' or 'vertical' in current version of FLGo</p> <code>'horizontal'</code> <code>scheduler</code> <code>instance of flgo.experiment.device_scheduler.BasicScheduler</code> <p>GPU scheduler that schedules GPU by checking their availability</p> <code>None</code> Source code in <code>flgo\\utils\\fflow.py</code> <pre><code>def tune(task: str, algorithm, option: dict = {}, model=None, Logger: flgo.experiment.logger.BasicLogger = flgo.experiment.logger.tune_logger.TuneLogger, Simulator: BasicSimulator=flgo.simulator.DefaultSimulator, scene='horizontal', scheduler=None):\n\"\"\"\n        Tune hyper-parameters for the specific (task, algorithm, model) in parallel.\n        Args:\n            task (str): the dictionary of the federated task\n            algorithm (module|class): the algorithm will be used to optimize the model in federated manner, which must contain pre-defined attributions (e.g. algorithm.Server and algorithm.Client for horizontal federated learning)\n            option (dict): the dict whose values should be of type list to construct the combinations\n            model (module|class): the model module that contains two methods: model.init_local_module(object) and model.init_global_module(object)\n            Logger (class): the class of the logger inherited from flgo.experiment.logger.BasicLogger\n            Simulator (class): the class of the simulator inherited from flgo.simulator.BasicSimulator\n            scene (str): 'horizontal' or 'vertical' in current version of FLGo\n            scheduler (instance of flgo.experiment.device_scheduler.BasicScheduler): GPU scheduler that schedules GPU by checking their availability\n        \"\"\"\n    # generate combinations of hyper-parameters\n    if 'gpu' in option.keys():\n        device_ids = option['gpu']\n        option.pop('gpu')\n        if not isinstance(device_ids, Iterable): device_ids = [device_ids]\n    else:\n        device_ids = [-1]\n    keys = list(option.keys())\n    for k in keys: option[k] = [option[k]] if (not isinstance(option[k], Iterable) or isinstance(option[k], str)) else option[k]\n    para_combs = [para_comb for para_comb in itertools.product(*(option[k] for k in keys))]\n    options = [{k:v for k,v in zip(keys, paras)} for paras in para_combs]\n    for op in options:op['log_file'] = True\n    if scheduler is None:\n        scheduler = flgo.experiment.device_scheduler.BasicScheduler(device_ids)\n    outputs = run_in_parallel(task, algorithm, options,model, devices=device_ids, Logger=Logger, Simulator=Simulator, scene=scene, scheduler=scheduler)\n    optimal_idx = int(np.argmin([min(output['valid_loss']) for output in outputs]))\n    optimal_para = options[optimal_idx]\n    print(\"The optimal combination of hyper-parameters is:\")\n    print('-----------------------------------------------')\n    for k,v in optimal_para.items():\n        if k=='gpu': continue\n        print(\"{}\\t|{}\".format(k,v))\n    print('-----------------------------------------------')\n    op_round = np.argmin(outputs[optimal_idx]['valid_loss'])\n    if 'eval_interval' in option.keys(): op_round = option['eval_interval']*op_round\n    print('The minimal validation loss occurs at the round {}'.format(op_round))\n</code></pre>"},{"location":"Docs/utils/fmodule/","title":"flgo.utils.fmodule","text":""},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule","title":"<code>FModule</code>","text":"<p>         Bases: <code>nn.Module</code></p> <p>This module implements commonly used model-level operators like add, sub, and so on.</p> <p>Example:</p> <pre><code>    &gt;&gt;&gt; class TestModel(FModule):\n    ...     def __init__(self):\n    ...         self.mlp = torch.nn.Linear(2,2, bias=False)\n    &gt;&gt;&gt; m1 = TestModel()\n    &gt;&gt;&gt; m2 = TestModel()\n    &gt;&gt;&gt; m3 = m1+m2\n    &gt;&gt;&gt; (m1.mlp.weight+m2.mlp.weight)==m3.mlp.weight\n</code></pre> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>class FModule(nn.Module):\nr\"\"\"\n    This module implements commonly used model-level operators like add, sub, and so on.\n\n    Example:\n    ```python\n        &gt;&gt;&gt; class TestModel(FModule):\n        ...     def __init__(self):\n        ...         self.mlp = torch.nn.Linear(2,2, bias=False)\n        &gt;&gt;&gt; m1 = TestModel()\n        &gt;&gt;&gt; m2 = TestModel()\n        &gt;&gt;&gt; m3 = m1+m2\n        &gt;&gt;&gt; (m1.mlp.weight+m2.mlp.weight)==m3.mlp.weight\n    ```\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.ingraph = False\n\n    def __add__(self, other):\n        if isinstance(other, int) and other == 0 : return self\n        if not isinstance(other, FModule): raise TypeError\n        return _model_add(self, other)\n\n    def __radd__(self, other):\n        return _model_add(self, other)\n\n    def __sub__(self, other):\n        if isinstance(other, int) and other == 0: return self\n        if not isinstance(other, FModule): raise TypeError\n        return _model_sub(self, other)\n\n    def __mul__(self, other):\n        return _model_scale(self, other)\n\n    def __rmul__(self, other):\n        return self*other\n\n    def __truediv__(self, other):\n        return self*(1.0/other)\n\n    def __pow__(self, power, modulo=None):\n        return _model_norm(self, power)\n\n    def __neg__(self):\n        return _model_scale(self, -1.0)\n\n    def __sizeof__(self):\n        if not hasattr(self, '__size'):\n            param_size = 0\n            param_sum = 0\n            for param in self.parameters():\n                param_size += param.nelement() * param.element_size()\n                param_sum += param.nelement()\n            buffer_size = 0\n            buffer_sum = 0\n            for buffer in self.buffers():\n                buffer_size += buffer.nelement() * buffer.element_size()\n                buffer_sum += buffer.nelement()\n            self.__size = param_size + buffer_size\n        return self.__size\n\n    def norm(self, p=2):\nr\"\"\"\n        Args:\n            p (float): p-norm\n\n        Returns:\n            the scale value of the p-norm of vectorized model parameters\n        \"\"\"\n        return self**p\n\n    def zeros_like(self):\nr\"\"\"\n        Returns:\n             a new model with the same architecture and all the parameters being set zero\n        \"\"\"\n        return self*0\n\n    def dot(self, other):\nr\"\"\"\n        Args:\n            other (Fmodule): the model with the same architecture of self\n\n        Returns:\n            the dot value of the two vectorized models\n        \"\"\"\n        return _model_dot(self, other)\n\n    def cos_sim(self, other):\nr\"\"\"\n        Args:\n            other (Fmodule): the model with the same architecture of self\n\n        Returns:\n            the cosine similarity value of the two vectorized models\n        \"\"\"\n        return _model_cossim(self, other)\n\n    def op_with_graph(self):\n        self.ingraph = True\n\n    def op_without_graph(self):\n        self.ingraph = False\n\n    def load(self, other):\nr\"\"\"\n        Set the values of model parameters the same as the values of another model\n        Args:\n            other (Fmodule): the model with the same architecture of self\n        \"\"\"\n        self.op_without_graph()\n        self.load_state_dict(other.state_dict())\n        return\n\n    def freeze_grad(self):\nr\"\"\"\n        All the gradients of the model parameters won't be computed after calling this method\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = False\n\n    def enable_grad(self):\nr\"\"\"\n        All the gradients of the model parameters will be computed after calling this method\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad = True\n\n    def zero_dict(self):\nr\"\"\"\n        Set all the values of model parameters to be zero\n        \"\"\"\n        self.op_without_graph()\n        for p in self.parameters():\n            p.data.zero_()\n\n    def normalize(self):\nr\"\"\"\n        Normalize the parameters of self to enable self.norm(2)=1\n        \"\"\"\n        self.op_without_graph()\n        self.load_state_dict((self/(self**2)).state_dict())\n\n    def get_device(self):\nr\"\"\"\n        Returns:\n            the device of the tensors of this model\n        \"\"\"\n        return next(self.parameters()).device\n\n    def count_parameters(self, output=True):\nr\"\"\"\n        Count the parameters for this model\n\n        Args:\n            output (bool): whether to output the information to the stdin (i.e. console)\n        Returns:\n            the number of all the parameters in this model\n        \"\"\"\n        try:\n            import prettytable as pt\n        except:\n            print('Please install prettytable through `pip install prettytable` before calling this func')\n            return\n        table = pt.PrettyTable([\"Modules\", \"Parameters\"])\n        total_params = 0\n        for name, parameter in self.named_parameters():\n            if not parameter.requires_grad:\n                table.add_row([name, 0])\n                continue\n            params = parameter.numel()\n            table.add_row([name, params])\n            total_params += params\n        if output:\n            print(table)\n            print(f\"TotalTrainableParams: {total_params}\")\n        return total_params\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.cos_sim","title":"<code>cos_sim(other)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>other</code> <code>Fmodule</code> <p>the model with the same architecture of self</p> required <p>Returns:</p> Type Description <p>the cosine similarity value of the two vectorized models</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def cos_sim(self, other):\nr\"\"\"\n    Args:\n        other (Fmodule): the model with the same architecture of self\n\n    Returns:\n        the cosine similarity value of the two vectorized models\n    \"\"\"\n    return _model_cossim(self, other)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.count_parameters","title":"<code>count_parameters(output=True)</code>","text":"<p>Count the parameters for this model</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>bool</code> <p>whether to output the information to the stdin (i.e. console)</p> <code>True</code> <p>Returns:</p> Type Description <p>the number of all the parameters in this model</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def count_parameters(self, output=True):\nr\"\"\"\n    Count the parameters for this model\n\n    Args:\n        output (bool): whether to output the information to the stdin (i.e. console)\n    Returns:\n        the number of all the parameters in this model\n    \"\"\"\n    try:\n        import prettytable as pt\n    except:\n        print('Please install prettytable through `pip install prettytable` before calling this func')\n        return\n    table = pt.PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in self.named_parameters():\n        if not parameter.requires_grad:\n            table.add_row([name, 0])\n            continue\n        params = parameter.numel()\n        table.add_row([name, params])\n        total_params += params\n    if output:\n        print(table)\n        print(f\"TotalTrainableParams: {total_params}\")\n    return total_params\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.dot","title":"<code>dot(other)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>other</code> <code>Fmodule</code> <p>the model with the same architecture of self</p> required <p>Returns:</p> Type Description <p>the dot value of the two vectorized models</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def dot(self, other):\nr\"\"\"\n    Args:\n        other (Fmodule): the model with the same architecture of self\n\n    Returns:\n        the dot value of the two vectorized models\n    \"\"\"\n    return _model_dot(self, other)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.enable_grad","title":"<code>enable_grad()</code>","text":"<p>All the gradients of the model parameters will be computed after calling this method</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def enable_grad(self):\nr\"\"\"\n    All the gradients of the model parameters will be computed after calling this method\n    \"\"\"\n    for p in self.parameters():\n        p.requires_grad = True\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.freeze_grad","title":"<code>freeze_grad()</code>","text":"<p>All the gradients of the model parameters won't be computed after calling this method</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def freeze_grad(self):\nr\"\"\"\n    All the gradients of the model parameters won't be computed after calling this method\n    \"\"\"\n    for p in self.parameters():\n        p.requires_grad = False\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.get_device","title":"<code>get_device()</code>","text":"<p>Returns:</p> Type Description <p>the device of the tensors of this model</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def get_device(self):\nr\"\"\"\n    Returns:\n        the device of the tensors of this model\n    \"\"\"\n    return next(self.parameters()).device\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.load","title":"<code>load(other)</code>","text":"<p>Set the values of model parameters the same as the values of another model</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Fmodule</code> <p>the model with the same architecture of self</p> required Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def load(self, other):\nr\"\"\"\n    Set the values of model parameters the same as the values of another model\n    Args:\n        other (Fmodule): the model with the same architecture of self\n    \"\"\"\n    self.op_without_graph()\n    self.load_state_dict(other.state_dict())\n    return\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.norm","title":"<code>norm(p=2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>p</code> <code>float</code> <p>p-norm</p> <code>2</code> <p>Returns:</p> Type Description <p>the scale value of the p-norm of vectorized model parameters</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def norm(self, p=2):\nr\"\"\"\n    Args:\n        p (float): p-norm\n\n    Returns:\n        the scale value of the p-norm of vectorized model parameters\n    \"\"\"\n    return self**p\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.normalize","title":"<code>normalize()</code>","text":"<p>Normalize the parameters of self to enable self.norm(2)=1</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def normalize(self):\nr\"\"\"\n    Normalize the parameters of self to enable self.norm(2)=1\n    \"\"\"\n    self.op_without_graph()\n    self.load_state_dict((self/(self**2)).state_dict())\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.zero_dict","title":"<code>zero_dict()</code>","text":"<p>Set all the values of model parameters to be zero</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def zero_dict(self):\nr\"\"\"\n    Set all the values of model parameters to be zero\n    \"\"\"\n    self.op_without_graph()\n    for p in self.parameters():\n        p.data.zero_()\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.FModule.zeros_like","title":"<code>zeros_like()</code>","text":"<p>Returns:</p> Type Description <p>a new model with the same architecture and all the parameters being set zero</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def zeros_like(self):\nr\"\"\"\n    Returns:\n         a new model with the same architecture and all the parameters being set zero\n    \"\"\"\n    return self*0\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.cos_sim","title":"<code>cos_sim(m1, m2)</code>","text":"<p>The cosine similarity value of the two models res=m1\u00b7m2/(||m1||*||m2||)</p> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>FModule</code> <p>model 1</p> required <code>m2</code> <code>FModule</code> <p>model 2</p> required <p>Returns:</p> Type Description <p>The cosine similarity value of the two models</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def cos_sim(m1, m2):\nr\"\"\"\n    The cosine similarity value of the two models res=m1\u00b7m2/(||m1||*||m2||)\n\n    Args:\n        m1 (FModule): model 1\n        m2 (FModule): model 2\n\n    Returns:\n        The cosine similarity value of the two models\n    \"\"\"\n    return m1.cos_sim(m2)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.dot","title":"<code>dot(m1, m2)</code>","text":"<p>The dot value of the two models res = m1\u00b7m2</p> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>FModule</code> <p>model 1</p> required <code>m2</code> <code>FModule</code> <p>model 2</p> required <p>Returns:</p> Type Description <p>The dot value of the two models</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def dot(m1, m2):\nr\"\"\"\n    The dot value of the two models res = m1\u00b7m2\n\n    Args:\n        m1 (FModule): model 1\n        m2 (FModule): model 2\n\n    Returns:\n        The dot value of the two models\n    \"\"\"\n    return m1.dot(m2)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.element_wise_func","title":"<code>element_wise_func(m, func)</code>","text":"<p>The element-wise function on this model</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>FModule</code> <p>the model</p> required <code>func</code> <p>element-wise function</p> required <p>Returns:</p> Type Description <p>The new model whose parameters satisfy mi=func(mi)</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def element_wise_func(m, func):\nr\"\"\"\n    The element-wise function on this model\n\n    Args:\n        m (FModule): the model\n        func: element-wise function\n\n    Returns:\n        The new model whose parameters satisfy mi=func(mi)\n    \"\"\"\n    if m is None: return None\n    res = m.__class__().to(m.get_device())\n    if m.ingraph:\n        res.op_with_graph()\n        ml = get_module_from_model(m)\n        for md in ml:\n            rd = _modeldict_element_wise(md._parameters, func)\n            for l in md._parameters.keys():\n                md._parameters[l] = rd[l]\n    else:\n        _modeldict_cp(res.state_dict(), _modeldict_element_wise(m.state_dict(), func))\n    return res\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.exp","title":"<code>exp(m)</code>","text":"<p>The element-wise res=exp(m) where all the model parameters satisfy mi=exp(mi)</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>FModule</code> <p>the model</p> required <p>Returns:</p> Type Description <p>The new model whose parameters satisfy mi=exp(mi)</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def exp(m):\nr\"\"\"\n    The element-wise res=exp(m) where all the model parameters satisfy mi=exp(mi)\n\n    Args:\n        m (FModule): the model\n\n    Returns:\n        The new model whose parameters satisfy mi=exp(mi)\n    \"\"\"\n    return element_wise_func(m, torch.exp)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.get_module_from_model","title":"<code>get_module_from_model(model, res=None)</code>","text":"<p>Walk through all the sub modules of a model and return them as a list</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>FModule</code> <p>model</p> required <code>res</code> <code>None</code> <p>should be remained None</p> <code>None</code> <p>Returns:</p> Type Description <p>The list of all the sub-modules of a model</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def get_module_from_model(model, res = None):\nr\"\"\"\n    Walk through all the sub modules of a model and return them as a list\n\n    Args:\n        model (FModule): model\n        res (None): should be remained None\n\n    Returns:\n        The list of all the sub-modules of a model\n    \"\"\"\n    if res==None: res = []\n    ch_names = [item[0] for item in model.named_children()]\n    if ch_names==[]:\n        if model._parameters:\n            res.append(model)\n    else:\n        for name in ch_names:\n            get_module_from_model(model.__getattr__(name), res)\n    return res\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.log","title":"<code>log(m)</code>","text":"<p>The element-wise res=log(m) where all the model parameters satisfy mi=log(mi)</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>FModule</code> <p>the model</p> required <p>Returns:</p> Type Description <p>The new model whose parameters satisfy mi=log(mi)</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def log(m):\nr\"\"\"\n    The element-wise res=log(m) where all the model parameters satisfy mi=log(mi)\n\n    Args:\n        m (FModule): the model\n\n    Returns:\n        The new model whose parameters satisfy mi=log(mi)\n    \"\"\"\n    return element_wise_func(m, torch.log)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.normalize","title":"<code>normalize(m)</code>","text":"<p>The new model that is the normalized version of the input model m=m/||m||_2</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>FModule</code> <p>the model</p> required <p>Returns:</p> Type Description <p>The new model that is the normalized version of the input model</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def normalize(m):\nr\"\"\"\n    The new model that is the normalized version of the input model m=m/||m||_2\n\n    Args:\n        m (FModule): the model\n\n    Returns:\n        The new model that is the normalized version of the input model\n    \"\"\"\n    return m/(m**2)\n</code></pre>"},{"location":"Docs/utils/fmodule/#flgo.utils.fmodule.with_multi_gpus","title":"<code>with_multi_gpus(func)</code>","text":"<p>Decorate functions whose first parameter is model to carry out all the operations on the same device</p> Source code in <code>flgo\\utils\\fmodule.py</code> <pre><code>def with_multi_gpus(func):\nr\"\"\"\n    Decorate functions whose first parameter is model to carry out all the operations on the same device\n    \"\"\"\n    def cal_on_personal_gpu(self, model, *args, **kargs):\n        origin_device = model.get_device()\n        # transfer to new device\n        new_args = []\n        new_kargs = {}\n        for arg in args:\n            narg = arg.to(self.device) if hasattr(arg, 'get_device') or hasattr(arg, 'device') else arg\n            new_args.append(narg)\n        for k,v in kargs.items():\n            nv = v.to(self.device) if hasattr(v, 'get_device') or hasattr(v, 'device') else v\n            new_kargs[k] = nv\n        model.to(self.device)\n        # calculating\n        res = func(self, model, *tuple(new_args), **new_kargs)\n        # transter to original device\n        model.to(origin_device)\n        if res is not None:\n            if type(res)==dict:\n                for k,v in res.items():\n                    nv = v.to(origin_device) if hasattr(v, 'get_device') or hasattr(v, 'device') else v\n                    res[k] = nv\n            elif type(res)==tuple or type(res)==list:\n                new_res = []\n                for v in res:\n                    nv = v.to(origin_device) if hasattr(v, 'get_device') or hasattr(v, 'device') else v\n                    new_res.append(nv)\n                if type(res)==tuple:\n                    res = tuple(new_res)\n            else:\n                res = res.to(origin_device) if hasattr(res, 'get_device') or hasattr(res, 'device') else res\n        return res\n    return cal_on_personal_gpu\n</code></pre>"},{"location":"Overview/","title":"Index","text":""},{"location":"Overview/#algorithm-integration","title":"Algorithm Integration","text":"<p>We have already implemented 10+ SOTA algorithms in recent years' top tiers conferences and tiers.</p> Method Reference Publication Tag FedAvg [McMahan et al., 2017] AISTATS' 2017 FedAsync [Cong Xie et al., 2019] Asynchronous FedBuff [John Nguyen et al., 2022] AISTATS 2022 Asynchronous TiFL [Zheng Chai et al., 2020] HPDC 2020 Communication-efficiency, responsiveness AFL [Mehryar Mohri et al., 2019] ICML 2019 Fairness FedFv [Zheng Wang et al., 2019] IJCAI 2021 Fairness FedMgda+ [Zeou Hu et al., 2022] IEEE TNSE 2022 Fairness, robustness FedProx [Tian Li et al., 2020] MLSys 2020 Non-I.I.D., Incomplete Training Mifa [Xinran Gu et al., 2021] NeurIPS 2021 Client Availability PowerofChoice [Yae Jee Cho et al., 2020] arxiv Biased Sampling, Fast-Convergence QFedAvg [Tian Li et al., 2020] ICLR 2020 Communication-efficient,fairness Scaffold [Sai Praneeth Karimireddy et al., 2020] ICML 2020 Non-I.I.D., Communication Capacity"},{"location":"Overview/#benchmark-gallary","title":"Benchmark Gallary","text":"Benchmark \u00a0 Type Scene \u00a0 \u00a0 Task \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CIFAR100 image horizontal classification CIFAR10 \u00a0 image horizontal classification CiteSeer graph horizontal classification Cora \u00a0 \u00a0graph horizontal classification PubMed \u00a0 graph horizontal classification MNIST \u00a0 image horizontal classification EMNIST \u00a0 image horizontal classification FEMINIST image horizontal classification FashionMINIST \u00a0 image horizontal classification ENZYMES \u00a0 graph horizontal classification Reddit \u00a0 text horizontal classification Sentiment140 \u00a0 text horizontal classification MUTAG \u00a0 graph horizontal classification Shakespeare \u00a0 text horizontal classification Synthetic \u00a0 table horizontal classification"},{"location":"Overview/#asyncsync-supported","title":"Async/Sync Supported","text":"<p>We set a virtual global clock and a client-state machine to simulate a real-world scenario for comparison on asynchronous  and synchronous strategies. Here we provide a comprehensive example to help understand the difference  between the two strategies in FLGo.</p> <p> For synchronous algorithms, the server would wait for the slowest clients.  In round 1,the server select a subset of idle clients (i.e. client i,u,v)  to join in training and the slowest client v dominates the duration of this  round (i.e. four time units). If there is anyone suffering from  training failure (i.e. being dropped out), the duration of the current round  should be the longest time that the server will wait for it (e.g. round 2 takes  the maximum waiting time of six units to wait for response from client v). </p> <p>For asynchronous algorithms, the server usually periodically samples the idle  clients to update models, where the length of the period is set as two time  units in our example. After sampling the currently idle clients, the server will  immediately checks whether there are packages currently returned from clients  (e.g. the server selects client j and receives the package from client k at time 13). </p>"},{"location":"Overview/#experimental-tools","title":"Experimental Tools","text":"<p>For experimental purposes </p>"},{"location":"Overview/#automatical-tuning","title":"Automatical Tuning","text":""},{"location":"Overview/#multi-scene-horizontal-and-vertical","title":"Multi-Scene (Horizontal and Vertical)","text":""},{"location":"Overview/#accelerating-by-multi-process","title":"Accelerating by Multi-Process","text":""},{"location":"Overview/#references","title":"References","text":"<p>[McMahan. et al., 2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.</p> <p>[Cong Xie. et al., 2019] Cong Xie, Sanmi Koyejo, Indranil Gupta. Asynchronous Federated Optimization. </p> <p>[John Nguyen. et al., 2022] John Nguyen, Kshitiz Malik, Hongyuan Zhan, Ashkan Yousefpour, Michael Rabbat, Mani Malek, Dzmitry Huba. Federated Learning with Buffered Asynchronous Aggregation. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2022.</p> <p>[Zheng Chai. et al., 2020] Zheng Chai, Ahsan Ali, Syed Zawad, Stacey Truex, Ali Anwar, Nathalie Baracaldo, Yi Zhou, Heiko Ludwig, Feng Yan, Yue Cheng. TiFL: A Tier-based Federated Learning System.In International Symposium on High-Performance Parallel and Distributed Computing(HPDC), 2020</p> <p>[Mehryar Mohri. et al., 2019] Mehryar Mohri, Gary Sivek, Ananda Theertha Suresh. Agnostic Federated Learning.In International Conference on Machine Learning(ICML), 2019</p> <p>[Zheng Wang. et al., 2021] Zheng Wang, Xiaoliang Fan, Jianzhong Qi, Chenglu Wen, Cheng Wang, Rongshan Yu. Federated Learning with Fair Averaging. In International Joint Conference on Artificial Intelligence, 2021</p> <p>[Zeou Hu. et al., 2022] Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, Yaoliang Yu. Federated Learning Meets Multi-objective Optimization. In IEEE Transactions on Network Science and Engineering, 2022</p> <p>[Tian Li. et al., 2020] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, Virginia Smith. Federated Optimization in Heterogeneous Networks. In Conference on Machine Learning and Systems, 2020</p> <p>[Xinran Gu. et al., 2021] Xinran Gu, Kaixuan Huang, Jingzhao Zhang, Longbo Huang. Fast Federated Learning in the Presence of Arbitrary Device Unavailability. In Neural Information Processing Systems(NeurIPS), 2021</p> <p>[Yae Jee Cho. et al., 2020] Yae Jee Cho, Jianyu Wang, Gauri Joshi. Client Selection in Federated Learning: Convergence Analysis and Power-of-Choice Selection Strategies. </p> <p>[Tian Li. et al., 2020] Tian Li, Maziar Sanjabi, Ahmad Beirami, Virginia Smith. Fair Resource Allocation in Federated Learning. In International Conference on Learning Representations, 2020</p>"},{"location":"Overview/Architecture/","title":"FLGo Framework","text":"<p>The whole workflow of FLGo is as shown in the above picture. FLGo framework mainly runs  by three steps. </p> <p>Firstly, given a ML task (i.e. dataset and model), FLGo converts it into a static federated  task through partitioning the original ML dataset into subsets of data owned by different  clients, and hide the task-specific details to the algorithms. </p> <p>Secondly, different federated algorithms can run on the fed static federated task to train  a particular model (e.g. CNN, MLP) . During training phase, the system simulator will create  a simulated environment where a virtual global clock can fairly measure the time and arbitrary  client behaviors can be modeled, which is also transparent to the implementation of algorithms. </p> <p>Finally, the experimental tracker in FLGo is responsible for tracing the running-time information  and organizing the results into tables or figures.</p> <p>The organization of all the modules is as below</p> <pre><code>\u251c\u2500 algorithm\n\u2502  \u251c\u2500 fedavg.py                   //fedavg algorithm\n\u2502  \u251c\u2500 ...\n\u2502  \u251c\u2500 fedasync.py                 //the base class for asynchronous federated algorithms\n\u2502  \u2514\u2500 fedbase.py                  //the base class for federated algorithms\n|\n\u251c\u2500 benchmark\n\u2502  \u251c\u2500 mnist_classification          //classification on mnist dataset\n\u2502  \u2502  \u251c\u2500 model                   //the corresponding model\n\u2502  |  \u2514\u2500 core.py                 //the core supporting for the dataset, and each contains three necessary classes(e.g. TaskGen, TaskReader, TaskCalculator)                         \n\u2502  \u251c\u2500 base.py                 // the base class for all fedtask\n\u2502  \u251c\u2500 ...\n\u2502  \u251c\u2500 RAW_DATA                   // storing the downloaded raw dataset\n\u2502  \u2514\u2500 toolkits                      //the basic tools for generating federated dataset\n\u2502     \u251c\u2500 cv                      // common federal division on cv\n\u2502     \u2502  \u251c\u2500 horizontal           // horizontal fedtask\n\u2502     \u2502  \u2502  \u2514\u2500 image_classification.py   // the base class for image classification\n\u2502     \u2502  \u2514\u2500 ...\n\u2502     \u251c\u2500 ...\n\u2502     \u251c\u2500 partition.py            // the parttion class for federal division\n\u2502     \u2514\u2500 visualization.py        // visualization after the data set is divided\n|\n\u251c\u2500 experiment\n\u2502  \u251c\u2500 logger                            //the class that records the experimental process\n\u2502  \u2502  \u251c\u2500 ...\n\u2502  |  \u2514\u2500 simple_logger.py               //a simple logger class\n\u2502  \u251c\u2500 analyzer.py                  //the class for analyzing and printing experimental results\n|  \u2514\u2500 device_scheduler.py                    // automatically schedule GPUs to run in parallel\n|\n\u251c\u2500 simulator                     //system heterogeneity simulation module\n\u2502  \u251c\u2500 base.py                           //the base class for simulate system heterogeneity\n\u2502  \u251c\u2500 default_simulator.py              //the default class for simulate system heterogeneity\n|  \u2514\u2500 ...\n|\n\u251c\u2500 utils\n\u2502  \u251c\u2500 fflow.py                          //option to read, initialize,...\n\u2502  \u2514\u2500 fmodule.py                        //model-level operators\n\u2514\u2500 requirements.txt \n</code></pre>"},{"location":"Overview/Architecture/algorithm/","title":"Algorithm","text":"<p>In FLGo, each algorithm is described by an independent file consisting of the objects  (i.e. server and clients in horizontal FL) with their actions. </p>"},{"location":"Overview/Architecture/algorithm/#horizontal-fl","title":"Horizontal FL","text":"<p> A classical procedure of FL training process is as shown in the figure above, where the server iteratively  broadcasts the global model to a subset of clients and aggregates the received locally  trained models from them. Following this scheme, a great number of FL algorithms can be  easily implemented by FLGo. For example, to implement methods that customize the local  training process (e.g. FedProx, MOON), developers only need to modify the function  <code>client.train(...)</code>. And a series of sampling strategies can be realized by only replacing  the function <code>server.sample()</code>. We also provide comprehensive tutorial for using FLGo  to implement the state of the art algorithms. In addition, asynchronous algorithms can  share the same scheme with synchronous algorithms in FLGo, where developers only need to  concern about the sampling strategy and how to deal with the currently received packages  from clients at each moment. </p>"},{"location":"Overview/Architecture/algorithm/#vertical-fl","title":"Vertical FL","text":"<p>To be completed.</p>"},{"location":"Overview/Architecture/benchmark/","title":"Benchmark","text":"<p>At the initialization phase, the original dataset is input to <code>TaskGenerator</code> that  accordingly and flexibly partitions the dataset into local sub-datasets owned by  clients and a testing dataset owned the server. And the local data is further divided  to training part and validation part for hyper-parameter tuning purpose. Then, all of  the division information on the original dataset will be stored by <code>TaskPipe</code> into  the disk as a static <code>fedtask</code>, where different federated algorithms can fairly  compare with each other on the same fedtask with a particular model. </p> <p>During the running-time phase,  <code>TaskPipe</code> first distributes the partitioned datasets  to clients and the server after loading the saved partition information and the original  dataset into memory. After the model training starts, Algorithm module can either use the  presetting <code>TaskCalculator</code> APIs to complement the task-specific calculations (i.e. loss  computation, transferring data across devices, evaluation, batching data) or optimize in  customized way. In this manner, the task-relevant details will be blinded to the algorithm  for most cases, which significantly eases the development of new algorithms. </p>"},{"location":"Overview/Architecture/experiment/","title":"Logger and analyzer","text":"<p> Although there are already several comprehensive experiment managers (e.g. wandb,  tensorboard), our <code>Experiment</code> module is compatible with them and enable  customizing experiments in a non-intrusive way to the codes, where users can create a  <code>logger</code> by modifying some APIs to track variables of interest and specify the customized  <code>logger</code> in optional parameters. </p> <p>After the <code>logger</code> stores the running-time information into records, the <code>analyzer</code> can read  them from the disk. A filter is designed to enable only selecting records of interest, and  several APIs are provided for quickly visualizing and analyzing the results by few codes.</p>"},{"location":"Overview/Architecture/experiment/#device-scheduler","title":"Device Scheduler","text":"<p>To be complete.</p>"},{"location":"Overview/Architecture/simulator/","title":"Simulator","text":""},{"location":"Overview/Architecture/simulator/#simulation-with-client-state-machine","title":"Simulation with Client-State Machine","text":"<p>We construct a client-state machine to simulate arbitrary system heterogeneiry. In  this state machine, a client's state will change as time goes by or some particular  actions were taken. For example, a client will be available with a probability at each  moment, and clients will be in state 'working' after they were selected if not dropping out. The transfer rules across states are described in the figure below</p> <p> We provide simple APIs for users to customize the system heterogeneity for simulation. Please see  Tutorial 5.1 for details.</p>"},{"location":"Tutorials/","title":"Index","text":"<p>In this tutorial, we provide comprehensive examples to show how to use FLGo to help your researches.</p> <p>Following our instructions, you can do things like</p> <ul> <li>easily reproduce and compare the results of different state-of-the-art methods</li> <li>fast verify your ideas by converting them into runnable codes</li> <li>conduct experiments under various data heterogeneiry and system heterogeneity</li> <li>manage your experimental records and visualize them with few codes</li> <li>...</li> </ul> <p>We organize this tutorial as follows:</p> <ol> <li>At the first part, we introduce ...</li> </ol> <p>All of our examples can be run on jupyter notebook. The source of our notebooks are in .</p>"},{"location":"Tutorials/0_Quick_Start/","title":"0 Quick Start","text":""},{"location":"Tutorials/0_Quick_Start/#install-flgo","title":"Install FLGo","text":"<p>Install FLGo through pip. </p> <pre><code>pip install flgo\n</code></pre> <p>If the package is not found, please use the command below to update pip</p> <pre><code>pip install --upgrade pip\n</code></pre>"},{"location":"Tutorials/0_Quick_Start/#create-your-first-federated-task","title":"Create Your First Federated Task","text":"<p>Here we take the classical federated benchmark, Federated MNIST [1], as the example, where the MNIST dataset is splitted into 100 parts identically and independently.</p> <pre><code>import flgo\nimport os\n\n# the target path of the task\ntask_path = './my_first_task'\n\n# create task configuration\ntask_config = {'benchmark':{'name': 'flgo.benchmark.mnist_classification'}, 'partitioner':{'name':'IIDPartitioner', 'para':{'num_clients':100}}}\n\n# generate the task if the task doesn't exist\nif not os.path.exist(task_path):\n    flgo.gen_task(task_config, task_path)\n</code></pre> <p>After running the codes above, a federated dataset is successfully created in the <code>task_path</code>. The visualization of the task is stored in <code>task_path/res.png</code> as below </p>"},{"location":"Tutorials/0_Quick_Start/#run-fedavg-to-train-your-model","title":"Run FedAvg to Train Your Model","text":"<p>Now we are going to run the classical federated optimization algorithm, FedAvg [1], on the task created by us to train a model.</p> <pre><code>import flgo.algorithm.fedavg as fedavg\n# create fedavg runner on the task\nrunner = flgo.init(task, fedavg, {'gpu':[0,],'log_file':True, 'num_steps':5})\nrunner.run()\n</code></pre>"},{"location":"Tutorials/0_Quick_Start/#show-training-result","title":"Show Training Result","text":"<p>The training result is saved as a record under the dictionary of the task <code>task_path/record</code>. We use the built-in analyzer to read and show it.</p> <pre><code>import flgo.experiment.analyzer\n# create the analysis plan\nanalysis_plan = {\n    'Selector':{'task': task_path, 'header':['fedavg',], },\n    'Painter':{'Curve':[{'args':{'x':'communication_round', 'y':'valid_loss'}}]},\n    'Table':{'min_value':[{'x':'valid_loss'}]},\n}\n\nflgo.experiment.analyzer.show(analysis_plan)\n</code></pre> <p></p>"}]}